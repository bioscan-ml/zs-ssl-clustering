{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnxDlvP8szOj"
   },
   "source": [
    "# Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-9BelQPaJVo2"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import datetime\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.colors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import scipy.stats\n",
    "import seaborn\n",
    "import sklearn.metrics\n",
    "import torchvision.datasets\n",
    "from IPython.display import display\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGS_DIR = \"figs\"\n",
    "os.makedirs(FIGS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9AH-GlrfS4Pf"
   },
   "outputs": [],
   "source": [
    "VALIDATION_DATASETS = [\"imagenet\", \"imagenette\", \"imagewoof\"]\n",
    "RESNET50_MODELS = [\n",
    "    \"random_resnet50\",\n",
    "    \"resnet50\",\n",
    "    \"mocov3_resnet50\",\n",
    "    \"dino_resnet50\",\n",
    "    \"vicreg_resnet50\",\n",
    "    \"clip_RN50\",\n",
    "]\n",
    "VITB16_MODELS = [\n",
    "    \"random_vitb16\",\n",
    "    \"vitb16\",\n",
    "    \"mocov3_vit_base\",\n",
    "    \"dino_vitb16\",\n",
    "    \"timm_vit_base_patch16_224.mae\",\n",
    "    \"mae_pretrain_vit_base_global\",\n",
    "    \"clip_vitb16\",\n",
    "]\n",
    "FT_RESNET50_MODELS = [\n",
    "    \"ft_mocov3_resnet50\",\n",
    "    \"ft_dino_resnet50\",\n",
    "    \"ft_vicreg_resnet50\",\n",
    "]\n",
    "FT_VITB16_MODELS = [\n",
    "    \"ft_mocov3_vit_base\",\n",
    "    \"ft_dino_vitb16\",\n",
    "    \"mae_finetuned_vit_base_global\",\n",
    "]\n",
    "FT_MODELS = FT_RESNET50_MODELS + FT_VITB16_MODELS\n",
    "ALL_MODELS = [\"none\"] + RESNET50_MODELS + VITB16_MODELS + FT_RESNET50_MODELS + FT_VITB16_MODELS\n",
    "\n",
    "RESNET50_MODELS_INTERLEAVED = [\n",
    "    \"random_resnet50\",\n",
    "    \"resnet50\",\n",
    "    \"mocov3_resnet50\",\n",
    "    \"ft_mocov3_resnet50\",\n",
    "    \"dino_resnet50\",\n",
    "    \"ft_dino_resnet50\",\n",
    "    \"vicreg_resnet50\",\n",
    "    \"ft_vicreg_resnet50\",\n",
    "]\n",
    "VITB16_MODELS_INTERLEAVED = [\n",
    "    \"random_vitb16\",\n",
    "    \"vitb16\",\n",
    "    \"mocov3_vit_base\",\n",
    "    \"ft_mocov3_vit_base\",\n",
    "    \"dino_vitb16\",\n",
    "    \"ft_dino_vitb16\",\n",
    "    \"timm_vit_base_patch16_224.mae\",\n",
    "    \"mae_pretrain_vit_base_global\",\n",
    "    \"mae_finetuned_vit_base_global\",\n",
    "]\n",
    "\n",
    "CLUSTERERS = [\n",
    "    \"KMeans\",\n",
    "    \"LouvainCommunities\",\n",
    "    \"AgglomerativeClustering\",\n",
    "    \"AffinityPropagation\",\n",
    "    \"SpectralClustering\",\n",
    "    \"HDBSCAN\",\n",
    "    \"OPTICS\",\n",
    "]\n",
    "ALL_CLUSTERERS = copy.deepcopy(CLUSTERERS)\n",
    "DISTANCE_METRICS = [\n",
    "    \"euclidean\",\n",
    "    \"l1\",\n",
    "    \"chebyshev\",\n",
    "    \"cosine\",\n",
    "    \"arccos\",\n",
    "    \"braycurtis\",\n",
    "    \"canberra\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE2FT = {\n",
    "    k: \"ft_\" + k\n",
    "    for k in [\n",
    "        \"mocov3_resnet50\",\n",
    "        \"dino_resnet50\",\n",
    "        \"vicreg_resnet50\",\n",
    "        \"mocov3_vit_base\",\n",
    "        \"dino_vitb16\",\n",
    "    ]\n",
    "}\n",
    "PRE2FT[\"mae_pretrain_vit_base_global\"] = \"mae_finetuned_vit_base_global\"\n",
    "FT2PRE = {v: k for k, v in PRE2FT.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0mJrqF3FAbEG"
   },
   "outputs": [],
   "source": [
    "DATASET2LS = {\n",
    "    \"imagenet\": \"-.\",\n",
    "    \"imagenette\": \"--\",\n",
    "    \"imagewoof\": \":\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OqPoCd4kbokG"
   },
   "outputs": [],
   "source": [
    "DEFAULT_PARAMS = {\n",
    "    \"all\": {\n",
    "        \"dim_reducer\": \"None\",\n",
    "        \"dim_reducer_man\": \"None\",\n",
    "        \"zscore\": False,\n",
    "        \"normalize\": False,\n",
    "        \"zscore2\": False,\n",
    "        \"ndim_correction\": False,\n",
    "    },\n",
    "    \"KMeans\": {\"clusterer\": \"KMeans\"},\n",
    "    \"LouvainCommunities\": {\n",
    "        \"clusterer\": \"LouvainCommunities\",\n",
    "        \"louvain_resolution\": 1.0,\n",
    "        \"louvain_threshold\": 1e-7,\n",
    "        \"louvain_remove_self_loops\": False,\n",
    "        \"distance_metric\": \"l2\",\n",
    "    },\n",
    "    \"AffinityPropagation\": {\n",
    "        \"clusterer\": \"AffinityPropagation\",\n",
    "        \"affinity_damping\": 0.9,\n",
    "        \"affinity_conv_iter\": 15,\n",
    "    },\n",
    "    \"SpectralClustering\": {\n",
    "        \"clusterer\": \"SpectralClustering\",\n",
    "        \"spectral_assigner\": \"cluster_qr\",\n",
    "        \"spectral_affinity\": \"nearest_neighbors\",\n",
    "        \"spectral_n_neighbors\": 10,\n",
    "        \"spectral_n_components\": None,\n",
    "    },\n",
    "    \"AgglomerativeClustering\": {\n",
    "        \"clusterer\": \"AgglomerativeClustering\",\n",
    "        \"distance_metric\": \"euclidean\",\n",
    "        \"aggclust_linkage\": \"ward\",\n",
    "    },\n",
    "    \"HDBSCAN\": {\n",
    "        \"clusterer\": \"HDBSCAN\",\n",
    "        \"hdbscan_method\": \"eom\",\n",
    "        \"min_samples\": 5,\n",
    "        \"max_samples\": 0.2,\n",
    "        \"distance_metric\": \"euclidean\",\n",
    "    },\n",
    "    \"OPTICS\": {\n",
    "        \"clusterer\": \"OPTICS\",\n",
    "        \"optics_method\": \"xi\",\n",
    "        \"optics_xi\": 0.05,\n",
    "        \"distance_metric\": \"euclidean\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VKTbU-Oasdl"
   },
   "source": [
    "## Set best params\n",
    "\n",
    "These were discovered by the search in hpsearch.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLuhE2xibLOC"
   },
   "source": [
    "### Num dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAgJs3H-uqiJ"
   },
   "outputs": [],
   "source": [
    "models = RESNET50_MODELS + VITB16_MODELS\n",
    "BEST_PARAMS = {clusterer: {model: copy.deepcopy(DEFAULT_PARAMS[clusterer]) for model in models} for clusterer in ALL_CLUSTERERS}\n",
    "\n",
    "# KMeans\n",
    "# Use UMAP (num dims unimportant; we select 50d for consistency) for every encoder except\n",
    "# - clip_RN50 : a little better to use PCA with 500d than UMAP. UMAP beats PCA if you\n",
    "#   reduce the PCA dims below 500.\n",
    "# - clip_vitb16 : same behaviour as clip_RN50\n",
    "# - timm_vit_base_patch16_224.mae : best is PCA 0.85 variance explained. Need at least\n",
    "#   200 PCA dims, and PCA perf beats UMAP throughout\n",
    "\n",
    "for model in RESNET50_MODELS + VITB16_MODELS:\n",
    "    if model.startswith(\"clip\") or model == \"timm_vit_base_patch16_224.mae\":\n",
    "        continue\n",
    "    BEST_PARAMS[\"KMeans\"][model].update({\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50})\n",
    "\n",
    "BEST_PARAMS[\"KMeans\"][\"clip_RN50\"].update({\"dim_reducer\": \"PCA\", \"ndim_reduced\": 500, \"zscore\": True, \"pca_variance\": None})\n",
    "BEST_PARAMS[\"KMeans\"][\"clip_vitb16\"].update({\"dim_reducer\": \"PCA\", \"ndim_reduced\": 500, \"zscore\": True, \"pca_variance\": None})\n",
    "BEST_PARAMS[\"KMeans\"][\"timm_vit_base_patch16_224.mae\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"pca_variance\": 0.85, \"zscore\": True, \"ndim_reduced\": None}\n",
    ")\n",
    "\n",
    "# AffinityPropagation\n",
    "# Use PCA with 10 dims for every encoder except\n",
    "# - resnet50 (supervised) : original embeddings, no reduction (AMI=0.62);\n",
    "#   perf gets worse if they are whitened (AMI=0.55) and although the perf increases\n",
    "#   as num dims are reduced it doesn't quite recover. PCA perf peaks at 10-20 dim (AMI=0.57).\n",
    "# - dino_resnet50 : does marginally better at UMAP 50 (AMI=0.52495) than PCA 10 (AMI=0.5044)\n",
    "# - timm_vit_base_patch16_224.mae : PCA 0.95 variance explained (AMI=0.303).\n",
    "#   Definite improvement from 10 to 20 dims, but not much improvement above that.\n",
    "\n",
    "for model in models:\n",
    "    if model in [\"resnet50\", \"dino_resnet50\", \"timm_vit_base_patch16_224.mae\"]:\n",
    "        continue\n",
    "    BEST_PARAMS[\"AffinityPropagation\"][model].update(\n",
    "        {\n",
    "            \"dim_reducer\": \"PCA\",\n",
    "            \"ndim_reduced\": 10,\n",
    "            \"zscore\": True,\n",
    "            \"pca_variance\": None,\n",
    "            \"dim_reducer_man\": \"None\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "BEST_PARAMS[\"AffinityPropagation\"][\"resnet50\"].update({\"dim_reducer\": \"None\", \"dim_reducer_man\": \"None\", \"zscore\": False})\n",
    "BEST_PARAMS[\"AffinityPropagation\"][\"dino_resnet50\"].update(\n",
    "    {\n",
    "        \"dim_reducer\": \"PCA\",\n",
    "        \"pca_variance\": 0.95,\n",
    "        \"zscore\": True,\n",
    "        \"ndim_reduced\": None,\n",
    "        \"dim_reducer_man\": \"None\",\n",
    "    }\n",
    ")\n",
    "BEST_PARAMS[\"AffinityPropagation\"][\"timm_vit_base_patch16_224.mae\"].update(\n",
    "    {\n",
    "        \"dim_reducer\": \"PCA\",\n",
    "        \"pca_variance\": 0.95,\n",
    "        \"zscore\": True,\n",
    "        \"ndim_reduced\": None,\n",
    "        \"dim_reducer_man\": \"None\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# AgglomerativeClustering\n",
    "# Use UMAP (num dims unimportant; we select 50d for consistency) for every encoder except\n",
    "# - timm_vit_base_patch16_224.mae : PCA 0.98 variance explained (i.e. nearly all\n",
    "#   dimensions kept), which is not noticably better than using 500 dim PCA but there is\n",
    "#   an increase compared to using less than 500d.\n",
    "\n",
    "for model in models:\n",
    "    if model == \"timm_vit_base_patch16_224.mae\":\n",
    "        continue\n",
    "    BEST_PARAMS[\"AgglomerativeClustering\"][model].update({\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50, \"dim_reducer\": \"None\"})\n",
    "\n",
    "BEST_PARAMS[\"AgglomerativeClustering\"][\"timm_vit_base_patch16_224.mae\"].update(\n",
    "    {\n",
    "        \"dim_reducer\": \"PCA\",\n",
    "        \"pca_variance\": 0.98,\n",
    "        \"zscore\": True,\n",
    "        \"ndim_reduced\": None,\n",
    "        \"dim_reducer_man\": \"None\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# HDBSCAN\n",
    "# Use UMAP for every encoder except\n",
    "# - timm_vit_base_patch16_224.mae : PCA 0.95 variance explained (AMI=0.085) which is\n",
    "#   not noticably better than PCA with 50 dim\n",
    "\n",
    "for model in models:\n",
    "    if model in [\"timm_vit_base_patch16_224.mae\"]:\n",
    "        continue\n",
    "    BEST_PARAMS[\"HDBSCAN\"][model].update({\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50, \"dim_reducer\": \"None\"})\n",
    "\n",
    "BEST_PARAMS[\"HDBSCAN\"][\"timm_vit_base_patch16_224.mae\"].update(\n",
    "    {\n",
    "        \"dim_reducer\": \"PCA\",\n",
    "        \"pca_variance\": 0.95,\n",
    "        \"zscore\": True,\n",
    "        \"ndim_reduced\": None,\n",
    "        \"dim_reducer_man\": \"None\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# OPTICS\n",
    "# Use UMAP for every encoder, no exceptions necessary\n",
    "for model in models:\n",
    "    BEST_PARAMS[\"OPTICS\"][model].update({\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50, \"dim_reducer\": \"None\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_PARAMS_v1 = copy.deepcopy(BEST_PARAMS)\n",
    "BEST_PARAMS_v1[\"_version\"] = \"v1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2nA5YxFqFo7x",
    "outputId": "3b02ac29-9036-42a0-8c5d-a888d9794865"
   },
   "outputs": [],
   "source": [
    "BEST_PARAMS_v2 = copy.deepcopy(BEST_PARAMS)\n",
    "BEST_PARAMS_v2[\"_version\"] = \"v2.0\"\n",
    "\n",
    "print(\"Updating dim choices for new method\")\n",
    "# Updated dim choices\n",
    "# (changed to this when we swapped to using weighted average instead of straight\n",
    "# average between Imagenet-1k, Imagenette, Imagewoof)\n",
    "\n",
    "# Changed KMeans clip_RN50 from PCA 500 to UMAP 50, so it uses fewer dimensions\n",
    "# (probably more stable than using 500-d which is what PCA needs to marginally beat UMAP)\n",
    "BEST_PARAMS_v2[\"KMeans\"][\"clip_RN50\"].update({\"dim_reducer\": None, \"ndim_reduced\": None, \"zscore\": False, \"pca_variance\": None})\n",
    "BEST_PARAMS_v2[\"KMeans\"][\"clip_RN50\"].update({\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50})\n",
    "# Changed KMeans MAE from PCA 85% to PCA 200\n",
    "# (since we see perf above plateaus at 200-d, there is no point going above that)\n",
    "BEST_PARAMS_v2[\"KMeans\"][\"timm_vit_base_patch16_224.mae\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"zscore\": True, \"ndim_reduced\": 200, \"pca_variance\": None}\n",
    ")\n",
    "# Changed KMeans clip_vitb16 from PCA 500 to PCA 75%\n",
    "# (gives a notably better train set AMI measurement above)\n",
    "BEST_PARAMS_v2[\"KMeans\"][\"clip_vitb16\"].update({\"dim_reducer\": \"PCA\", \"zscore\": True, \"pca_variance\": 0.75, \"ndim_reduced\": None})\n",
    "\n",
    "# Changed AffinityPropagation dino_resnet50 from PCA 95% to PCA 10\n",
    "# (performance is basically equal, so no point using higher-dim space;\n",
    "# could have done UMAP 50 instead with basically equal train AMI to PCA 10,\n",
    "# but didn't for consistency with other models)\n",
    "BEST_PARAMS_v2[\"AffinityPropagation\"][\"dino_resnet50\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"zscore\": True, \"ndim_reduced\": 10, \"pca_variance\": None}\n",
    ")\n",
    "# Changed AffinityPropagation MAE from PCA 95% to PCA 100\n",
    "BEST_PARAMS_v2[\"AffinityPropagation\"][\"timm_vit_base_patch16_224.mae\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"zscore\": True, \"ndim_reduced\": 100, \"pca_variance\": None}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Updating dim choices to use Affinity Prop dim results found with 0.9 damping,\"\n",
    "    \" prefering PCA reduction by percentage variance explained\"\n",
    ")\n",
    "BEST_PARAMS_v3 = {clusterer: {model: copy.deepcopy(DEFAULT_PARAMS[clusterer]) for model in ALL_MODELS} for clusterer in ALL_CLUSTERERS}\n",
    "BEST_PARAMS_v3[\"_version\"] = \"v3.0\"\n",
    "\n",
    "# KMeans\n",
    "for model in RESNET50_MODELS + VITB16_MODELS + FT_MODELS:\n",
    "    if model == \"none\" or model.startswith(\"random\") or model.startswith(\"clip\") or model == \"timm_vit_base_patch16_224.mae\":\n",
    "        continue\n",
    "    BEST_PARAMS_v3[\"KMeans\"][model].update({\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50})\n",
    "\n",
    "BEST_PARAMS_v3[\"KMeans\"][\"none\"].update({\"image_size\": 32, \"dim_reducer\": \"PCA\", \"pca_variance\": 0.98, \"zscore\": True})\n",
    "BEST_PARAMS_v3[\"KMeans\"][\"random_resnet50\"].update({\"dim_reducer\": \"PCA\", \"pca_variance\": 0.95, \"zscore\": True})\n",
    "BEST_PARAMS_v3[\"KMeans\"][\"random_vitb16\"].update({\"dim_reducer\": \"PCA\", \"ndim_reduced\": 100, \"zscore\": True})\n",
    "\n",
    "BEST_PARAMS_v3[\"KMeans\"][\"clip_RN50\"].update({\"dim_reducer\": \"PCA\", \"pca_variance\": 0.85, \"zscore\": True})\n",
    "BEST_PARAMS_v3[\"KMeans\"][\"clip_vitb16\"].update({\"dim_reducer\": \"PCA\", \"pca_variance\": 0.75, \"zscore\": True})\n",
    "BEST_PARAMS_v3[\"KMeans\"][\"timm_vit_base_patch16_224.mae\"].update({\"dim_reducer\": \"PCA\", \"pca_variance\": 0.85, \"zscore\": True})\n",
    "\n",
    "# AffinityPropagation\n",
    "for model in ALL_MODELS:\n",
    "    BEST_PARAMS_v3[\"AffinityPropagation\"][model].update({\"affinity_damping\": 0.9})\n",
    "\n",
    "for model in [\n",
    "    \"resnet50\",\n",
    "    \"clip_RN50\",\n",
    "    \"vitb16\",\n",
    "    \"mocov3_vit_base\",\n",
    "    \"mae_pretrain_vit_base_global\",\n",
    "    \"dino_vitb16\",\n",
    "    \"clip_vitb16\",\n",
    "] + FT_MODELS:\n",
    "    BEST_PARAMS_v3[\"AffinityPropagation\"][model].update({\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50})\n",
    "for model in [\"mocov3_resnet50\", \"vicreg_resnet50\", \"dino_resnet50\"]:\n",
    "    BEST_PARAMS_v3[\"AffinityPropagation\"][model].update(\n",
    "        {\n",
    "            \"dim_reducer_man\": \"PaCMAP\",\n",
    "            \"ndim_reduced_man\": 50,\n",
    "            \"dim_reducer_man_nn\": None,\n",
    "        }\n",
    "    )\n",
    "\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"none\"].update({\"image_size\": 32, \"dim_reducer\": \"PCA\", \"pca_variance\": 0.8, \"zscore\": True})\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"random_resnet50\"].update({\"dim_reducer\": \"PCA\", \"pca_variance\": 0.99, \"zscore\": True})\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"random_vitb16\"].update({\"dim_reducer\": \"PCA\", \"pca_variance\": 0.98, \"zscore\": True})\n",
    "\n",
    "BEST_PARAMS_v3[\"KMeans\"][\"timm_vit_base_patch16_224.mae\"].update({\"dim_reducer\": \"PCA\", \"pca_variance\": 0.99, \"zscore\": True})\n",
    "\n",
    "# AgglomerativeClustering\n",
    "for model in ALL_MODELS:\n",
    "    if model == \"none\" or model.startswith(\"random\") or model == \"timm_vit_base_patch16_224.mae\":\n",
    "        continue\n",
    "    BEST_PARAMS_v3[\"AgglomerativeClustering\"][model].update({\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50, \"dim_reducer\": \"None\"})\n",
    "\n",
    "BEST_PARAMS_v3[\"AgglomerativeClustering\"][\"none\"].update({\"image_size\": 32, \"dim_reducer\": \"PCA\", \"pca_variance\": 0.75, \"zscore\": True})\n",
    "BEST_PARAMS_v3[\"AgglomerativeClustering\"][\"random_resnet50\"].update({\"dim_reducer\": \"PCA\", \"pca_variance\": 0.98, \"zscore\": True})\n",
    "BEST_PARAMS_v3[\"AgglomerativeClustering\"][\"random_vitb16\"].update({\"dim_reducer\": \"PCA\", \"pca_variance\": 0.85, \"zscore\": True})\n",
    "BEST_PARAMS_v3[\"AgglomerativeClustering\"][\"timm_vit_base_patch16_224.mae\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"pca_variance\": 0.98, \"zscore\": True}\n",
    ")\n",
    "\n",
    "# HDBSCAN\n",
    "for model in ALL_MODELS:\n",
    "    if model in [\"timm_vit_base_patch16_224.mae\"]:\n",
    "        continue\n",
    "    BEST_PARAMS_v3[\"HDBSCAN\"][model].update({\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50, \"dim_reducer\": \"None\"})\n",
    "\n",
    "BEST_PARAMS_v3[\"HDBSCAN\"][\"none\"].update({\"image_size\": 32})\n",
    "BEST_PARAMS_v3[\"HDBSCAN\"][\"timm_vit_base_patch16_224.mae\"].update({\"dim_reducer\": \"PCA\", \"pca_variance\": 0.95, \"zscore\": True})\n",
    "\n",
    "# OPTICS - TODO\n",
    "# Use UMAP for every encoder, no exceptions necessary (not checked raw or random)\n",
    "for model in ALL_MODELS:\n",
    "    BEST_PARAMS_v3[\"OPTICS\"][model].update({\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50, \"dim_reducer\": \"None\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Updating dim choices to use Affinity Prop dim results found with 0.9 damping,\" \" stop PCA at 95%\")\n",
    "BEST_PARAMS_v4 = {clusterer: {model: copy.deepcopy(DEFAULT_PARAMS[clusterer]) for model in ALL_MODELS} for clusterer in ALL_CLUSTERERS}\n",
    "BEST_PARAMS_v4[\"_version\"] = \"v4.0\"\n",
    "for clusterer in BEST_PARAMS_v4:\n",
    "    if clusterer.startswith(\"_\"):\n",
    "        continue\n",
    "    BEST_PARAMS_v4[clusterer][\"none\"].update({\"image_size\": 32})\n",
    "\n",
    "# KMeans\n",
    "for model in RESNET50_MODELS + VITB16_MODELS + FT_MODELS:\n",
    "    if (\n",
    "        model == \"none\"\n",
    "        or model.startswith(\"random\")\n",
    "        or model.startswith(\"clip\")\n",
    "        or model == \"timm_vit_base_patch16_224.mae\"\n",
    "        or model == \"mae_pretrain_vit_base_global\"\n",
    "    ):\n",
    "        continue\n",
    "    BEST_PARAMS_v4[\"KMeans\"][model].update({\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50})\n",
    "\n",
    "BEST_PARAMS_v4[\"KMeans\"][\"none\"].update({\"dim_reducer\": \"PCA\", \"pca_variance\": 0.90, \"zscore\": True})\n",
    "BEST_PARAMS_v4[\"KMeans\"][\"random_resnet50\"].update({\"dim_reducer\": \"PCA\", \"pca_variance\": 0.95, \"zscore\": True})\n",
    "BEST_PARAMS_v4[\"KMeans\"][\"random_vitb16\"].update({\"dim_reducer\": \"PCA\", \"ndim_reduced\": 100, \"zscore\": True})\n",
    "\n",
    "BEST_PARAMS_v4[\"KMeans\"][\"clip_RN50\"].update({\"dim_reducer\": \"PCA\", \"pca_variance\": 0.85, \"zscore\": True})\n",
    "BEST_PARAMS_v4[\"KMeans\"][\"clip_vitb16\"].update({\"dim_reducer\": \"PCA\", \"pca_variance\": 0.75, \"zscore\": True})\n",
    "BEST_PARAMS_v4[\"KMeans\"][\"timm_vit_base_patch16_224.mae\"].update({\"dim_reducer\": \"PCA\", \"pca_variance\": 0.95, \"zscore\": True})\n",
    "BEST_PARAMS_v4[\"KMeans\"][\"mae_pretrain_vit_base_global\"].update({\"dim_reducer\": \"PCA\", \"pca_variance\": 0.9, \"zscore\": True})\n",
    "\n",
    "# AffinityPropagation\n",
    "for model in ALL_MODELS:\n",
    "    BEST_PARAMS_v4[\"AffinityPropagation\"][model].update({\"affinity_damping\": 0.9})\n",
    "\n",
    "for model in [\n",
    "    \"resnet50\",\n",
    "    \"clip_RN50\",\n",
    "    \"vitb16\",\n",
    "    \"mocov3_vit_base\",\n",
    "    \"mae_pretrain_vit_base_global\",\n",
    "    \"dino_vitb16\",\n",
    "    \"clip_vitb16\",\n",
    "] + FT_MODELS:\n",
    "    BEST_PARAMS_v4[\"AffinityPropagation\"][model].update({\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50})\n",
    "for model in [\"mocov3_resnet50\", \"vicreg_resnet50\", \"dino_resnet50\"]:\n",
    "    # tbc\n",
    "    BEST_PARAMS_v4[\"AffinityPropagation\"][model].update({\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50, \"dim_reducer_man_nn\": None})\n",
    "\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"none\"].update({\"dim_reducer\": \"PCA\", \"pca_variance\": 0.8, \"zscore\": True})\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"random_resnet50\"].update({\"dim_reducer\": \"PCA\", \"pca_variance\": 0.9, \"zscore\": True})\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"random_vitb16\"].update({\"dim_reducer\": \"PCA\", \"pca_variance\": 0.9, \"zscore\": True})\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"timm_vit_base_patch16_224.mae\"].update({\"dim_reducer\": \"PCA\", \"ndim_reduced\": 200, \"zscore\": True})\n",
    "\n",
    "# AgglomerativeClustering\n",
    "for model in ALL_MODELS:\n",
    "    if model == \"none\" or model.startswith(\"random\") or model == \"timm_vit_base_patch16_224.mae\" or model == \"mae_pretrain_vit_base_global\":\n",
    "        continue\n",
    "    BEST_PARAMS_v4[\"AgglomerativeClustering\"][model].update({\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50, \"dim_reducer\": \"None\"})\n",
    "\n",
    "BEST_PARAMS_v4[\"AgglomerativeClustering\"][\"none\"].update({\"dim_reducer\": \"PCA\", \"ndim_reduced\": 200, \"zscore\": True})\n",
    "BEST_PARAMS_v4[\"AgglomerativeClustering\"][\"random_resnet50\"].update({\"dim_reducer\": \"PCA\", \"ndim_reduced\": 200, \"zscore\": True})\n",
    "BEST_PARAMS_v4[\"AgglomerativeClustering\"][\"random_vitb16\"].update({\"dim_reducer\": \"PCA\", \"pca_variance\": 0.85, \"zscore\": True})\n",
    "BEST_PARAMS_v4[\"AgglomerativeClustering\"][\"timm_vit_base_patch16_224.mae\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"pca_variance\": 0.90, \"zscore\": True}\n",
    ")\n",
    "BEST_PARAMS_v4[\"AgglomerativeClustering\"][\"mae_pretrain_vit_base_global\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"pca_variance\": 0.85, \"zscore\": True}\n",
    ")\n",
    "\n",
    "# HDBSCAN\n",
    "for model in ALL_MODELS:\n",
    "    if model in [\"timm_vit_base_patch16_224.mae\"]:\n",
    "        continue\n",
    "    BEST_PARAMS_v4[\"HDBSCAN\"][model].update({\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50, \"dim_reducer\": \"None\"})\n",
    "\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"timm_vit_base_patch16_224.mae\"].update({\"dim_reducer\": \"PCA\", \"pca_variance\": 0.95, \"zscore\": True})\n",
    "\n",
    "# OPTICS - TODO\n",
    "# Use UMAP for every encoder, no exceptions necessary (not checked raw or random)\n",
    "for model in ALL_MODELS:\n",
    "    BEST_PARAMS_v4[\"OPTICS\"][model].update({\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50, \"dim_reducer\": \"None\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_PARAMS_v5 = copy.deepcopy(BEST_PARAMS_v4)\n",
    "BEST_PARAMS_v5[\"_version\"] = \"v5.0\"\n",
    "\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"none\"].update({\"zscore\": False, \"dim_reducer\": \"None\", \"dim_reducer_man\": \"None\"})\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"random_resnet50\"].update(\n",
    "    {\n",
    "        \"zscore\": True,\n",
    "        \"dim_reducer\": \"PCA\",\n",
    "        \"dim_reducer_man\": \"None\",\n",
    "        \"ndim_reduced\": 200,\n",
    "    }\n",
    ")\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"resnet50\"].update({\"zscore\": False, \"dim_reducer\": \"None\", \"dim_reducer_man\": \"None\"})\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"mocov3_resnet50\"].update({\"zscore\": False, \"dim_reducer\": \"None\", \"dim_reducer_man\": \"None\"})\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"dino_resnet50\"].update(\n",
    "    {\n",
    "        \"zscore\": True,\n",
    "        \"dim_reducer\": \"PCA\",\n",
    "        \"dim_reducer_man\": \"None\",\n",
    "        \"pca_variance\": 0.8,\n",
    "    }\n",
    ")\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"vicreg_resnet50\"].update({\"zscore\": False, \"dim_reducer\": \"None\", \"dim_reducer_man\": \"None\"})\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"clip_RN50\"].update(\n",
    "    {\n",
    "        \"zscore\": True,\n",
    "        \"dim_reducer\": \"PCA\",\n",
    "        \"dim_reducer_man\": \"None\",\n",
    "        \"pca_variance\": 0.9,\n",
    "    }\n",
    ")\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"random_vitb16\"].update(\n",
    "    {\n",
    "        \"zscore\": True,\n",
    "        \"dim_reducer\": \"PCA\",\n",
    "        \"dim_reducer_man\": \"None\",\n",
    "        \"pca_variance\": 0.95,\n",
    "    }\n",
    ")\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"vitb16\"].update(\n",
    "    {\n",
    "        \"zscore\": True,\n",
    "        \"dim_reducer\": \"PCA\",\n",
    "        \"dim_reducer_man\": \"None\",\n",
    "        \"pca_variance\": 0.7,\n",
    "    }\n",
    ")\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"mocov3_vit_base\"].update(\n",
    "    {\n",
    "        \"zscore\": True,\n",
    "        \"dim_reducer\": \"PCA\",\n",
    "        \"dim_reducer_man\": \"None\",\n",
    "        \"pca_variance\": 0.85,\n",
    "    }\n",
    ")\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"dino_vitb16\"].update(\n",
    "    {\n",
    "        \"zscore\": True,\n",
    "        \"dim_reducer\": \"PCA\",\n",
    "        \"dim_reducer_man\": \"None\",\n",
    "        \"pca_variance\": 0.9,\n",
    "    }\n",
    ")\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"timm_vit_base_patch16_224.mae\"].update(\n",
    "    {\"zscore\": True, \"dim_reducer\": \"None\", \"dim_reducer_man\": \"None\"}\n",
    ")\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"mae_pretrain_vit_base_global\"].update(\n",
    "    {\"zscore\": True, \"dim_reducer\": \"None\", \"dim_reducer_man\": \"None\"}\n",
    ")\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"clip_vitb16\"].update(\n",
    "    {\n",
    "        \"zscore\": True,\n",
    "        \"dim_reducer\": \"PCA\",\n",
    "        \"dim_reducer_man\": \"None\",\n",
    "        \"pca_variance\": 0.7,\n",
    "    }\n",
    ")\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"ft_mocov3_resnet50\"].update({\"zscore\": False, \"dim_reducer\": \"None\", \"dim_reducer_man\": \"None\"})\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"ft_dino_resnet50\"].update(\n",
    "    {\n",
    "        \"zscore\": True,\n",
    "        \"dim_reducer\": \"PCA\",\n",
    "        \"dim_reducer_man\": \"None\",\n",
    "        \"pca_variance\": 0.8,\n",
    "    }\n",
    ")\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"ft_vicreg_resnet50\"].update({\"zscore\": False, \"dim_reducer\": \"None\", \"dim_reducer_man\": \"None\"})\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"ft_mocov3_vit_base\"].update(\n",
    "    {\n",
    "        \"zscore\": True,\n",
    "        \"dim_reducer\": \"PCA\",\n",
    "        \"dim_reducer_man\": \"None\",\n",
    "        \"pca_variance\": 0.95,\n",
    "    }\n",
    ")\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"ft_dino_vitb16\"].update(\n",
    "    {\n",
    "        \"zscore\": True,\n",
    "        \"dim_reducer\": \"PCA\",\n",
    "        \"dim_reducer_man\": \"None\",\n",
    "        \"pca_variance\": 0.9,\n",
    "    }\n",
    ")\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"mae_finetuned_vit_base_global\"].update(\n",
    "    {\n",
    "        \"zscore\": True,\n",
    "        \"dim_reducer\": \"PCA\",\n",
    "        \"dim_reducer_man\": \"None\",\n",
    "        \"pca_variance\": 0.75,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"none\"].update({\"zscore\": False, \"dim_reducer\": \"None\", \"dim_reducer_man\": \"None\"})\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"random_resnet50\"].update(\n",
    "    {\n",
    "        \"zscore\": True,\n",
    "        \"dim_reducer\": \"PCA\",\n",
    "        \"dim_reducer_man\": \"None\",\n",
    "        \"pca_variance\": 0.7,\n",
    "    }\n",
    ")\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"resnet50\"].update({\"dim_reducer\": \"None\", \"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50})\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"mocov3_resnet50\"].update({\"dim_reducer\": \"None\", \"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50})\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"dino_resnet50\"].update(\n",
    "    {\n",
    "        \"zscore\": True,\n",
    "        \"dim_reducer\": \"PCA\",\n",
    "        \"dim_reducer_man\": \"None\",\n",
    "        \"pca_variance\": 0.75,\n",
    "    }\n",
    ")\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"vicreg_resnet50\"].update(\n",
    "    {\n",
    "        \"zscore\": True,\n",
    "        \"dim_reducer\": \"PCA\",\n",
    "        \"dim_reducer_man\": \"None\",\n",
    "        \"pca_variance\": 0.9,\n",
    "    }\n",
    ")\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"random_vitb16\"].update(\n",
    "    {\n",
    "        \"zscore\": True,\n",
    "        \"dim_reducer\": \"PCA\",\n",
    "        \"dim_reducer_man\": \"None\",\n",
    "        \"pca_variance\": 0.75,\n",
    "    }\n",
    ")\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"vitb16\"].update(\n",
    "    {\n",
    "        \"zscore\": True,\n",
    "        \"dim_reducer\": \"PCA\",\n",
    "        \"dim_reducer_man\": \"None\",\n",
    "        \"ndim_reduced\": 10,\n",
    "    }\n",
    ")\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"mocov3_vit_base\"].update(\n",
    "    {\n",
    "        \"zscore\": True,\n",
    "        \"dim_reducer\": \"PCA\",\n",
    "        \"dim_reducer_man\": \"None\",\n",
    "        \"pca_variance\": 0.75,\n",
    "    }\n",
    ")\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"dino_vitb16\"].update(\n",
    "    {\n",
    "        \"zscore\": True,\n",
    "        \"dim_reducer\": \"PCA\",\n",
    "        \"dim_reducer_man\": \"None\",\n",
    "        \"pca_variance\": 0.75,\n",
    "    }\n",
    ")\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"timm_vit_base_patch16_224.mae\"].update(\n",
    "    {\n",
    "        \"zscore\": True,\n",
    "        \"dim_reducer\": \"PCA\",\n",
    "        \"dim_reducer_man\": \"None\",\n",
    "        \"pca_variance\": 0.75,\n",
    "    }\n",
    ")\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"mae_pretrain_vit_base_global\"].update(\n",
    "    {\"zscore\": True, \"dim_reducer\": \"None\", \"dim_reducer_man\": \"None\"}\n",
    ")\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"ft_mocov3_resnet50\"].update(\n",
    "    {\"dim_reducer\": \"None\", \"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50}\n",
    ")\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"ft_dino_resnet50\"].update({\"dim_reducer\": \"None\", \"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50})\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"ft_vicreg_resnet50\"].update(\n",
    "    {\"dim_reducer\": \"None\", \"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50}\n",
    ")  # adjusted 20 -> 50\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"ft_mocov3_vit_base\"].update(\n",
    "    {\n",
    "        \"zscore\": True,\n",
    "        \"dim_reducer\": \"PCA\",\n",
    "        \"dim_reducer_man\": \"None\",\n",
    "        \"ndim_reduced\": 100,\n",
    "    }\n",
    ")\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"ft_dino_vitb16\"].update(\n",
    "    {\n",
    "        \"zscore\": True,\n",
    "        \"dim_reducer\": \"PCA\",\n",
    "        \"dim_reducer_man\": \"None\",\n",
    "        \"ndim_reduced\": 10,\n",
    "    }\n",
    ")\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"mae_finetuned_vit_base_global\"].update(\n",
    "    {\"dim_reducer\": \"None\", \"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50}\n",
    ")  # adjusted 10 -> 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9scRlYvoa9tr"
   },
   "source": [
    "### Agglomerative specific settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cv8_GiZ5PfDF"
   },
   "outputs": [],
   "source": [
    "for model in [\n",
    "    \"resnet50\",\n",
    "    \"mocov3_resnet50\",\n",
    "    \"vicreg_resnet50\",\n",
    "    \"vitb16\",\n",
    "    \"timm_vit_base_patch16_224.mae\",\n",
    "]:\n",
    "    BEST_PARAMS_v1[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"euclidean\",\n",
    "            \"aggclust_linkage\": \"ward\",\n",
    "        }\n",
    "    )\n",
    "for model in [\"dino_resnet50\", \"clip_RN50\", \"dino_vitb16\"]:\n",
    "    BEST_PARAMS_v1[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"euclidean\",\n",
    "            \"aggclust_linkage\": \"average\",\n",
    "        }\n",
    "    )\n",
    "for model in [\"mocov3_vit_base\", \"clip_vitb16\"]:\n",
    "    BEST_PARAMS_v1[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"chebyshev\",\n",
    "            \"aggclust_linkage\": \"average\",\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4CouZvuSjPvc"
   },
   "outputs": [],
   "source": [
    "# vicreg_resnet50 is the only change from v1 to v2\n",
    "for model in [\"resnet50\", \"mocov3_resnet50\", \"vitb16\", \"timm_vit_base_patch16_224.mae\"]:\n",
    "    BEST_PARAMS_v2[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"euclidean\",\n",
    "            \"aggclust_linkage\": \"ward\",\n",
    "        }\n",
    "    )\n",
    "for model in [\"vicreg_resnet50\", \"dino_resnet50\", \"clip_RN50\", \"dino_vitb16\"]:\n",
    "    BEST_PARAMS_v2[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"euclidean\",\n",
    "            \"aggclust_linkage\": \"average\",\n",
    "        }\n",
    "    )\n",
    "for model in [\"mocov3_vit_base\", \"clip_vitb16\"]:\n",
    "    BEST_PARAMS_v2[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"chebyshev\",\n",
    "            \"aggclust_linkage\": \"average\",\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4CouZvuSjPvc"
   },
   "outputs": [],
   "source": [
    "for model in [\"none\", \"resnet50\", \"mocov3_resnet50\", \"vitb16\"] + FT_MODELS:\n",
    "    BEST_PARAMS_v3[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"euclidean\",\n",
    "            \"aggclust_linkage\": \"ward\",\n",
    "        }\n",
    "    )\n",
    "for model in [\"vicreg_resnet50\", \"dino_resnet50\", \"clip_RN50\", \"dino_vitb16\"]:\n",
    "    BEST_PARAMS_v3[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"euclidean\",\n",
    "            \"aggclust_linkage\": \"average\",\n",
    "        }\n",
    "    )\n",
    "for model in [\"mocov3_vit_base\", \"clip_vitb16\", \"random_resnet50\", \"random_vitb16\"]:\n",
    "    BEST_PARAMS_v3[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"chebyshev\",\n",
    "            \"aggclust_linkage\": \"average\",\n",
    "        }\n",
    "    )\n",
    "for model in [\"timm_vit_base_patch16_224.mae\"]:\n",
    "    BEST_PARAMS_v3[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"cosine\",\n",
    "            \"aggclust_linkage\": \"average\",\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - mae_pretrain_vit_base_global\n",
    "# - clip_vitb16 (leaving as-is for now)\n",
    "for model in ALL_MODELS:\n",
    "    BEST_PARAMS_v4[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"tbd\",\n",
    "            \"aggclust_linkage\": \"tbd\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "for model in [\"resnet50\", \"mocov3_resnet50\", \"vitb16\"] + FT_MODELS:\n",
    "    BEST_PARAMS_v4[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"euclidean\",\n",
    "            \"aggclust_linkage\": \"ward\",\n",
    "        }\n",
    "    )\n",
    "for model in [\"vicreg_resnet50\", \"dino_resnet50\", \"clip_RN50\", \"dino_vitb16\"]:\n",
    "    BEST_PARAMS_v4[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"euclidean\",\n",
    "            \"aggclust_linkage\": \"average\",\n",
    "        }\n",
    "    )\n",
    "for model in [\"mocov3_vit_base\", \"clip_vitb16\", \"random_resnet50\", \"random_vitb16\"]:\n",
    "    BEST_PARAMS_v4[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"chebyshev\",\n",
    "            \"aggclust_linkage\": \"average\",\n",
    "        }\n",
    "    )\n",
    "for model in [\"none\", \"timm_vit_base_patch16_224.mae\", \"mae_pretrain_vit_base_global\"]:\n",
    "    BEST_PARAMS_v4[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"cosine\",\n",
    "            \"aggclust_linkage\": \"average\",\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - mae_pretrain_vit_base_global\n",
    "# - clip_vitb16 (leaving as-is for now)\n",
    "for model in ALL_MODELS:\n",
    "    BEST_PARAMS_v5[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"tbd\",\n",
    "            \"aggclust_linkage\": \"tbd\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "for model in [\"resnet50\", \"mocov3_resnet50\", \"vitb16\"] + FT_MODELS:\n",
    "    BEST_PARAMS_v5[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"euclidean\",\n",
    "            \"aggclust_linkage\": \"ward\",\n",
    "        }\n",
    "    )\n",
    "for model in [\"vicreg_resnet50\", \"dino_resnet50\", \"clip_RN50\", \"dino_vitb16\"]:\n",
    "    BEST_PARAMS_v5[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"euclidean\",\n",
    "            \"aggclust_linkage\": \"average\",\n",
    "        }\n",
    "    )\n",
    "for model in [\"mocov3_vit_base\", \"clip_vitb16\", \"random_resnet50\", \"random_vitb16\"]:\n",
    "    BEST_PARAMS_v5[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"chebyshev\",\n",
    "            \"aggclust_linkage\": \"average\",\n",
    "        }\n",
    "    )\n",
    "for model in [\"none\", \"timm_vit_base_patch16_224.mae\", \"mae_pretrain_vit_base_global\"]:\n",
    "    BEST_PARAMS_v5[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"cosine\",\n",
    "            \"aggclust_linkage\": \"average\",\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Thn1g4rpeoo"
   },
   "outputs": [],
   "source": [
    "BEST_PARAMS_v1[\"AC w/ C\"] = copy.deepcopy(BEST_PARAMS_v1[\"AgglomerativeClustering\"])\n",
    "BEST_PARAMS_v1[\"AC w/o C\"] = copy.deepcopy(BEST_PARAMS_v1[\"AgglomerativeClustering\"])\n",
    "BEST_PARAMS_v2[\"AC w/ C\"] = copy.deepcopy(BEST_PARAMS_v2[\"AgglomerativeClustering\"])\n",
    "BEST_PARAMS_v2[\"AC w/o C\"] = copy.deepcopy(BEST_PARAMS_v2[\"AgglomerativeClustering\"])\n",
    "BEST_PARAMS_v3[\"AC w/ C\"] = copy.deepcopy(BEST_PARAMS_v3[\"AgglomerativeClustering\"])\n",
    "BEST_PARAMS_v3[\"AC w/o C\"] = copy.deepcopy(BEST_PARAMS_v3[\"AgglomerativeClustering\"])\n",
    "BEST_PARAMS_v4[\"AC w/ C\"] = copy.deepcopy(BEST_PARAMS_v4[\"AgglomerativeClustering\"])\n",
    "BEST_PARAMS_v4[\"AC w/o C\"] = copy.deepcopy(BEST_PARAMS_v4[\"AgglomerativeClustering\"])\n",
    "BEST_PARAMS_v5[\"AC w/ C\"] = copy.deepcopy(BEST_PARAMS_v5[\"AgglomerativeClustering\"])\n",
    "BEST_PARAMS_v5[\"AC w/o C\"] = copy.deepcopy(BEST_PARAMS_v5[\"AgglomerativeClustering\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Gixn5KGtuNr"
   },
   "outputs": [],
   "source": [
    "for model in BEST_PARAMS_v1[\"AC w/ C\"]:\n",
    "    BEST_PARAMS_v1[\"AC w/ C\"][model].update({\"aggclust_dist_thresh\": None})\n",
    "for model in BEST_PARAMS_v2[\"AC w/ C\"]:\n",
    "    BEST_PARAMS_v2[\"AC w/ C\"][model].update({\"aggclust_dist_thresh\": None})\n",
    "for model in BEST_PARAMS_v3[\"AC w/ C\"]:\n",
    "    BEST_PARAMS_v3[\"AC w/ C\"][model].update({\"aggclust_dist_thresh\": None})\n",
    "for model in BEST_PARAMS_v4[\"AC w/ C\"]:\n",
    "    BEST_PARAMS_v4[\"AC w/ C\"][model].update({\"aggclust_dist_thresh\": None})\n",
    "for model in BEST_PARAMS_v5[\"AC w/ C\"]:\n",
    "    BEST_PARAMS_v5[\"AC w/ C\"][model].update({\"aggclust_dist_thresh\": None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7EqXneH6kv27"
   },
   "outputs": [],
   "source": [
    "for model in BEST_PARAMS_v2[\"AC w/o C\"]:\n",
    "    BEST_PARAMS_v2[\"AC w/o C\"][model].update({\"zscore2\": \"average\", \"ndim_correction\": True})\n",
    "for model in BEST_PARAMS_v3[\"AC w/o C\"]:\n",
    "    BEST_PARAMS_v3[\"AC w/o C\"][model].update({\"zscore2\": \"average\", \"ndim_correction\": True})\n",
    "for model in BEST_PARAMS_v4[\"AC w/o C\"]:\n",
    "    BEST_PARAMS_v4[\"AC w/o C\"][model].update({\"zscore2\": \"average\", \"ndim_correction\": True})\n",
    "for model in BEST_PARAMS_v5[\"AC w/o C\"]:\n",
    "    BEST_PARAMS_v5[\"AC w/o C\"][model].update({\"zscore2\": \"average\", \"ndim_correction\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yrmctdctSRgu"
   },
   "outputs": [],
   "source": [
    "# Run AgglomerativeClustering experiments with number of clusters unknown\n",
    "# \tresnet50        \t20.0\n",
    "# \tmocov3_resnet50 \t20.0\n",
    "# \tvicreg_resnet50 \t20.0\n",
    "# \tvitb16 \t            20.0\n",
    "# \tdino_resnet50     \t 1.0\n",
    "# \tclip_RN50 \t         1.0\n",
    "# \tdino_vitb16 \t     2.0\n",
    "# \tmocov3_vit_base \t 1.0\n",
    "# \tclip_vitb16 \t     0.5\n",
    "# \ttimm_vit_base_patch16_224.mae \t200.0\n",
    "\n",
    "for model in [\"resnet50\", \"mocov3_resnet50\", \"vicreg_resnet50\", \"vitb16\"]:\n",
    "    BEST_PARAMS_v1[\"AC w/o C\"][model].update({\"aggclust_dist_thresh\": 20.0})\n",
    "for model in [\"dino_resnet50\", \"clip_RN50\", \"mocov3_vit_base\"]:\n",
    "    BEST_PARAMS_v1[\"AC w/o C\"][model].update({\"aggclust_dist_thresh\": 1.0})\n",
    "BEST_PARAMS_v1[\"AC w/o C\"][\"dino_vitb16\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v1[\"AC w/o C\"][\"clip_vitb16\"][\"aggclust_dist_thresh\"] = 0.5\n",
    "BEST_PARAMS_v1[\"AC w/o C\"][\"timm_vit_base_patch16_224.mae\"][\"aggclust_dist_thresh\"] = 200.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nEW3rz_5aQ70"
   },
   "outputs": [],
   "source": [
    "BEST_PARAMS_v2[\"AC w/o C\"][\"resnet50\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v2[\"AC w/o C\"][\"mocov3_resnet50\"][\"aggclust_dist_thresh\"] = 10.0\n",
    "BEST_PARAMS_v2[\"AC w/o C\"][\"vicreg_resnet50\"][\"aggclust_dist_thresh\"] = 0.5\n",
    "BEST_PARAMS_v2[\"AC w/o C\"][\"dino_resnet50\"][\"aggclust_dist_thresh\"] = 0.5\n",
    "BEST_PARAMS_v2[\"AC w/o C\"][\"clip_RN50\"][\"aggclust_dist_thresh\"] = 0.5\n",
    "BEST_PARAMS_v2[\"AC w/o C\"][\"vitb16\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v2[\"AC w/o C\"][\"mocov3_vit_base\"][\"aggclust_dist_thresh\"] = 1.0\n",
    "BEST_PARAMS_v2[\"AC w/o C\"][\"timm_vit_base_patch16_224.mae\"][\"aggclust_dist_thresh\"] = 5.0\n",
    "BEST_PARAMS_v2[\"AC w/o C\"][\"dino_vitb16\"][\"aggclust_dist_thresh\"] = 0.2\n",
    "BEST_PARAMS_v2[\"AC w/o C\"][\"clip_vitb16\"][\"aggclust_dist_thresh\"] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_PARAMS_v3[\"AC w/o C\"][\"none\"][\"aggclust_dist_thresh\"] = 10.0\n",
    "BEST_PARAMS_v3[\"AC w/o C\"][\"random_resnet50\"][\"aggclust_dist_thresh\"] = 10.0\n",
    "BEST_PARAMS_v3[\"AC w/o C\"][\"resnet50\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v3[\"AC w/o C\"][\"mocov3_resnet50\"][\"aggclust_dist_thresh\"] = 10.0\n",
    "BEST_PARAMS_v3[\"AC w/o C\"][\"dino_resnet50\"][\"aggclust_dist_thresh\"] = 0.5\n",
    "BEST_PARAMS_v3[\"AC w/o C\"][\"vicreg_resnet50\"][\"aggclust_dist_thresh\"] = 0.5\n",
    "BEST_PARAMS_v3[\"AC w/o C\"][\"clip_RN50\"][\"aggclust_dist_thresh\"] = 0.5\n",
    "BEST_PARAMS_v3[\"AC w/o C\"][\"random_vitb16\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v3[\"AC w/o C\"][\"vitb16\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v3[\"AC w/o C\"][\"mocov3_vit_base\"][\"aggclust_dist_thresh\"] = 1.0\n",
    "BEST_PARAMS_v3[\"AC w/o C\"][\"dino_vitb16\"][\"aggclust_dist_thresh\"] = 0.2\n",
    "BEST_PARAMS_v3[\"AC w/o C\"][\"timm_vit_base_patch16_224.mae\"][\"aggclust_dist_thresh\"] = 0.5\n",
    "BEST_PARAMS_v3[\"AC w/o C\"][\"clip_vitb16\"][\"aggclust_dist_thresh\"] = 1.0\n",
    "BEST_PARAMS_v3[\"AC w/o C\"][\"ft_mocov3_resnet50\"][\"aggclust_dist_thresh\"] = 1.0\n",
    "BEST_PARAMS_v3[\"AC w/o C\"][\"ft_dino_resnet50\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v3[\"AC w/o C\"][\"ft_vicreg_resnet50\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v3[\"AC w/o C\"][\"ft_mocov3_vit_base\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v3[\"AC w/o C\"][\"ft_dino_vitb16\"][\"aggclust_dist_thresh\"] = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - none\n",
    "# - random_resnet50\n",
    "# - timm_vit_base_patch16_224.mae\n",
    "# - mae_pretrain_vit_base_global\n",
    "# - clip_vitb16 (leave as-is)\n",
    "# - ft_mocov3_resnet50 (tbc)\n",
    "# - mae_finetuned_vit_base_global\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"resnet50\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"mocov3_resnet50\"][\"aggclust_dist_thresh\"] = 10.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"dino_resnet50\"][\"aggclust_dist_thresh\"] = 0.5\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"vicreg_resnet50\"][\"aggclust_dist_thresh\"] = 0.5\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"random_vitb16\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"vitb16\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"mocov3_vit_base\"][\"aggclust_dist_thresh\"] = 1.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"dino_vitb16\"][\"aggclust_dist_thresh\"] = 0.2\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"ft_mocov3_resnet50\"][\"aggclust_dist_thresh\"] = 2.0  # tbc\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"ft_dino_resnet50\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"ft_vicreg_resnet50\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"ft_mocov3_vit_base\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"ft_dino_vitb16\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"_version\"] = \"v4.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - none\n",
    "# - timm_vit_base_patch16_224.mae (tbc)\n",
    "# - mae_pretrain_vit_base_global\n",
    "# - clip_vitb16 (leave as-is)\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"random_resnet50\"][\"aggclust_dist_thresh\"] = 10.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"resnet50\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"mocov3_resnet50\"][\"aggclust_dist_thresh\"] = 10.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"dino_resnet50\"][\"aggclust_dist_thresh\"] = 0.5\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"vicreg_resnet50\"][\"aggclust_dist_thresh\"] = 0.5\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"clip_RN50\"][\"aggclust_dist_thresh\"] = 0.5\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"random_vitb16\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"vitb16\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"mocov3_vit_base\"][\"aggclust_dist_thresh\"] = 1.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"dino_vitb16\"][\"aggclust_dist_thresh\"] = 0.2\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"timm_vit_base_patch16_224.mae\"][\"aggclust_dist_thresh\"] = 0.5  # tbc\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"clip_vitb16\"][\"aggclust_dist_thresh\"] = 1.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"ft_mocov3_resnet50\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"ft_dino_resnet50\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"ft_vicreg_resnet50\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"ft_mocov3_vit_base\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"ft_dino_vitb16\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"mae_finetuned_vit_base_global\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"_version\"] = \"v4.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v4.4\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"none\"][\"aggclust_dist_thresh\"] = 0.71\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"random_resnet50\"][\"aggclust_dist_thresh\"] = 10.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"resnet50\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"mocov3_resnet50\"][\"aggclust_dist_thresh\"] = 10.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"dino_resnet50\"][\"aggclust_dist_thresh\"] = 0.5\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"vicreg_resnet50\"][\"aggclust_dist_thresh\"] = 0.5\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"clip_RN50\"][\"aggclust_dist_thresh\"] = 0.5\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"random_vitb16\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"vitb16\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"mocov3_vit_base\"][\"aggclust_dist_thresh\"] = 1.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"dino_vitb16\"][\"aggclust_dist_thresh\"] = 0.2\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"timm_vit_base_patch16_224.mae\"][\"aggclust_dist_thresh\"] = 0.71\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"mae_pretrain_vit_base_global\"][\"aggclust_dist_thresh\"] = 0.71\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"clip_vitb16\"][\"aggclust_dist_thresh\"] = 1.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"ft_mocov3_resnet50\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"ft_dino_resnet50\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"ft_vicreg_resnet50\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"ft_mocov3_vit_base\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"ft_dino_vitb16\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"mae_finetuned_vit_base_global\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"_version\"] = \"v4.4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v5.0\n",
    "BEST_PARAMS_v5[\"AC w/o C\"][\"none\"][\"aggclust_dist_thresh\"] = 0.71\n",
    "BEST_PARAMS_v5[\"AC w/o C\"][\"random_resnet50\"][\"aggclust_dist_thresh\"] = 10.0\n",
    "BEST_PARAMS_v5[\"AC w/o C\"][\"resnet50\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v5[\"AC w/o C\"][\"mocov3_resnet50\"][\"aggclust_dist_thresh\"] = 10.0\n",
    "BEST_PARAMS_v5[\"AC w/o C\"][\"dino_resnet50\"][\"aggclust_dist_thresh\"] = 0.5\n",
    "BEST_PARAMS_v5[\"AC w/o C\"][\"vicreg_resnet50\"][\"aggclust_dist_thresh\"] = 0.5\n",
    "BEST_PARAMS_v5[\"AC w/o C\"][\"clip_RN50\"][\"aggclust_dist_thresh\"] = 0.5\n",
    "BEST_PARAMS_v5[\"AC w/o C\"][\"random_vitb16\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v5[\"AC w/o C\"][\"vitb16\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v5[\"AC w/o C\"][\"mocov3_vit_base\"][\"aggclust_dist_thresh\"] = 1.0\n",
    "BEST_PARAMS_v5[\"AC w/o C\"][\"dino_vitb16\"][\"aggclust_dist_thresh\"] = 0.2\n",
    "BEST_PARAMS_v5[\"AC w/o C\"][\"timm_vit_base_patch16_224.mae\"][\"aggclust_dist_thresh\"] = 0.71\n",
    "BEST_PARAMS_v5[\"AC w/o C\"][\"mae_pretrain_vit_base_global\"][\"aggclust_dist_thresh\"] = 0.71\n",
    "BEST_PARAMS_v5[\"AC w/o C\"][\"clip_vitb16\"][\"aggclust_dist_thresh\"] = 1.0\n",
    "BEST_PARAMS_v5[\"AC w/o C\"][\"ft_mocov3_resnet50\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v5[\"AC w/o C\"][\"ft_dino_resnet50\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v5[\"AC w/o C\"][\"ft_vicreg_resnet50\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v5[\"AC w/o C\"][\"ft_mocov3_vit_base\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v5[\"AC w/o C\"][\"ft_dino_vitb16\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v5[\"AC w/o C\"][\"mae_finetuned_vit_base_global\"][\"aggclust_dist_thresh\"] = 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Affinity Prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in BEST_PARAMS_v1[\"AffinityPropagation\"]:\n",
    "    BEST_PARAMS_v1[\"AffinityPropagation\"][model][\"affinity_damping\"] = 0.5\n",
    "for model in BEST_PARAMS_v2[\"AffinityPropagation\"]:\n",
    "    BEST_PARAMS_v2[\"AffinityPropagation\"][model][\"affinity_damping\"] = 0.5\n",
    "for model in BEST_PARAMS_v3[\"AffinityPropagation\"]:\n",
    "    BEST_PARAMS_v3[\"AffinityPropagation\"][model][\"affinity_damping\"] = 0.9\n",
    "for model in BEST_PARAMS_v4[\"AffinityPropagation\"]:\n",
    "    BEST_PARAMS_v4[\"AffinityPropagation\"][model][\"affinity_damping\"] = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"none\"][\"affinity_damping\"] = 0.85\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"random_resnet50\"][\"affinity_damping\"] = 0.5\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"resnet50\"][\"affinity_damping\"] = 0.9\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"mocov3_resnet50\"][\"affinity_damping\"] = 0.8\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"dino_resnet50\"][\"affinity_damping\"] = 0.8\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"vicreg_resnet50\"][\"affinity_damping\"] = 0.75\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"clip_RN50\"][\"affinity_damping\"] = 0.85\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"random_vitb16\"][\"affinity_damping\"] = 0.7\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"vitb16\"][\"affinity_damping\"] = 0.9\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"mocov3_vit_base\"][\"affinity_damping\"] = 0.75\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"dino_vitb16\"][\"affinity_damping\"] = 0.85\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"timm_vit_base_patch16_224.mae\"][\"affinity_damping\"] = 0.5\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"mae_pretrain_vit_base_global\"][\"affinity_damping\"] = 0.9  # To match\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"clip_vitb16\"][\"affinity_damping\"] = 0.95\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"ft_mocov3_resnet50\"][\"affinity_damping\"] = 0.9  # Match supervised/ft resnet50\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"ft_vicreg_resnet50\"][\"affinity_damping\"] = 0.9\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"ft_dino_vitb16\"][\"affinity_damping\"] = 0.9\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"ft_mocov3_vit_base\"][\"affinity_damping\"] = 0.9  # Match supervised/ft resnet50\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"mae_finetuned_vit_base_global\"][\"affinity_damping\"] = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"none\"][\"affinity_damping\"] = 0.85\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"random_resnet50\"][\"affinity_damping\"] = 0.9\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"resnet50\"][\"affinity_damping\"] = 0.9\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"mocov3_resnet50\"][\"affinity_damping\"] = 0.75\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"dino_resnet50\"][\"affinity_damping\"] = 0.9\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"vicreg_resnet50\"][\"affinity_damping\"] = 0.8\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"clip_RN50\"][\"affinity_damping\"] = 0.85\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"random_vitb16\"][\"affinity_damping\"] = 0.95\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"vitb16\"][\"affinity_damping\"] = 0.9\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"mocov3_vit_base\"][\"affinity_damping\"] = 0.75\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"dino_vitb16\"][\"affinity_damping\"] = 0.85\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"timm_vit_base_patch16_224.mae\"][\"affinity_damping\"] = 0.6\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"mae_pretrain_vit_base_global\"][\"affinity_damping\"] = 0.6\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"clip_vitb16\"][\"affinity_damping\"] = 0.95\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"ft_mocov3_resnet50\"][\"affinity_damping\"] = 0.95\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"ft_dino_resnet50\"][\"affinity_damping\"] = 0.9\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"ft_vicreg_resnet50\"][\"affinity_damping\"] = 0.9\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"ft_mocov3_vit_base\"][\"affinity_damping\"] = 0.95\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"ft_dino_vitb16\"][\"affinity_damping\"] = 0.9\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"mae_finetuned_vit_base_global\"][\"affinity_damping\"] = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_PARAMS_v5[\"AffinityPropagation\"][\"none\"][\"affinity_damping\"] = 0.85\n",
    "BEST_PARAMS_v5[\"AffinityPropagation\"][\"random_resnet50\"][\"affinity_damping\"] = 0.9\n",
    "BEST_PARAMS_v5[\"AffinityPropagation\"][\"resnet50\"][\"affinity_damping\"] = 0.9\n",
    "BEST_PARAMS_v5[\"AffinityPropagation\"][\"mocov3_resnet50\"][\"affinity_damping\"] = 0.75\n",
    "BEST_PARAMS_v5[\"AffinityPropagation\"][\"dino_resnet50\"][\"affinity_damping\"] = 0.9\n",
    "BEST_PARAMS_v5[\"AffinityPropagation\"][\"vicreg_resnet50\"][\"affinity_damping\"] = 0.8\n",
    "BEST_PARAMS_v5[\"AffinityPropagation\"][\"clip_RN50\"][\"affinity_damping\"] = 0.85\n",
    "BEST_PARAMS_v5[\"AffinityPropagation\"][\"random_vitb16\"][\"affinity_damping\"] = 0.95\n",
    "BEST_PARAMS_v5[\"AffinityPropagation\"][\"vitb16\"][\"affinity_damping\"] = 0.9\n",
    "BEST_PARAMS_v5[\"AffinityPropagation\"][\"mocov3_vit_base\"][\"affinity_damping\"] = 0.75\n",
    "BEST_PARAMS_v5[\"AffinityPropagation\"][\"dino_vitb16\"][\"affinity_damping\"] = 0.85\n",
    "BEST_PARAMS_v5[\"AffinityPropagation\"][\"timm_vit_base_patch16_224.mae\"][\"affinity_damping\"] = 0.6\n",
    "BEST_PARAMS_v5[\"AffinityPropagation\"][\"mae_pretrain_vit_base_global\"][\"affinity_damping\"] = 0.6\n",
    "BEST_PARAMS_v5[\"AffinityPropagation\"][\"clip_vitb16\"][\"affinity_damping\"] = 0.95\n",
    "BEST_PARAMS_v5[\"AffinityPropagation\"][\"ft_mocov3_resnet50\"][\"affinity_damping\"] = 0.95\n",
    "BEST_PARAMS_v5[\"AffinityPropagation\"][\"ft_dino_resnet50\"][\"affinity_damping\"] = 0.9\n",
    "BEST_PARAMS_v5[\"AffinityPropagation\"][\"ft_vicreg_resnet50\"][\"affinity_damping\"] = 0.9\n",
    "BEST_PARAMS_v5[\"AffinityPropagation\"][\"ft_mocov3_vit_base\"][\"affinity_damping\"] = 0.95\n",
    "BEST_PARAMS_v5[\"AffinityPropagation\"][\"ft_dino_vitb16\"][\"affinity_damping\"] = 0.9\n",
    "BEST_PARAMS_v5[\"AffinityPropagation\"][\"mae_finetuned_vit_base_global\"][\"affinity_damping\"] = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KeuptU7acB0d"
   },
   "source": [
    "### HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHODpPwVL0FT"
   },
   "outputs": [],
   "source": [
    "for model in RESNET50_MODELS + VITB16_MODELS:\n",
    "    BEST_PARAMS_v1[\"HDBSCAN\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"euclidean\",\n",
    "            \"hdbscan_method\": \"eom\",\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZoUxa6TG03b9"
   },
   "source": [
    "v2 selection\n",
    "\n",
    "|    | model                         | distance_metric   | hdbscan_method   |      AMI |\n",
    "|---:|:------------------------------|:------------------|:-----------------|---------:|\n",
    "|  0 | resnet50                      | euclidean         | eom              | 0.828368 |\n",
    "|  1 | mocov3_resnet50               | euclidean         | eom              | 0.531644 |\n",
    "|  2 | vicreg_resnet50               | l1                | eom              | 0.472324 |\n",
    "|  3 | dino_resnet50                 | l1                | eom              | 0.503147 |\n",
    "|  4 | clip_RN50                     | l1                | eom              | 0.461363 |\n",
    "|  5 | vitb16                        | chebyshev         | eom              | 0.906110 |\n",
    "|  6 | mocov3_vit_base               | euclidean         | eom              | 0.629966 |\n",
    "|  7 | timm_vit_base_patch16_224.mae | euclidean         | eom              | 0.070495 |\n",
    "|  8 | dino_vitb16                   | l1                | eom              | 0.691547 |\n",
    "|  9 | clip_vitb16                   | l1                | eom              | 0.592489 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDMQjXpoKb6_"
   },
   "outputs": [],
   "source": [
    "for model in RESNET50_MODELS + VITB16_MODELS:\n",
    "    BEST_PARAMS_v2[\"HDBSCAN\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"euclidean\",\n",
    "            \"hdbscan_method\": \"eom\",\n",
    "        }\n",
    "    )\n",
    "for model in [\n",
    "    \"vicreg_resnet50\",\n",
    "    \"dino_resnet50\",\n",
    "    \"clip_RN50\",\n",
    "    \"dino_vitb16\",\n",
    "    \"clip_vitb16\",\n",
    "]:\n",
    "    BEST_PARAMS_v2[\"HDBSCAN\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"l1\",\n",
    "        }\n",
    "    )\n",
    "BEST_PARAMS_v2[\"HDBSCAN\"][\"vitb16\"][\"distance_metric\"] = \"chebyshev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nKYseOkOz8RU"
   },
   "outputs": [],
   "source": [
    "for model in [\n",
    "    \"resnet50\",\n",
    "    \"mocov3_resnet50\",\n",
    "    \"mocov3_vit_base\",\n",
    "    \"timm_vit_base_patch16_224.mae\",\n",
    "]:\n",
    "    BEST_PARAMS_v3[\"HDBSCAN\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"euclidean\",\n",
    "            \"hdbscan_method\": \"eom\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "for model in [\n",
    "    \"random_resnet50\",\n",
    "    \"vicreg_resnet50\",\n",
    "    \"dino_resnet50\",\n",
    "    \"clip_RN50\",\n",
    "    \"random_vitb16\",\n",
    "    \"dino_vitb16\",\n",
    "    \"clip_vitb16\",\n",
    "]:\n",
    "    BEST_PARAMS_v3[\"HDBSCAN\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"l1\",\n",
    "            \"hdbscan_method\": \"eom\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "for model in [\"vitb16\"]:\n",
    "    BEST_PARAMS_v3[\"HDBSCAN\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"chebyshev\",\n",
    "            \"hdbscan_method\": \"eom\",\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"none\"][\"distance_metric\"] = \"euclidean\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"none\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"random_resnet50\"][\"distance_metric\"] = \"l1\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"random_resnet50\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"resnet50\"][\"distance_metric\"] = \"euclidean\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"resnet50\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"mocov3_resnet50\"][\"distance_metric\"] = \"euclidean\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"mocov3_resnet50\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"dino_resnet50\"][\"distance_metric\"] = \"l1\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"dino_resnet50\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"vicreg_resnet50\"][\"distance_metric\"] = \"l1\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"vicreg_resnet50\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"clip_RN50\"][\"distance_metric\"] = \"l1\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"clip_RN50\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"random_vitb16\"][\"distance_metric\"] = \"l1\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"random_vitb16\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"vitb16\"][\"distance_metric\"] = \"chebyshev\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"vitb16\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"mocov3_vit_base\"][\"distance_metric\"] = \"euclidean\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"mocov3_vit_base\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"dino_vitb16\"][\"distance_metric\"] = \"l1\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"dino_vitb16\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"timm_vit_base_patch16_224.mae\"][\"distance_metric\"] = \"euclidean\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"timm_vit_base_patch16_224.mae\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"mae_pretrain_vit_base_global\"][\"distance_metric\"] = \"l1\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"mae_pretrain_vit_base_global\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"clip_vitb16\"][\"distance_metric\"] = \"l1\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"clip_vitb16\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"ft_mocov3_resnet50\"][\"distance_metric\"] = \"chebyshev\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"ft_mocov3_resnet50\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"ft_dino_resnet50\"][\"distance_metric\"] = \"l1\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"ft_dino_resnet50\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"ft_vicreg_resnet50\"][\"distance_metric\"] = \"euclidean\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"ft_vicreg_resnet50\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"ft_mocov3_vit_base\"][\"distance_metric\"] = \"chebyshev\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"ft_mocov3_vit_base\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"ft_dino_vitb16\"][\"distance_metric\"] = \"chebyshev\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"ft_dino_vitb16\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"mae_finetuned_vit_base_global\"][\"distance_metric\"] = \"chebyshev\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"mae_finetuned_vit_base_global\"][\"hdbscan_method\"] = \"eom\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"none\"][\"distance_metric\"] = \"euclidean\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"none\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"random_resnet50\"][\"distance_metric\"] = \"l1\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"random_resnet50\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"resnet50\"][\"distance_metric\"] = \"euclidean\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"resnet50\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"mocov3_resnet50\"][\"distance_metric\"] = \"euclidean\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"mocov3_resnet50\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"dino_resnet50\"][\"distance_metric\"] = \"l1\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"dino_resnet50\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"vicreg_resnet50\"][\"distance_metric\"] = \"l1\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"vicreg_resnet50\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"clip_RN50\"][\"distance_metric\"] = \"l1\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"clip_RN50\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"random_vitb16\"][\"distance_metric\"] = \"l1\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"random_vitb16\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"vitb16\"][\"distance_metric\"] = \"chebyshev\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"vitb16\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"mocov3_vit_base\"][\"distance_metric\"] = \"euclidean\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"mocov3_vit_base\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"dino_vitb16\"][\"distance_metric\"] = \"l1\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"dino_vitb16\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"timm_vit_base_patch16_224.mae\"][\"distance_metric\"] = \"euclidean\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"timm_vit_base_patch16_224.mae\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"mae_pretrain_vit_base_global\"][\"distance_metric\"] = \"l1\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"mae_pretrain_vit_base_global\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"clip_vitb16\"][\"distance_metric\"] = \"l1\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"clip_vitb16\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"ft_mocov3_resnet50\"][\"distance_metric\"] = \"chebyshev\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"ft_mocov3_resnet50\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"ft_dino_resnet50\"][\"distance_metric\"] = \"l1\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"ft_dino_resnet50\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"ft_vicreg_resnet50\"][\"distance_metric\"] = \"euclidean\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"ft_vicreg_resnet50\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"ft_mocov3_vit_base\"][\"distance_metric\"] = \"chebyshev\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"ft_mocov3_vit_base\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"ft_dino_vitb16\"][\"distance_metric\"] = \"chebyshev\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"ft_dino_vitb16\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"mae_finetuned_vit_base_global\"][\"distance_metric\"] = \"chebyshev\"\n",
    "BEST_PARAMS_v5[\"HDBSCAN\"][\"mae_finetuned_vit_base_global\"][\"hdbscan_method\"] = \"eom\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"none\"][\"spectral_n_neighbors\"] = 10\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"random_resnet50\"][\"spectral_n_neighbors\"] = 50\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"resnet50\"][\"spectral_n_neighbors\"] = 20\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"mocov3_resnet50\"][\"spectral_n_neighbors\"] = 30\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"dino_resnet50\"][\"spectral_n_neighbors\"] = 10\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"vicreg_resnet50\"][\"spectral_n_neighbors\"] = 10\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"clip_RN50\"][\"spectral_n_neighbors\"] = 30\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"random_vitb16\"][\"spectral_n_neighbors\"] = 50\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"vitb16\"][\"spectral_n_neighbors\"] = 30\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"mocov3_vit_base\"][\"spectral_n_neighbors\"] = 50\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"dino_vitb16\"][\"spectral_n_neighbors\"] = 10\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"timm_vit_base_patch16_224.mae\"][\"spectral_n_neighbors\"] = 10\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"mae_pretrain_vit_base_global\"][\"spectral_n_neighbors\"] = 30\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"clip_vitb16\"][\"spectral_n_neighbors\"] = 20\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"ft_mocov3_resnet50\"][\"spectral_n_neighbors\"] = 30\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"ft_dino_resnet50\"][\"spectral_n_neighbors\"] = 20\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"ft_vicreg_resnet50\"][\"spectral_n_neighbors\"] = 20\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"ft_mocov3_vit_base\"][\"spectral_n_neighbors\"] = 50\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"ft_dino_vitb16\"][\"spectral_n_neighbors\"] = 50\n",
    "BEST_PARAMS_v5[\"SpectralClustering\"][\"mae_finetuned_vit_base_global\"][\"spectral_n_neighbors\"] = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"none\"].update({\"distance_metric\": \"l2\", \"louvain_remove_self_loops\": False})\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"random_resnet50\"].update({\"distance_metric\": \"l2\", \"louvain_remove_self_loops\": False})\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"resnet50\"].update({\"distance_metric\": \"l2\", \"louvain_remove_self_loops\": False})\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"mocov3_resnet50\"].update({\"distance_metric\": \"l1\", \"louvain_remove_self_loops\": False})\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"dino_resnet50\"].update({\"distance_metric\": \"l2\", \"louvain_remove_self_loops\": False})\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"vicreg_resnet50\"].update({\"distance_metric\": \"l2\", \"louvain_remove_self_loops\": False})\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"random_vitb16\"].update({\"distance_metric\": \"l1\", \"louvain_remove_self_loops\": True})\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"vitb16\"].update({\"distance_metric\": \"chebyshev\", \"louvain_remove_self_loops\": False})\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"mocov3_vit_base\"].update({\"distance_metric\": \"l2\", \"louvain_remove_self_loops\": False})\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"dino_vitb16\"].update({\"distance_metric\": \"l2\", \"louvain_remove_self_loops\": False})\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"timm_vit_base_patch16_224.mae\"].update({\"distance_metric\": \"l1\", \"louvain_remove_self_loops\": False})\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"mae_pretrain_vit_base_global\"].update({\"distance_metric\": \"l2\", \"louvain_remove_self_loops\": False})\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"ft_mocov3_resnet50\"].update({\"distance_metric\": \"l1\", \"louvain_remove_self_loops\": False})\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"ft_dino_resnet50\"].update({\"distance_metric\": \"l2\", \"louvain_remove_self_loops\": False})\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"ft_vicreg_resnet50\"].update({\"distance_metric\": \"chebyshev\", \"louvain_remove_self_loops\": False})\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"ft_mocov3_vit_base\"].update({\"distance_metric\": \"l2\", \"louvain_remove_self_loops\": True})\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"ft_dino_vitb16\"].update({\"distance_metric\": \"l2\", \"louvain_remove_self_loops\": False})\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"mae_finetuned_vit_base_global\"].update({\"distance_metric\": \"l2\", \"louvain_remove_self_loops\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"none\"].update({\"louvain_resolution\": 1.2})\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"random_resnet50\"].update({\"louvain_resolution\": 2.0})\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"resnet50\"].update({\"louvain_resolution\": 1.0})  # Tied - used default\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"mocov3_resnet50\"].update({\"louvain_resolution\": 1.4})\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"dino_resnet50\"].update({\"louvain_resolution\": 1.0})\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"vicreg_resnet50\"].update({\"louvain_resolution\": 1.0})\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"random_vitb16\"].update({\"louvain_resolution\": 3.0})\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"vitb16\"].update({\"louvain_resolution\": 1.1})\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"mocov3_vit_base\"].update({\"louvain_resolution\": 1.0})\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"dino_vitb16\"].update({\"louvain_resolution\": 1.0})\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"timm_vit_base_patch16_224.mae\"].update({\"louvain_resolution\": 1.1})\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"mae_pretrain_vit_base_global\"].update({\"louvain_resolution\": 1.1})\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"ft_mocov3_resnet50\"].update({\"louvain_resolution\": 1.0})  # Tied - used default\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"ft_dino_resnet50\"].update({\"louvain_resolution\": 1.0})  # Tied - used default\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"ft_vicreg_resnet50\"].update({\"louvain_resolution\": 1.0})  # Tied - used default\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"ft_mocov3_vit_base\"].update({\"louvain_resolution\": 1.0})\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"ft_dino_vitb16\"].update({\"louvain_resolution\": 1.2})\n",
    "BEST_PARAMS_v5[\"LouvainCommunities\"][\"mae_finetuned_vit_base_global\"].update({\"louvain_resolution\": 1.0})  # Tied - used default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, set overall hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_PARAMS = BEST_PARAMS_v5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSVZM4cns-gm"
   },
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8w21wu6JUk4d"
   },
   "outputs": [],
   "source": [
    "def categorical_cmap(nc, nsc, cmap=\"tab10\", continuous=False):\n",
    "    \"\"\"\n",
    "    Create a colormap with a certain number of shades of colours.\n",
    "\n",
    "    Based on https://stackoverflow.com/a/47232942/1960959\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nc : int\n",
    "        Number of categories.\n",
    "    nsc : int\n",
    "        Number of shades per category.\n",
    "    cmap : str, default=tab10\n",
    "        Original colormap to extend into multiple shades.\n",
    "    continuous : bool, default=False\n",
    "        Whether ``cmap`` is continous. Otherwise it is treated\n",
    "        as categorical with adjacent colors unrelated.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.colors.ListedColormap\n",
    "        New cmap which alternates between ``nsc`` shades of ``nc``\n",
    "        colors from ``cmap``.\n",
    "    \"\"\"\n",
    "    if nc > plt.get_cmap(cmap).N:\n",
    "        raise ValueError(\"Too many categories for colormap.\")\n",
    "    if continuous:\n",
    "        ccolors = plt.get_cmap(cmap)(np.linspace(0, 1, nc))\n",
    "    else:\n",
    "        ccolors = plt.get_cmap(cmap)(np.arange(nc, dtype=int))\n",
    "    cols = np.zeros((nc * nsc, 3))\n",
    "    for i, c in enumerate(ccolors):\n",
    "        chsv = matplotlib.colors.rgb_to_hsv(c[:3])\n",
    "        arhsv = np.tile(chsv, nsc).reshape(nsc, 3)\n",
    "        arhsv[:, 1] = np.linspace(chsv[1], 0.25, nsc)\n",
    "        arhsv[:, 2] = np.linspace(chsv[2], 1, nsc)\n",
    "        rgb = matplotlib.colors.hsv_to_rgb(arhsv)\n",
    "        cols[i * nsc : (i + 1) * nsc, :] = rgb\n",
    "    cmap = matplotlib.colors.ListedColormap(cols)\n",
    "    return cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "id": "Zt-xAviSUwWV",
    "outputId": "0feb0c9b-c97d-4ec7-955f-8fdcc521be3c"
   },
   "outputs": [],
   "source": [
    "categorical_cmap(len(RESNET50_MODELS), len(VALIDATION_DATASETS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zs_ssl_clustering.datasets import image_dataset_sizes\n",
    "\n",
    "\n",
    "def clip_imgsize(dataset, target_image_size):\n",
    "    if target_image_size is None:\n",
    "        return target_image_size\n",
    "    dataset_imsize = image_dataset_sizes(dataset)[1]\n",
    "    if dataset_imsize is None:\n",
    "        return target_image_size\n",
    "    if hasattr(dataset_imsize, \"__len__\"):\n",
    "        dataset_imsize = min(dataset_imsize)\n",
    "    return min(target_image_size, dataset_imsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixup_filter(filters):\n",
    "    dataset = filters.get(\"dataset_name\", filters.get(\"dataset\", None))\n",
    "    if dataset and \"image_size\" in filters:\n",
    "        filters[\"image_size\"] = clip_imgsize(dataset, filters[\"image_size\"])\n",
    "    if dataset and \"min_samples\" in filters:\n",
    "        if dataset.lower() in [\"celeba\", \"utkface\", \"bioscan1m\"]:\n",
    "            filters[\"min_samples\"] = 2\n",
    "    return filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i5hY2oc2I-PO"
   },
   "outputs": [],
   "source": [
    "def select_rows(df, filters, allow_missing=True, fixup=True):\n",
    "    if fixup:\n",
    "        filters = fixup_filter(filters)\n",
    "    select = np.ones(len(df), dtype=bool)\n",
    "    for col, val in filters.items():\n",
    "        if col == \"dataset\":\n",
    "            col = \"dataset_name\"\n",
    "        if col == \"clusterer\":\n",
    "            col = \"clusterer_name\"\n",
    "        if val is None or val == \"None\" or val == \"none\":\n",
    "            select_i = pd.isna(df[col])\n",
    "            select_i |= df[col] == \"None\"\n",
    "            select_i |= df[col] == \"none\"\n",
    "        else:\n",
    "            select_i = df[col] == val\n",
    "            select_i |= df[col] == str(val)\n",
    "            if allow_missing or val == \"None\" or val == \"none\":\n",
    "                select_i |= pd.isna(df[col])\n",
    "        select &= select_i\n",
    "    return df[select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7iOVrUnC_Jg-"
   },
   "outputs": [],
   "source": [
    "def find_differing_columns(df, cols=None):\n",
    "    if cols is None:\n",
    "        cols = df.columns\n",
    "    my_cols = []\n",
    "    for col in cols:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        if df[col].nunique(dropna=False) > 1:\n",
    "            my_cols.append(col)\n",
    "    return my_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wI7MkSQiz89O"
   },
   "outputs": [],
   "source": [
    "def filter2command(*filters, partition=\"val\"):\n",
    "    f = {}\n",
    "    for filter in filters:\n",
    "        for k, v in filter.items():\n",
    "            f[k] = v\n",
    "    dataset = f.get(\"dataset\", \"\")\n",
    "    clusterer = f.get(\"clusterer\", \"\")\n",
    "\n",
    "    mem = 2  # RAM in gigabytes\n",
    "\n",
    "    if clusterer in [\"LouvainCommunities\"]:\n",
    "        if dataset in [\"inaturalist\"]:\n",
    "            # 100,000 samples\n",
    "            mem = 3_700\n",
    "        elif dataset in [\"imagenet-sketch\", \"imagenet\"]:\n",
    "            # 50,000 samples\n",
    "            mem = 926\n",
    "        elif dataset in [\"places365\"]:\n",
    "            # 36,500 samples\n",
    "            mem = 494\n",
    "        elif dataset in [\"imagenet-r\"]:\n",
    "            # 30,000 samples\n",
    "            mem = 333\n",
    "        elif dataset in [\"svhn\"]:\n",
    "            # 26,000 samples\n",
    "            mem = 250\n",
    "        elif dataset in [\"bioscan1m\", \"nabirds\"]:\n",
    "            # 24,600 samples\n",
    "            mem = 224\n",
    "        elif dataset in [\"celeba\"]:\n",
    "            # 20,000 samples\n",
    "            mem = 128\n",
    "        elif dataset in [\n",
    "            \"imagenetv2\",\n",
    "            \"cifar10\",\n",
    "            \"cifar100\",\n",
    "            \"lsun\",\n",
    "            \"mnist\",\n",
    "            \"fashionmnist\",\n",
    "            \"stanfordcars\",\n",
    "            \"breakhis\",\n",
    "        ]:\n",
    "            # 8,000 - 10,000 samples\n",
    "            mem = 32\n",
    "        elif dataset in [\"flowers102\", \"utkface\"]:\n",
    "            # 5,925 - 6,200 samples\n",
    "            mem = 18\n",
    "        elif dataset.startswith(\"in9\") or dataset in [\"eurosat\"]:\n",
    "            # 4,500 samples\n",
    "            mem = 8\n",
    "        elif dataset in [\"imagenette\", \"imagewoof\", \"aircraft\"]:\n",
    "            # 3,333 - 3,930 samples\n",
    "            mem = 6\n",
    "        elif dataset in [\"imagenet-o\", \"dtd\"]:\n",
    "            # 2,000 samples\n",
    "            mem = 4\n",
    "        else:\n",
    "            mem = 12\n",
    "\n",
    "    elif clusterer in [\"AffinityPropagation\"]:\n",
    "        if dataset in [\"inaturalist\"]:\n",
    "            # 100,000 samples\n",
    "            mem = 292\n",
    "        elif dataset in [\"imagenet-sketch\", \"imagenet\"]:\n",
    "            # 50,000 samples\n",
    "            mem = 72\n",
    "        elif dataset in [\"places365\", \"imagenet-r\", \"svhn\", \"bioscan1m\", \"nabirds\"]:\n",
    "            # 24,600 - 36,500 samples\n",
    "            mem = 48\n",
    "        elif dataset in [\"celeba\"]:\n",
    "            # 20,000 samples\n",
    "            mem = 12\n",
    "        elif dataset in [\n",
    "            \"imagenetv2\",\n",
    "            \"cifar10\",\n",
    "            \"cifar100\",\n",
    "            \"lsun\",\n",
    "            \"mnist\",\n",
    "            \"fashionmnist\",\n",
    "            \"stanfordcars\",\n",
    "        ]:\n",
    "            # 8,000 - 10,000 samples\n",
    "            mem = 6\n",
    "        elif dataset.startswith(\"in9\") or dataset in [\n",
    "            \"flowers102\",\n",
    "            \"utkface\",\n",
    "            \"eurosat\",\n",
    "            \"aircraft\",\n",
    "            \"breakhis\",\n",
    "            \"imagenet-o\",\n",
    "            \"dtd\",\n",
    "        ]:\n",
    "            # 1,900 - 6,200 samples\n",
    "            mem = 2\n",
    "        elif dataset in [\"imagenette\", \"imagewoof\"]:\n",
    "            # 3,930 samples\n",
    "            mem = 1\n",
    "        else:\n",
    "            mem = 8\n",
    "\n",
    "    elif clusterer in [\"AgglomerativeClustering\", \"SpectralClustering\"]:\n",
    "        if dataset in [\"inaturalist\"]:\n",
    "            # 100,000 samples\n",
    "            mem = 72\n",
    "        elif dataset in [\"imagenet-sketch\", \"imagenet\"]:\n",
    "            # 50,000 samples\n",
    "            mem = 20\n",
    "        elif dataset in [\"places365\", \"imagenet-r\", \"svhn\", \"bioscan1m\", \"nabirds\"]:\n",
    "            # 24,600 - 36,500 samples\n",
    "            mem = 16\n",
    "        elif dataset in [\"celeba\"]:\n",
    "            # 20,000 samples\n",
    "            mem = 12\n",
    "        elif dataset in [\n",
    "            \"imagenetv2\",\n",
    "            \"cifar10\",\n",
    "            \"cifar100\",\n",
    "            \"lsun\",\n",
    "            \"mnist\",\n",
    "            \"fashionmnist\",\n",
    "            \"stanfordcars\",\n",
    "        ]:\n",
    "            # 8,000 - 10,000 samples\n",
    "            mem = 6\n",
    "        elif dataset.startswith(\"in9\") or dataset in [\n",
    "            \"flowers102\",\n",
    "            \"utkface\",\n",
    "            \"eurosat\",\n",
    "            \"aircraft\",\n",
    "            \"breakhis\",\n",
    "            \"imagenet-o\",\n",
    "            \"dtd\",\n",
    "        ]:\n",
    "            # 1,900 - 6,200 samples\n",
    "            mem = 4\n",
    "        elif dataset in [\"imagenette\", \"imagewoof\"]:\n",
    "            # 3,930 samples\n",
    "            mem = 2\n",
    "        else:\n",
    "            mem = 8\n",
    "        if clusterer == \"SpectralClustering\":\n",
    "            snn = f.get(\"spectral_n_neighbors\", 100)\n",
    "            if snn <= 10:\n",
    "                mem = mem * 8 / 20\n",
    "            elif snn <= 20:\n",
    "                mem = mem * 3 / 4\n",
    "            mem = int(np.ceil(mem))\n",
    "\n",
    "    elif clusterer in [\"HDBSCAN\", \"KMeans\"]:\n",
    "        if dataset in [\"inaturalist\"]:\n",
    "            # 100,000 samples\n",
    "            mem = 6\n",
    "        elif dataset in [\"imagenet-sketch\", \"imagenet\"]:\n",
    "            # 50,000 samples\n",
    "            mem = 4\n",
    "        elif dataset in [\"places365\", \"imagenet-r\", \"svhn\", \"bioscan1m\", \"nabirds\"]:\n",
    "            # 24,600 - 36,500 samples\n",
    "            mem = 4\n",
    "        elif dataset in [\"celeba\"]:\n",
    "            # 20,000 samples\n",
    "            mem = 4\n",
    "        elif dataset in [\n",
    "            \"imagenetv2\",\n",
    "            \"cifar10\",\n",
    "            \"cifar100\",\n",
    "            \"lsun\",\n",
    "            \"mnist\",\n",
    "            \"fashionmnist\",\n",
    "            \"stanfordcars\",\n",
    "        ]:\n",
    "            # 8,000 - 10,000 samples\n",
    "            mem = 2\n",
    "        elif dataset.startswith(\"in9\") or dataset in [\n",
    "            \"flowers102\",\n",
    "            \"utkface\",\n",
    "            \"eurosat\",\n",
    "            \"aircraft\",\n",
    "            \"breakhis\",\n",
    "            \"imagenet-o\",\n",
    "            \"dtd\",\n",
    "        ]:\n",
    "            # 1,900 - 6,200 samples\n",
    "            mem = 2\n",
    "        elif dataset in [\"imagenette\", \"imagewoof\"]:\n",
    "            # 3,930 samples\n",
    "            mem = 1\n",
    "        else:\n",
    "            mem = 4\n",
    "\n",
    "    if mem > 300:\n",
    "        return \"\"\n",
    "    if mem > 129:\n",
    "        pass\n",
    "\n",
    "    mem = f\"{mem}G\"\n",
    "\n",
    "    if partition == \"val\":\n",
    "        seed = 100\n",
    "    elif partition == \"test\":\n",
    "        seed = 1\n",
    "    else:\n",
    "        seed = 0\n",
    "    s = (\n",
    "        f\"sbatch --array={seed} --mem={mem}\"\n",
    "        f' --job-name=\"zsc-{f.get(\"model\", \"\")}-{dataset}-{clusterer}\"'\n",
    "        f\" slurm/cluster.slrm --partition={partition}\"\n",
    "    )\n",
    "    for k, v in f.items():\n",
    "        if v is None:\n",
    "            continue\n",
    "        if k == \"zscore\":\n",
    "            if v == \"False\" or not v:\n",
    "                s += \" --no-zscore\"\n",
    "            elif v == \"True\" or v:\n",
    "                s += \" --zscore\"\n",
    "            continue\n",
    "        if k == \"normalize\":\n",
    "            if v == \"False\" or not v:\n",
    "                pass\n",
    "            elif v == \"True\" or v:\n",
    "                s += \" --normalize\"\n",
    "            continue\n",
    "        if k == \"zscore2\":\n",
    "            if v == \"False\" or not v:\n",
    "                s += \" --no-zscore2\"\n",
    "            elif v == \"average\":\n",
    "                s += \" --azscore2\"\n",
    "            elif v == \"standard\" or v:\n",
    "                s += \" --zscore2\"\n",
    "            continue\n",
    "        if k == \"ndim_correction\":\n",
    "            if v == \"False\" or not v:\n",
    "                s += \" --no-ndim-correction\"\n",
    "            elif v == \"True\" or v:\n",
    "                s += \" --ndim-correction\"\n",
    "            continue\n",
    "        if k == \"louvain_remove_self_loops\":\n",
    "            if v == \"False\" or not v:\n",
    "                s += \" --louvain-keep-self\"\n",
    "            elif v == \"True\" or v:\n",
    "                pass\n",
    "            continue\n",
    "        s += f\" --{k.replace('_', '-')}={v}\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOKljXoMNGG7"
   },
   "source": [
    "# Final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude CLIP from analysis\n",
    "RESNET50_MODELS = [v for v in RESNET50_MODELS if not v.startswith(\"clip\")]\n",
    "VITB16_MODELS = [v for v in VITB16_MODELS if not v.startswith(\"clip\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "CuPbp58ONv4Z"
   },
   "outputs": [],
   "source": [
    "TEST_DATASETS = [\n",
    "    \"imagenet\",\n",
    "    \"imagenetv2\",\n",
    "    \"imagenet-o\",\n",
    "    \"cifar10\",\n",
    "    \"cifar100\",\n",
    "    \"in9original\",\n",
    "    \"in9mixedrand\",\n",
    "    # \"in9onlybgt\",\n",
    "    \"in9onlyfg\",\n",
    "    \"imagenet-r\",\n",
    "    \"imagenet-sketch\",\n",
    "    \"aircraft\",\n",
    "    \"stanfordcars\",\n",
    "    \"flowers102\",\n",
    "    \"bioscan1m\",\n",
    "    \"nabirds\",\n",
    "    \"inaturalist\",\n",
    "    \"celeba\",\n",
    "    \"utkface\",\n",
    "    \"breakhis\",\n",
    "    \"dtd\",\n",
    "    \"eurosat\",\n",
    "    \"lsun\",\n",
    "    \"places365\",\n",
    "    \"mnist\",\n",
    "    \"fashionmnist\",\n",
    "    \"svhn\",\n",
    "]\n",
    "DATASET2SH = {\n",
    "    \"aircraft\": \"Air\",\n",
    "    \"bioscan1m\": \"Bio\",\n",
    "    \"breakhis\": \"BHis\",\n",
    "    \"celeba\": \"CelA\",\n",
    "    \"cifar10\": \"C10\",\n",
    "    \"cifar100\": \"C100\",\n",
    "    \"dtd\": \"DTD\",\n",
    "    \"eurosat\": \"ESAT\",\n",
    "    \"flowers102\": \"F102\",\n",
    "    \"fashionmnist\": \"Fash\",\n",
    "    \"imagenet\": \"IN1k\",\n",
    "    \"imagenet-o\": \"IN-O\",\n",
    "    \"imagenet-r\": \"IN-R\",\n",
    "    \"imagenet-sketch\": \"IN-S\",\n",
    "    \"imagenetv2\": \"INv2\",\n",
    "    \"imagenette\": \"IN10\",\n",
    "    \"imagewoof\": \"INwf\",\n",
    "    \"in9original\": \"IN9\",\n",
    "    \"in9mixednext\": \"9-MN\",\n",
    "    \"in9mixedrand\": \"9-MR\",\n",
    "    \"in9mixedsame\": \"9-MS\",\n",
    "    \"in9nofg\": \"9-NoFG\",\n",
    "    \"in9onlybgb\": \"9-BGB\",\n",
    "    \"in9onlybgt\": \"9-BGT\",\n",
    "    \"in9onlyfg\": \"9-FG\",\n",
    "    \"inaturalist\": \"iNat\",\n",
    "    \"lsun\": \"LSU\",\n",
    "    \"mnist\": \"MNST\",\n",
    "    \"nabirds\": \"Birds\",\n",
    "    \"places365\": \"P365\",\n",
    "    \"stanfordcars\": \"Cars\",\n",
    "    \"svhn\": \"SVHN\",\n",
    "    \"utkface\": \"UTKF\",\n",
    "}\n",
    "MODEL_GROUPS = {\n",
    "    \"ResNet-50\": RESNET50_MODELS,\n",
    "    \"ViT-B\": VITB16_MODELS,\n",
    "    \"ResNet-50 [FT]\": FT_RESNET50_MODELS,\n",
    "    \"ViT-B [FT]\": FT_VITB16_MODELS,\n",
    "    \"all\": ALL_MODELS,\n",
    "}\n",
    "MODEL2SH = {\n",
    "    \"none\": \"Raw image\",\n",
    "    \"random_resnet50\": \"Rand.\",  # \"Random\",\n",
    "    \"random_vitb16\": \"Rand.\",  # \"Random\",\n",
    "    \"resnet50\": \"X-Ent.\",\n",
    "    \"mocov3_resnet50\": \"MoCo-v3\",\n",
    "    \"dino_resnet50\": \"DINO\",\n",
    "    \"vicreg_resnet50\": \"VICReg\",\n",
    "    \"clip_RN50\": \"CLIP\",\n",
    "    \"vitb16\": \"X-Ent.\",\n",
    "    \"mocov3_vit_base\": \"MoCo-v3\",\n",
    "    \"dino_vitb16\": \"DINO\",\n",
    "    \"timm_vit_base_patch16_224.mae\": \"MAE (CLS)\",\n",
    "    \"mae_pretrain_vit_base_global\": \"MAE (avg)\",\n",
    "    \"clip_vitb16\": \"CLIP\",\n",
    "    \"ft_mocov3_resnet50\": \"MoCo-v3 [FT]\",\n",
    "    \"ft_dino_resnet50\": \"DINO [FT]\",\n",
    "    \"ft_vicreg_resnet50\": \"VICReg [FT]\",\n",
    "    \"ft_mocov3_vit_base\": \"MoCo-v3 [FT]\",\n",
    "    \"ft_dino_vitb16\": \"DINO [FT]\",\n",
    "    \"mae_finetuned_vit_base_global\": \"MAE (avg) [FT]\",\n",
    "}\n",
    "CLUSTERER2SH = {\n",
    "    \"KMeans\": \"K-Means\",\n",
    "    \"SpectralClustering\": \"Spectral\",\n",
    "    \"AffinityPropagation\": \"Affinity Prop\",\n",
    "    \"AgglomerativeClustering\": \"AC\",\n",
    "    \"AC w/ C\": \"AC w/  C\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL2SH_ARCH = dict(MODEL2SH)\n",
    "for k, v in MODEL2SH.items():\n",
    "    if \"resnet\" in k or \"RN50\" in k:\n",
    "        MODEL2SH_ARCH[k] = f\"ResNet-50 {v}\"\n",
    "    elif \"vit\" in k:\n",
    "        MODEL2SH_ARCH[k] = f\"ViT-B {v}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATASETS_GROUPED = {\n",
    "    \"In-domain\": [\n",
    "        \"imagenet\",\n",
    "        \"imagenetv2\",\n",
    "        \"cifar10\",\n",
    "        \"cifar100\",\n",
    "        \"in9original\",\n",
    "    ],\n",
    "    \"Domain-shift\": [\n",
    "        \"in9onlyfg\",\n",
    "        # \"in9onlybgt\",\n",
    "        \"in9mixedrand\",\n",
    "        \"imagenet-r\",\n",
    "        \"imagenet-sketch\",\n",
    "    ],\n",
    "    \"Near-OOD\": [\n",
    "        \"imagenet-o\",\n",
    "        \"lsun\",\n",
    "        \"places365\",\n",
    "    ],\n",
    "    \"Fine-grained\": [\n",
    "        \"aircraft\",\n",
    "        \"stanfordcars\",\n",
    "        \"flowers102\",\n",
    "        \"bioscan1m\",\n",
    "        \"nabirds\",\n",
    "        \"inaturalist\",\n",
    "    ],\n",
    "    \"Far-OOD\": [\n",
    "        \"celeba\",\n",
    "        \"utkface\",\n",
    "        \"breakhis\",\n",
    "        \"dtd\",\n",
    "        \"eurosat\",\n",
    "        \"mnist\",\n",
    "        \"fashionmnist\",\n",
    "        \"svhn\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "DATASETGROUP2TITLE = {\n",
    "    \"Domain-shift\": \"Domain-shifted\",\n",
    "    \"Out-of-distribution\": \"OOD\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN9_DATASETS = [\n",
    "    \"in9original\",\n",
    "    \"in9onlyfg\",\n",
    "    \"in9nofg\",\n",
    "    \"in9onlybgt\",\n",
    "    \"in9mixedsame\",\n",
    "    \"in9mixedrand\",\n",
    "]\n",
    "IN92SH = {\n",
    "    \"in9original\": \"OG\",\n",
    "    \"in9mixednext\": \"MN\",\n",
    "    \"in9mixedrand\": \"MR\",\n",
    "    \"in9mixedsame\": \"MS\",\n",
    "    \"in9nofg\": r\"FG$^\\text{C}$\",\n",
    "    \"in9onlybgb\": \"BG(B)\",\n",
    "    \"in9onlybgt\": \"BG\",\n",
    "    \"in9onlyfg\": \"FG\",\n",
    "    \"in9bggap\": \"Gap\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTERER2COLORSTR = {\n",
    "    \"KMeans\": \"tab:purple\",\n",
    "    \"SpectralClustering\": \"tab:cyan\",\n",
    "    \"AC w/ C\": \"tab:red\",\n",
    "    \"AC w/o C\": \"tab:orange\",\n",
    "    \"AffinityPropagation\": \"tab:green\",\n",
    "    \"HDBSCAN\": \"tab:blue\",\n",
    "}\n",
    "CLUSTERER2COLORRGB = {k: matplotlib.colors.to_rgb(v) for k, v in CLUSTERER2COLORSTR.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICLR2024\n",
    "MODEL2COLORSTR = {\n",
    "    \"none\": \"black\",\n",
    "    \"random_resnet50\": \"tab:grey\",\n",
    "    \"random_vitb16\": \"tab:grey\",\n",
    "    \"resnet50\": \"tab:red\",\n",
    "    \"mocov3_resnet50\": \"tab:cyan\",\n",
    "    \"dino_resnet50\": \"tab:green\",\n",
    "    \"vicreg_resnet50\": \"tab:purple\",\n",
    "    \"clip_RN50\": \"tab:blue\",\n",
    "    \"vitb16\": \"tab:red\",\n",
    "    \"mocov3_vit_base\": \"tab:cyan\",\n",
    "    \"dino_vitb16\": \"tab:green\",\n",
    "    \"timm_vit_base_patch16_224.mae\": \"tab:olive\",\n",
    "    \"mae_pretrain_vit_base_global\": \"tab:brown\",\n",
    "    \"clip_vitb16\": \"tab:blue\",\n",
    "    \"mae_finetuned_vit_base_global\": \"tab:brown\",\n",
    "}\n",
    "MODEL2COLORRGB = {k: matplotlib.colors.to_rgb(v) for k, v in MODEL2COLORSTR.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICML2024\n",
    "MODEL2COLORSTR = {\n",
    "    \"none\": \"black\",\n",
    "    \"random_resnet50\": \"dimgrey\",\n",
    "    \"random_vitb16\": \"dimgrey\",\n",
    "    \"resnet50\": \"tab:red\",\n",
    "    \"mocov3_resnet50\": \"tab:green\",\n",
    "    \"dino_resnet50\": \"tab:purple\",\n",
    "    \"vicreg_resnet50\": \"tab:orange\",\n",
    "    \"clip_RN50\": \"tab:olive\",\n",
    "    \"vitb16\": \"tab:red\",\n",
    "    \"mocov3_vit_base\": \"tab:green\",\n",
    "    \"dino_vitb16\": \"tab:purple\",\n",
    "    \"timm_vit_base_patch16_224.mae\": \"tab:blue\",\n",
    "    \"mae_pretrain_vit_base_global\": \"tab:brown\",\n",
    "    \"clip_vitb16\": \"tab:olive\",\n",
    "    \"mae_finetuned_vit_base_global\": \"tab:brown\",\n",
    "}\n",
    "MODEL2COLORRGB = {k: matplotlib.colors.to_rgb(v) for k, v in MODEL2COLORSTR.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in FT_MODELS:\n",
    "    MODEL2COLORRGB[model] = tuple(c * 0.8 for c in MODEL2COLORRGB[FT2PRE[model]])\n",
    "for model in RESNET50_MODELS + VITB16_MODELS:\n",
    "    MODEL2COLORRGB[model] = tuple(1 - (1 - c) * 0.7 for c in MODEL2COLORRGB[model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabulate hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC2TABLE = {\n",
    "    \"l1\": r\"$\\ell_1$\",\n",
    "    \"cityblock\": r\"$\\ell_1$\",\n",
    "    \"manhattan\": r\"$\\ell_1$\",\n",
    "    \"l2\": r\"$\\ell_2$\",\n",
    "    \"euclidean\": r\"$\\ell_2$\",\n",
    "    \"chebyshev\": r\"$\\ell_\\infty$\",\n",
    "    \"infinity\": r\"$\\ell_\\infty$\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC2TABLE = {\n",
    "    \"l1\": r\"$L1$\",\n",
    "    \"cityblock\": r\"$L1$\",\n",
    "    \"manhattan\": r\"$L1$\",\n",
    "    \"l2\": r\"$L2$\",\n",
    "    \"euclidean\": r\"$L2$\",\n",
    "    \"chebyshev\": r\"$L\\infty$\",\n",
    "    \"infinity\": r\"$L\\infty$\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterers = [\"KMeans\", \"AC w/o C\", \"AffinityPropagation\", \"HDBSCAN\"]\n",
    "\n",
    "model_groups = {\n",
    "    \"---\": [\"none\"],\n",
    "    \"RN50\": RESNET50_MODELS + FT_RESNET50_MODELS,\n",
    "    \"ViT-B\": VITB16_MODELS + FT_VITB16_MODELS,\n",
    "}\n",
    "\n",
    "latex_table = r\"% Hyperparameters \" + f\"{BEST_PARAMS['_version']}\" + \"\\n\"\n",
    "now_str = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "latex_table += r\"% Generated \" + now_str + \"\\n\"\n",
    "label = \"hparams\"\n",
    "latex_table += r\"\\label{tab:\" + label + r\"}\" + \"\\n\"\n",
    "latex_table += r\"% \\resizebox{\\columnwidth}{!}{%\" + \"\\n\"\n",
    "latex_table += r\"\\begin{tabular}{lllllrccrr}\" + \"\\n\"\n",
    "latex_table += r\"\\toprule\" + \"\\n\"\n",
    "# Begin main header row\n",
    "\n",
    "latex_table += r\"      &             &   \"\n",
    "latex_table += r\" &           & &                &      & \\multicolumn{2}{c}{Agg. Clustering} & Aff. Prop. \\\\\"\n",
    "latex_table += r\" \\\\\" + \"\\n\"\n",
    "latex_table += r\"\\cmidrule(l){8-9} \\cmidrule(l){10-10}\" + \"\\n\"\n",
    "latex_table += r\"Arch. & \" + f\"{'Encoder':<11s}\" + r\" & FT\"\n",
    "latex_table += r\" & Clusterer & \\multicolumn{2}{c}{Dim Reduction} & Metric & Linkage & Dist. Thr. & Damping\"\n",
    "latex_table += r\" \\\\\" + \"\\n\"\n",
    "# Begin table contents\n",
    "latex_table += r\"\\midrule\" + \"\\n\"\n",
    "\n",
    "for i_group, group in enumerate(model_groups):\n",
    "    if i_group > 0:\n",
    "        latex_table += r\"\\midrule\" + \"\\n\"\n",
    "    latex_table += group + \"\\n\"\n",
    "    for i_model, model in enumerate(list(model_groups[group])):\n",
    "        if i_model != 0:\n",
    "            latex_table += r\"\\cmidrule(l){2-10}\" + \"\\n\"\n",
    "        for i_clusterer, clusterer in enumerate(clusterers):\n",
    "            model_sh = MODEL2SH.get(model, model) if i_clusterer == 0 else \"\"\n",
    "            if MODEL2SH.get(model, model).endswith(\" [FT]\"):\n",
    "                model_sh = f\"{model_sh[:-4]:<10s}\" + r\" & \\checkmark\"\n",
    "            else:\n",
    "                model_sh = f\"{model_sh:<10s} &\"\n",
    "            latex_table += f\"& {model_sh:<23s}\"\n",
    "            clusterername = CLUSTERER2SH.get(clusterer, clusterer)\n",
    "            latex_table += f\" & {clusterername:<13s}\"\n",
    "\n",
    "            dim_reducer = \"None\"\n",
    "            dim_reduced = None\n",
    "            if BEST_PARAMS[clusterer][model].get(\"dim_reducer_man\", None):\n",
    "                dim_reducer = BEST_PARAMS[clusterer][model][\"dim_reducer_man\"]\n",
    "                dim_reduced = BEST_PARAMS[clusterer][model][\"ndim_reduced_man\"]\n",
    "            elif BEST_PARAMS[clusterer][model].get(\"dim_reducer\", None):\n",
    "                dim_reducer = BEST_PARAMS[clusterer][model][\"dim_reducer\"]\n",
    "                if BEST_PARAMS[clusterer][model].get(\"ndim_reduced\", None):\n",
    "                    dim_reduced = BEST_PARAMS[clusterer][model][\"ndim_reduced\"]\n",
    "                elif BEST_PARAMS[clusterer][model].get(\"pca_variance\", None):\n",
    "                    dim_reduced = BEST_PARAMS[clusterer][model][\"pca_variance\"]\n",
    "                    dim_reduced = f\"{dim_reduced:.2f}\"\n",
    "            distance_metric = BEST_PARAMS[clusterer][model].get(\"distance_metric\", None)\n",
    "            affinity_damping = BEST_PARAMS[clusterer][model].get(\"affinity_damping\", None)\n",
    "            aggclust_linkage = BEST_PARAMS[clusterer][model].get(\"aggclust_linkage\", None)\n",
    "            aggclust_dist_thresh = BEST_PARAMS[clusterer][model].get(\"aggclust_dist_thresh\", None)\n",
    "\n",
    "            dim_reduced = dim_reduced if dim_reduced else r\"\\noval{}\"\n",
    "            distance_metric = distance_metric if distance_metric else r\"\\noval{}\"\n",
    "            distance_metric = METRIC2TABLE.get(distance_metric, distance_metric)\n",
    "            affinity_damping = f\"{affinity_damping:8.2f}\" if affinity_damping else r\"\\noval{}\"\n",
    "            aggclust_linkage = aggclust_linkage if aggclust_linkage else r\"\\noval{}\"\n",
    "            aggclust_dist_thresh = f\"{aggclust_dist_thresh:8.2f}\" if aggclust_dist_thresh else r\"\\noval{}\"\n",
    "\n",
    "            latex_table += f\" & {dim_reducer:<4s} & {dim_reduced:<4}\"\n",
    "            latex_table += f\" & {distance_metric:<14s}\"\n",
    "            latex_table += f\" & {aggclust_linkage:<8s} & {aggclust_dist_thresh}\"\n",
    "            latex_table += f\" & {affinity_damping}\"\n",
    "            latex_table += r\" \\\\\" + \"\\n\"\n",
    "\n",
    "latex_table += r\"\\bottomrule\" + \"\\n\"\n",
    "latex_table += r\"\\end{tabular}\" + \"\\n\"\n",
    "latex_table += r\"% }\" + \"\\n\"\n",
    "\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRNXaeD-tWUo"
   },
   "source": [
    "## Fetch results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_df_long = pd.DataFrame({\"id\": []})\n",
    "config_keys = set()\n",
    "summary_keys = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previous results from CSV file\n",
    "CSV_FNAME = \"test_runs_df.csv\"\n",
    "if os.path.isfile(CSV_FNAME):\n",
    "    pass\n",
    "    # runs_df_long = test_runs_df = pd.read_csv(CSV_FNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "dytOzTA3NMeF",
    "outputId": "19a7efdd-c32a-4f3d-c467-092870aba23e"
   },
   "outputs": [],
   "source": [
    "# Project is specified by <entity/project-name>\n",
    "api = wandb.Api(timeout=720)\n",
    "runs = api.runs(\n",
    "    \"uoguelph_mlrg/zs-ssl-clustering\",\n",
    "    filters={\n",
    "        \"state\": \"Finished\",\n",
    "        \"config.partition\": \"test\",\n",
    "    },  # \"config.predictions_dir\": \"y_pred\"},\n",
    "    per_page=10_000,\n",
    ")\n",
    "len(runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(runs_df_long)} runs currently in dataframe\")\n",
    "rows_to_add = []\n",
    "existing_ids = set(runs_df_long[\"id\"].values)\n",
    "for run in tqdm(runs):\n",
    "    if run.id in existing_ids:\n",
    "        if len(rows_to_add) >= len(runs) - len(runs_df_long):\n",
    "            break\n",
    "        continue\n",
    "    # .summary contains the output keys/values for metrics like accuracy.\n",
    "    #  We call ._json_dict to omit large files\n",
    "    summary = run.summary._json_dict\n",
    "    # .config contains the hyperparameters.\n",
    "    #  We remove special values that start with _.\n",
    "    config = {k: v for k, v in run.config.items() if not k.startswith(\"_\")}\n",
    "    # .name is the human-readable name of the run.\n",
    "    row = {\"id\": run.id, \"name\": run.name}\n",
    "    row.update({k: v for k, v in config.items() if not k.startswith(\"_\")})\n",
    "    row.update({k: v for k, v in summary.items() if not k.startswith(\"_\")})\n",
    "    if \"_timestamp\" in summary:\n",
    "        row[\"_timestamp\"] = summary[\"_timestamp\"]\n",
    "    rows_to_add.append(row)\n",
    "    config_keys = config_keys.union(config.keys())\n",
    "    summary_keys = summary_keys.union(summary.keys())\n",
    "\n",
    "if not len(rows_to_add):\n",
    "    print(\"No new runs to add\")\n",
    "else:\n",
    "    print(f\"Adding {len(rows_to_add)} runs\")\n",
    "    runs_df_long = pd.concat([runs_df_long, pd.DataFrame.from_records(rows_to_add)])\n",
    "print(f\"{len(runs_df_long)} runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove entries without an AMI metric\n",
    "test_runs_df = runs_df_long[~runs_df_long[\"AMI\"].isna()]\n",
    "len(test_runs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle changed default value for spectral_assigner after config arg was introduced\n",
    "if \"spectral_n_components\" not in test_runs_df.columns:\n",
    "    test_runs_df[\"spectral_n_components\"] = None\n",
    "\n",
    "if \"spectral_assigner\" not in test_runs_df.columns:\n",
    "    test_runs_df[\"spectral_assigner\"] = None\n",
    "select = test_runs_df[\"clusterer_name\"] != \"SpectralClustering\"\n",
    "test_runs_df.loc[select, \"spectral_assigner\"] = None\n",
    "select = (test_runs_df[\"clusterer_name\"] == \"SpectralClustering\") & pd.isna(test_runs_df[\"spectral_assigner\"])\n",
    "test_runs_df.loc[select, \"spectral_assigner\"] = \"kmeans\"\n",
    "\n",
    "# Accidentally wasn't clearing this hparam when it was unused\n",
    "if \"spectral_affinity\" not in test_runs_df.columns:\n",
    "    test_runs_df[\"spectral_affinity\"] = None\n",
    "select = test_runs_df[\"clusterer_name\"] != \"SpectralClustering\"\n",
    "test_runs_df.loc[select, \"spectral_affinity\"] = None\n",
    "\n",
    "if \"zscore2\" not in test_runs_df.columns:\n",
    "    test_runs_df[\"zscore2\"] = False\n",
    "test_runs_df.loc[pd.isna(test_runs_df[\"zscore2\"]), \"zscore2\"] = False\n",
    "\n",
    "if \"ndim_correction\" not in test_runs_df.columns:\n",
    "    test_runs_df[\"ndim_correction\"] = False\n",
    "test_runs_df.loc[pd.isna(test_runs_df[\"ndim_correction\"]), \"ndim_correction\"] = False\n",
    "\n",
    "if \"dim_reducer_man_nn\" not in test_runs_df.columns:\n",
    "    test_runs_df[\"dim_reducer_man_nn\"] = None\n",
    "\n",
    "if \"image_size\" not in test_runs_df.columns:\n",
    "    test_runs_df[\"image_size\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV file, so we can optionally skip downloading them\n",
    "test_runs_df.to_csv(CSV_FNAME, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "6bohRRomX9gO"
   },
   "outputs": [],
   "source": [
    "config_keys = config_keys.difference({\"workers\", \"memory_avail_GB\", \"memory_total_GB\", \"memory_slurm\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "k_KrOF5GNj9l",
    "outputId": "7649f12f-19ed-4113-bc66-88681d41ba3a"
   },
   "outputs": [],
   "source": [
    "test_runs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result loading utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"mocov3_resnet50\"\n",
    "dataset = \"imagenet\"\n",
    "clusterer = \"AC w/o C\"\n",
    "metric_key = \"AMI\"\n",
    "\n",
    "my_override_fields = {}\n",
    "\n",
    "filter1 = {\"model\": model, \"dataset\": dataset}\n",
    "filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "filter2.update(filter1)\n",
    "filter2.update(my_override_fields)\n",
    "filter2 = fixup_filter(filter2)\n",
    "sdf = select_rows(test_runs_df, filter2, allow_missing=False)\n",
    "my_val = np.nanmedian(sdf[metric_key])\n",
    "\n",
    "print(f\"{metric_key} = {my_val * 100:.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_results_table(\n",
    "    models,\n",
    "    clusterers,\n",
    "    datasets,\n",
    "    metric_keys=\"AMI\",\n",
    "    override_fields=None,\n",
    "    return_cmds=False,\n",
    "    verbosity=0,\n",
    "):\n",
    "    if override_fields is None:\n",
    "        override_fields = {}\n",
    "\n",
    "    do_squeeze = False\n",
    "    if isinstance(metric_keys, str):\n",
    "        do_squeeze = True\n",
    "        metric_keys = [metric_keys]\n",
    "\n",
    "    result_table = np.nan * np.ones((len(models), len(clusterers), len(datasets), len(metric_keys)))\n",
    "    cmds = []\n",
    "\n",
    "    for i_model, model in enumerate(models):\n",
    "        for i_clusterer, clusterer in enumerate(clusterers):\n",
    "            for i_dataset, dataset in enumerate(datasets):\n",
    "                filter1 = {\"model\": model, \"dataset\": dataset}\n",
    "                filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "                filter2.update(filter1)\n",
    "                filter2.update(override_fields)\n",
    "                filter2 = fixup_filter(filter2)\n",
    "                if dataset == \"in9bggap\":\n",
    "                    filter2[\"dataset\"] = \"in9mixedsame\"\n",
    "                sdf = select_rows(test_runs_df, filter2, allow_missing=False)\n",
    "                missing_val = False\n",
    "                if len(sdf) > 0:\n",
    "                    for i_key, key in enumerate(metric_keys):\n",
    "                        val = np.nanmedian(sdf[key])\n",
    "                        result_table[i_model, i_clusterer, i_dataset, i_key] = val\n",
    "                        if np.isnan(val):\n",
    "                            missing_val = True\n",
    "                if len(sdf) < 1 or missing_val:\n",
    "                    if verbosity >= 1:\n",
    "                        print(f\"No data for {model}-{dataset}-{clusterer}\\n{filter2}\")\n",
    "                    cmds.append(filter2command(filter2, partition=\"test\"))\n",
    "\n",
    "                if dataset == \"in9bggap\":\n",
    "                    filter2[\"dataset\"] = \"in9mixedrand\"\n",
    "                    sdf = select_rows(test_runs_df, filter2, allow_missing=False)\n",
    "                    for i_key, key in enumerate(metric_keys):\n",
    "                        result_table[i_model, i_clusterer, i_dataset, i_key] -= np.nanmedian(sdf[key])\n",
    "\n",
    "    if do_squeeze:\n",
    "        result_table = np.squeeze(result_table, axis=3)\n",
    "\n",
    "    if return_cmds:\n",
    "        return result_table, cmds\n",
    "    else:\n",
    "        return result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_generator(indict, pre=None):\n",
    "    pre = pre[:] if pre else []\n",
    "    if isinstance(indict, dict):\n",
    "        for key, value in indict.items():\n",
    "            if isinstance(value, dict):\n",
    "                for d in dict_generator(value, pre + [key]):\n",
    "                    yield d\n",
    "            elif isinstance(value, list) or isinstance(value, tuple):\n",
    "                for v in value:\n",
    "                    for d in dict_generator(v, pre + [key]):\n",
    "                        yield d\n",
    "            else:\n",
    "                yield pre + [key, value]\n",
    "    else:\n",
    "        yield pre + [indict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_flat_hierarchy_from_dict(indict, pad_right=True):\n",
    "    groups_flattened = list(dict_generator(indict))\n",
    "    depth = max(len(m) for m in groups_flattened)\n",
    "    if pad_right:\n",
    "        groups_flattened = [m + [\"\"] * (depth - len(m)) for m in groups_flattened]\n",
    "    else:\n",
    "        groups_flattened = [[\"\"] * (depth - len(m)) + m for m in groups_flattened]\n",
    "\n",
    "    return groups_flattened"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draft table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "_cBYVKo7PZFY",
    "outputId": "a1b016d5-a137-4100-b22b-67482af9ce74",
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric_key = \"AMI\"\n",
    "show_pc = True\n",
    "show_fmt = \"{:5.1f}\"\n",
    "show_commands = False\n",
    "eps = 0.001\n",
    "override_fields = {\n",
    "    # \"aggclust_dist_thresh\": None,  # to flip between unknown/known n clusters for AC\n",
    "    # \"predictions_dir\": \"y_pred\",\n",
    "}\n",
    "\n",
    "# KMeans  AffinityPropagation  AgglomerativeClustering  HDBSCAN\n",
    "backbones = MODEL_GROUPS.keys()\n",
    "clusterer = \"AgglomerativeClustering\"\n",
    "\n",
    "best_results = {k: [] for k in TEST_DATASETS}\n",
    "for dummy in [True, False]:\n",
    "    cmds = []\n",
    "    latex_table = r\"% Results for \" + f\"{metric_key}, {clusterer}\" + \"\\n\"\n",
    "    now_str = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    latex_table += r\"% Generated \" + now_str + \"\\n\"\n",
    "    latex_table += r\"% Using hparams \" + BEST_PARAMS[\"_version\"] + \"\\n\"\n",
    "    latex_table += r\"\\label{tab:\" + clusterer + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"\\resizebox{\\textwidth}{!}{%\" + \"\\n\"\n",
    "    latex_table += r\"\\begin{tabular}{ll\" + r\"r\" * len(TEST_DATASETS) + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    latex_table += r\"& \" + f\"{'Encoder':<11s}\"\n",
    "    for dataset in TEST_DATASETS:\n",
    "        latex_table += r\"&\" + \"{:^15s}\".format(DATASET2SH.get(dataset, dataset))\n",
    "    latex_table += r\"\\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    for i_group, model_group_name in enumerate(list(backbones)):\n",
    "        if i_group > 0:\n",
    "            latex_table += r\"\\midrule\" + \"\\n\"\n",
    "        for i_model, model in enumerate(MODEL_GROUPS[model_group_name]):\n",
    "            if i_model == 0:\n",
    "                latex_table += r\"\\parbox[t]{2mm}{\\multirow{5}{*}{\\rotatebox[origin=c]{90}{\" + model_group_name + \"}}}\"\n",
    "                latex_table += \"\\n\"\n",
    "            latex_table += f\"& {MODEL2SH.get(model, model):<10s}\"\n",
    "            for i_dataset, dataset in enumerate(TEST_DATASETS):\n",
    "                latex_table += \" &\"\n",
    "                filter1 = {\n",
    "                    \"model\": model,\n",
    "                    \"dataset\": dataset,\n",
    "                    \"clusterer\": clusterer,\n",
    "                }\n",
    "                filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "                filter2.update(filter1)\n",
    "                filter2.update(override_fields)\n",
    "                filter2 = fixup_filter(filter2)\n",
    "                sdf = select_rows(test_runs_df, filter2, allow_missing=False)\n",
    "                if len(sdf) < 1:\n",
    "                    # print(f\"No data for {filter2}\")\n",
    "                    if clusterer == \"AffinityPropagation\" and dataset in [\n",
    "                        \"imagenet\",\n",
    "                        \"inaturalist\",\n",
    "                    ]:\n",
    "                        continue\n",
    "                        pass\n",
    "                    cmds.append(filter2command(filter2, partition=\"test\"))\n",
    "                    continue\n",
    "                if len(sdf) > 1:\n",
    "                    if sum(np.abs(sdf[metric_key] - sdf.iloc[0][metric_key]) > 1e-6) > 0:\n",
    "                        print()\n",
    "                        print(\n",
    "                            f\"More than one result with {metric_key} values\",\n",
    "                            list(sdf[metric_key]),\n",
    "                        )\n",
    "                        print(f\"for search {filter2}\")\n",
    "                        dif_cols = find_differing_columns(sdf, config_keys)\n",
    "                        print(f\"columns which differ: {dif_cols}\")\n",
    "                        if dif_cols:\n",
    "                            for col in dif_cols:\n",
    "                                print(f\"  {col}: {list(sdf[col])}\")\n",
    "                my_val = np.median(sdf[metric_key])\n",
    "                if dummy:\n",
    "                    best_results[dataset].append(my_val)\n",
    "                    continue\n",
    "                is_best = my_val + eps >= np.max(best_results[dataset])\n",
    "                if len(best_results[dataset]) > 1:\n",
    "                    is_secd = my_val + eps >= np.sort(best_results[dataset])[-2]\n",
    "                else:\n",
    "                    is_secd = False\n",
    "                if show_pc:\n",
    "                    my_val = my_val * 100\n",
    "                latex_table += \" $\"\n",
    "                if is_best:\n",
    "                    latex_table += r\"\\tcf{\"\n",
    "                elif is_secd:\n",
    "                    latex_table += r\"\\tcs{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                latex_table += show_fmt.format(my_val)\n",
    "                latex_table += r\"}\" if is_best or is_secd else \" \"\n",
    "                latex_table += \"$\"\n",
    "            latex_table += r\" \\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\bottomrule\" + \"\\n\"\n",
    "    latex_table += r\"\\end{tabular}\" + \"\\n\"\n",
    "    latex_table += r\"}\" + \"\\n\"\n",
    "\n",
    "print()\n",
    "print(f\"There are {len(cmds)} commands to execute to generate missing datapoints\")\n",
    "if show_commands:\n",
    "    for cmd in cmds:\n",
    "        print(cmd)\n",
    "\n",
    "print()\n",
    "print(\"Done!\")\n",
    "print()\n",
    "print(f\"Here is your results table for {clusterer}:\")\n",
    "print()\n",
    "print()\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-9lxUOUuhaj"
   },
   "source": [
    "## Grouping by encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "n8sCItDK_2VT",
    "outputId": "18c89619-30d7-44fb-8f43-897901b62704",
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric_key = \"AMI\"\n",
    "show_pc = True\n",
    "show_fmt = \"{:4.0f}\"\n",
    "show_commands = False\n",
    "eps = 0.001\n",
    "override_fields = {\n",
    "    # \"predictions_dir\": \"y_pred\",\n",
    "}\n",
    "\n",
    "backbone = \"ResNet-50\"\n",
    "\n",
    "CLUSTERERS = [\n",
    "    \"KMeans\",\n",
    "    \"AgglomerativeClustering\",\n",
    "    \"AgglomerativeClustering\",\n",
    "    \"AffinityPropagation\",\n",
    "    \"HDBSCAN\",\n",
    "]\n",
    "print(MODEL2SH)\n",
    "\n",
    "best_results = {k: [] for k in TEST_DATASETS}\n",
    "for dummy in [True, False]:\n",
    "    cmds = []\n",
    "    latex_table = r\"% Results for \" + f\"{metric_key}, {backbone}\" + \"\\n\"\n",
    "    now_str = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    latex_table += r\"% Generated \" + now_str + \"\\n\"\n",
    "    latex_table += r\"% Using hparams \" + BEST_PARAMS[\"_version\"] + \"\\n\"\n",
    "    latex_table += r\"\\label{tab:\" + backbone + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"\\resizebox{\\textwidth}{!}{%\" + \"\\n\"\n",
    "    latex_table += r\"\\begin{tabular}{ll\" + r\"r\" * len(TEST_DATASETS) + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    latex_table += r\"& \" + f\"{'Clusterer':<11s}\"\n",
    "    for dataset in TEST_DATASETS:\n",
    "        latex_table += r\"&\" + \"{:^15s}\".format(DATASET2SH.get(dataset, dataset))\n",
    "    latex_table += r\"\\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    print(MODEL_GROUPS[backbone])\n",
    "    for i_group, model in enumerate(list(MODEL_GROUPS[backbone])):\n",
    "        print(model)\n",
    "        if i_group > 0:\n",
    "            latex_table += r\"\\midrule\" + \"\\n\"\n",
    "\n",
    "        first_agg = True\n",
    "        for i_clusterer, clusterer in enumerate(CLUSTERERS):\n",
    "            if i_clusterer == 0:\n",
    "                latex_table += r\"\\parbox[t]{2mm}{\\multirow{5}{*}{\\rotatebox[origin=c]{90}{\" + MODEL2SH[model] + \"}}}\"\n",
    "                latex_table += \"\\n\"\n",
    "            clusterername = CLUSTERER2SH.get(clusterer, clusterer)\n",
    "\n",
    "            my_override_fields = override_fields.copy()\n",
    "            if first_agg and clusterer == \"AgglomerativeClustering\" and metric_key != \"num_cluster_pred\":\n",
    "                first_agg = False\n",
    "                my_override_fields[\"aggclust_dist_thresh\"] = None\n",
    "                clusterername = \"AC  w/ C\"\n",
    "            elif clusterer == \"AgglomerativeClustering\":\n",
    "                clusterername = \"AC w/o C\"\n",
    "                if \"aggclust_dist_thresh\" in my_override_fields:\n",
    "                    del my_override_fields[\"aggclust_dist_thresh\"]\n",
    "\n",
    "            if clusterer == \"HDBSCAN\" and dataset in [\"celeba\", \"utkface\"]:\n",
    "                my_override_fields[\"min_samples\"] = 2\n",
    "            elif \"min_samples\" in my_override_fields:\n",
    "                del my_override_fields[\"min_samples\"]\n",
    "\n",
    "            latex_table += f\"& {clusterername:<10s}\"\n",
    "            for i_dataset, dataset in enumerate(TEST_DATASETS):\n",
    "                latex_table += \" &\"\n",
    "                filter1 = {\n",
    "                    \"model\": model,\n",
    "                    \"dataset\": dataset,\n",
    "                    \"clusterer\": clusterer,\n",
    "                }\n",
    "                filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "                filter2.update(filter1)\n",
    "                filter2.update(my_override_fields)\n",
    "                filter2 = fixup_filter(filter2)\n",
    "                sdf = select_rows(test_runs_df, filter2, allow_missing=False)\n",
    "                if len(sdf) < 1:\n",
    "                    # print(f\"No data for {filter2}\")\n",
    "                    cmds.append(filter2command(filter2, partition=\"test\"))\n",
    "                    continue\n",
    "                if len(sdf) > 1:\n",
    "                    if sum(np.abs(sdf[metric_key] - sdf.iloc[0][metric_key]) > 1e-6) > 0:\n",
    "                        print()\n",
    "                        print(\n",
    "                            f\"More than one result with {metric_key} values\",\n",
    "                            list(sdf[metric_key]),\n",
    "                        )\n",
    "                        print(f\"for search {filter2}\")\n",
    "                        dif_cols = find_differing_columns(sdf, config_keys)\n",
    "                        print(f\"columns which differ: {dif_cols}\")\n",
    "                        if dif_cols:\n",
    "                            for col in dif_cols:\n",
    "                                print(f\"  {col}: {list(sdf[col])}\")\n",
    "                my_val = np.median(sdf[metric_key])\n",
    "                if dummy:\n",
    "                    best_results[dataset].append(my_val)\n",
    "                    continue\n",
    "                is_best = my_val + eps >= np.max(best_results[dataset])\n",
    "                if len(best_results[dataset]) > 1:\n",
    "                    is_secd = my_val + eps >= np.sort(best_results[dataset])[-2]\n",
    "                else:\n",
    "                    is_secd = False\n",
    "                if show_pc:\n",
    "                    my_val = my_val * 100\n",
    "                latex_table += \" $\"\n",
    "                if is_best:\n",
    "                    latex_table += r\"\\tcf{\"\n",
    "                elif is_secd:\n",
    "                    latex_table += r\"\\tcs{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                latex_table += show_fmt.format(my_val)\n",
    "                latex_table += r\"}\" if is_best or is_secd else \" \"\n",
    "                latex_table += \"$\"\n",
    "            latex_table += r\" \\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\bottomrule\" + \"\\n\"\n",
    "    latex_table += r\"\\end{tabular}\" + \"\\n\"\n",
    "    latex_table += r\"}\" + \"\\n\"\n",
    "\n",
    "print()\n",
    "print(f\"There are {len(cmds)} commands to execute to generate missing datapoints\")\n",
    "if show_commands:\n",
    "    for cmd in cmds:\n",
    "        print(cmd)\n",
    "\n",
    "print()\n",
    "print(\"Done!\")\n",
    "print()\n",
    "print(f\"Here is your results table for {clusterer}:\")\n",
    "print()\n",
    "print()\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_isFUjNsuYc4"
   },
   "source": [
    "## Grouping by clusterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "MyWQRN9iqBkI",
    "outputId": "2642efe3-b0ef-46f3-dc70-f2f5c69645d4"
   },
   "outputs": [],
   "source": [
    "metric_key = \"AMI\"  # AMI  num_cluster_pred  silhouette-euclidean_pred  silhouette-og-euclidean_pred\n",
    "show_pc = True\n",
    "show_fmt = \"{:4.0f}\"\n",
    "show_commands = False\n",
    "highlight_best = True\n",
    "use_si_num = False\n",
    "eps = 0.005\n",
    "override_fields = {\n",
    "    # \"predictions_dir\": \"y_pred\",\n",
    "}\n",
    "\n",
    "backbone = \"all\"  # \"ResNet-50\" or \"ViT-B\"\n",
    "\n",
    "if metric_key == \"num_cluster_pred\":\n",
    "    CLUSTERERS = [\"AC w/o C\", \"AffinityPropagation\", \"HDBSCAN\"]\n",
    "    show_pc = False\n",
    "    show_fmt = \"{:4.0f}\"\n",
    "    highlight_best = False\n",
    "    use_si_num = True\n",
    "else:\n",
    "    CLUSTERERS = [\n",
    "        \"KMeans\",\n",
    "        \"SpectralClustering\",\n",
    "        \"AC w/ C\",\n",
    "        \"AC w/o C\",\n",
    "        \"AffinityPropagation\",\n",
    "        \"HDBSCAN\",\n",
    "    ]\n",
    "if metric_key.startswith(\"silhouette\"):\n",
    "    show_pc = False\n",
    "    show_fmt = \"{:5.2f}\"\n",
    "\n",
    "print(MODEL_GROUPS)\n",
    "\n",
    "best_results = {k: [] for k in TEST_DATASETS}\n",
    "best_results_grouped = {k: defaultdict(list) for k in TEST_DATASETS}\n",
    "\n",
    "for dummy in [True, False]:\n",
    "    cmds = []\n",
    "    latex_table = r\"% Results for \" + f\"{metric_key}, {backbone}\" + \"\\n\"\n",
    "    now_str = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    latex_table += r\"% Generated \" + now_str + \"\\n\"\n",
    "    latex_table += r\"% Using hparams \" + BEST_PARAMS[\"_version\"] + \"\\n\"\n",
    "    label = backbone\n",
    "    if metric_key == \"AMI\":\n",
    "        latex_table += r\"\\label{tab:\" + label + r\"}\" + \"\\n\"\n",
    "    label = metric_key.replace(\"_\", \"-\") + \":\" + label\n",
    "    latex_table += r\"\\label{tab:\" + label + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"\\resizebox{\\textwidth}{!}{%\" + \"\\n\"\n",
    "    latex_table += r\"\\begin{tabular}{ll\" + r\"r\" * len(TEST_DATASETS) + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    latex_table += r\"& \" + f\"{'Encoder':<11s}\"\n",
    "    for dataset in TEST_DATASETS:\n",
    "        latex_table += r\"&\" + \"{:^15s}\".format(DATASET2SH.get(dataset, dataset))\n",
    "    latex_table += r\"\\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    print(MODEL_GROUPS[backbone])\n",
    "    if metric_key == \"num_cluster_pred\":\n",
    "        latex_table += r\"& Num targets\"\n",
    "        for i_dataset, dataset in enumerate(TEST_DATASETS):\n",
    "            sdf = select_rows(test_runs_df, {\"dataset\": dataset}, allow_missing=False)\n",
    "            sdf = sdf[~pd.isna(sdf[\"num_cluster_true\"])]\n",
    "            latex_table += r\"& \"\n",
    "            latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "            latex_table += f\"{sdf.iloc[0]['num_cluster_true'].item()}\"\n",
    "            latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "        latex_table += r\"\\\\\" + \"\\n\"\n",
    "        latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    elif metric_key.endswith(\"_pred\"):\n",
    "        metric_key2 = metric_key.replace(\"_pred\", \"_true\")\n",
    "        clusterername = \"G.T.\"\n",
    "        latex_table += (\n",
    "            r\"\\parbox[t]{2mm}{\\multirow{\"\n",
    "            + str(len(MODEL_GROUPS[backbone]))\n",
    "            + r\"}{*}{\"\n",
    "            + r\"\\scalebox{0.9}{\"\n",
    "            + r\"\\rotatebox[origin=c]{90}{\"\n",
    "            + clusterername\n",
    "            + r\"}}}\"\n",
    "            + r\"}\"\n",
    "        )\n",
    "        latex_table += \"\\n\"\n",
    "        for i_group, model in enumerate(list(MODEL_GROUPS[backbone])):\n",
    "            latex_table += f\"& {MODEL2SH[model]:<10s}\"\n",
    "            for i_dataset, dataset in enumerate(TEST_DATASETS):\n",
    "                latex_table += \" &\"\n",
    "                filter1 = {\"model\": model, \"dataset\": dataset}\n",
    "                if model == \"timm_vit_base_patch16_224.mae\":\n",
    "                    filter1[\"dim_reducer\"] = \"PCA\"\n",
    "                    filter1[\"pca_variance\"] = 0.95\n",
    "                else:\n",
    "                    filter1[\"dim_reducer_man\"] = \"UMAP\"\n",
    "                    filter1[\"ndim_reduced_man\"] = 50\n",
    "                    filter1[\"dim_reducer_man_metric\"] = \"euclidean\"\n",
    "                sdf = select_rows(test_runs_df, filter1, allow_missing=False)\n",
    "                sdf = sdf[~pd.isna(sdf[metric_key2])]\n",
    "                my_val = np.nanmedian(sdf[metric_key])\n",
    "                if sum(sdf[metric_key2] != my_val) > 0:\n",
    "                    pass\n",
    "                if dummy:\n",
    "                    best_results_grouped[dataset][clusterername].append(my_val)\n",
    "                    continue\n",
    "                is_best_grp = my_val + eps >= np.max(best_results_grouped[dataset][clusterername])\n",
    "                latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "                latex_table += \"     \"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best_grp:\n",
    "                    latex_table += r\"\\tcg{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                latex_table += show_fmt.format(my_val)\n",
    "                if highlight_best:\n",
    "                    latex_table += r\"}\" if is_best_grp else \" \"\n",
    "                latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "\n",
    "            latex_table += r\" \\\\\" + \"\\n\"\n",
    "        latex_table += r\"\\toprule\" + \"\\n\"\n",
    "\n",
    "    first_agg = True\n",
    "    for i_clusterer, clusterer in enumerate(CLUSTERERS):\n",
    "        clusterername = CLUSTERER2SH.get(clusterer, clusterer)\n",
    "        my_override_fields = override_fields.copy()\n",
    "        if first_agg and clusterer == \"AgglomerativeClustering\" and metric_key != \"num_cluster_pred\":\n",
    "            first_agg = False\n",
    "            my_override_fields[\"aggclust_dist_thresh\"] = None\n",
    "            clusterername = \"AC  w/ C\"\n",
    "        elif clusterer == \"AgglomerativeClustering\":\n",
    "            clusterername = \"AC w/o C\"\n",
    "            if \"aggclust_dist_thresh\" in my_override_fields:\n",
    "                del my_override_fields[\"aggclust_dist_thresh\"]\n",
    "\n",
    "        if i_clusterer > 0:\n",
    "            latex_table += r\"\\midrule\" + \"\\n\"\n",
    "\n",
    "        latex_table += (\n",
    "            r\"\\parbox[t]{2mm}{\\multirow{\"\n",
    "            + str(len(MODEL_GROUPS[backbone]))\n",
    "            + r\"}{*}{\"\n",
    "            + r\"\\scalebox{0.9}{\"\n",
    "            + r\"\\rotatebox[origin=c]{90}{\"\n",
    "            + clusterername\n",
    "            + r\"}}}\"\n",
    "            + r\"}\"\n",
    "        )\n",
    "        latex_table += \"\\n\"\n",
    "\n",
    "        for i_group, model in enumerate(list(MODEL_GROUPS[backbone])):\n",
    "            latex_table += f\"& {MODEL2SH[model]:<10s}\"\n",
    "            for i_dataset, dataset in enumerate(TEST_DATASETS):\n",
    "                latex_table += \" &\"\n",
    "                filter1 = {\"model\": model, \"dataset\": dataset}\n",
    "                filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "                filter2.update(filter1)\n",
    "                filter2.update(my_override_fields)\n",
    "                filter2 = fixup_filter(filter2)\n",
    "                sdf = select_rows(test_runs_df, filter2, allow_missing=False)\n",
    "                if len(sdf) < 1:\n",
    "                    # print(f\"No data for {model}-{dataset}-{clusterer}\\n{filter2}\")\n",
    "                    if clusterer == \"AffinityPropagation\" and dataset in [\n",
    "                        \"imagenet\",\n",
    "                        \"places365\",\n",
    "                        \"imagenet-r\",\n",
    "                        \"svhn\",\n",
    "                        \"bioscan1m\",\n",
    "                        \"nabirds\",\n",
    "                    ]:\n",
    "                        cmds.append(filter2command(filter2, partition=\"test\"))\n",
    "                    if not dummy:\n",
    "                        # latex_table += r\"\\multicolumn{1}{c}{--}\"\n",
    "                        latex_table += r\"   --  \"\n",
    "                    continue\n",
    "                if len(sdf) > 1:\n",
    "                    if sum(np.abs(sdf[metric_key] - sdf.iloc[0][metric_key]) > 1e-6) > 0:\n",
    "                        print()\n",
    "                        print(\n",
    "                            f\"More than one result with {metric_key} values\",\n",
    "                            list(sdf[metric_key]),\n",
    "                        )\n",
    "                        print(f\"for search {filter2}\")\n",
    "                        dif_cols = find_differing_columns(sdf, config_keys)\n",
    "                        print(f\"columns which differ: {dif_cols}\")\n",
    "                        if dif_cols:\n",
    "                            for col in dif_cols:\n",
    "                                print(f\"  {col}: {list(sdf[col])}\")\n",
    "                my_val = np.nanmedian(sdf[metric_key])\n",
    "                if dummy:\n",
    "                    best_results[dataset].append(my_val)\n",
    "                    best_results_grouped[dataset][clusterername].append(my_val)\n",
    "                    continue\n",
    "                if np.isnan(my_val):\n",
    "                    latex_table += r\"   --  \"\n",
    "                    continue\n",
    "                is_best = my_val + eps >= np.max(best_results[dataset])\n",
    "                if len(best_results[dataset]) > 1:\n",
    "                    is_secd = my_val + eps >= np.sort(best_results[dataset])[-2]\n",
    "                else:\n",
    "                    is_secd = False\n",
    "                is_best_grp = my_val + eps >= np.max(best_results_grouped[dataset][clusterername])\n",
    "                sc_base = np.nanmedian(best_results[dataset])\n",
    "                sc_top = np.max(best_results[dataset])\n",
    "                sc = 100 * max(0, (my_val - sc_base) / (sc_top - sc_base))\n",
    "                latex_table += r\"\\cellcolor{cbg!\" + f\"{sc:.0f}\" + \"}\"\n",
    "                if show_pc:\n",
    "                    my_val = my_val * 100\n",
    "                latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best:\n",
    "                    latex_table += r\"\\tcf{\"\n",
    "                elif is_secd:\n",
    "                    latex_table += r\"\\tcs{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best_grp:\n",
    "                    latex_table += r\"\\tcg{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                latex_table += show_fmt.format(my_val)\n",
    "                if highlight_best:\n",
    "                    latex_table += r\"}\" if is_best or is_secd else \" \"\n",
    "                    latex_table += r\"}\" if is_best_grp else \" \"\n",
    "                latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "            latex_table += r\" \\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\bottomrule\" + \"\\n\"\n",
    "    latex_table += r\"\\end{tabular}\" + \"\\n\"\n",
    "    latex_table += r\"}\" + \"\\n\"\n",
    "\n",
    "print()\n",
    "print(f\"There are {len(cmds)} commands to execute to generate missing datapoints\")\n",
    "if show_commands:\n",
    "    for cmd in cmds:\n",
    "        print(cmd)\n",
    "\n",
    "print()\n",
    "print(\"Done!\")\n",
    "print()\n",
    "print(f\"Here is your results table for {metric_key}, {backbone}:\")\n",
    "print()\n",
    "print()\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cmd in cmds:\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With grouped datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_GROUPS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "MyWQRN9iqBkI",
    "outputId": "2642efe3-b0ef-46f3-dc70-f2f5c69645d4"
   },
   "outputs": [],
   "source": [
    "metric_key = \"AMI\"  # AMI  num_cluster_pred  silhouette-euclidean_pred  silhouette-og-euclidean_pred\n",
    "show_pc = True\n",
    "show_fmt = \"{:4.0f}\"\n",
    "show_commands = False\n",
    "highlight_best = True\n",
    "colour_bg = False\n",
    "use_si_num = False\n",
    "eps = 0.005\n",
    "override_fields = {\n",
    "    # \"predictions_dir\": \"y_pred\",\n",
    "}\n",
    "\n",
    "backbone = \"ViT-B\"  # \"ResNet-50\" \"ViT-B\"\n",
    "model_group = MODEL_GROUPS[backbone]\n",
    "# model_group = RESNET50_MODELS + FT_RESNET50_MODELS\n",
    "model_group = VITB16_MODELS + FT_VITB16_MODELS\n",
    "\n",
    "if metric_key == \"num_cluster_pred\":\n",
    "    CLUSTERERS = [\"AC w/o C\", \"AffinityPropagation\", \"HDBSCAN\"]\n",
    "    show_pc = False\n",
    "    show_fmt = \"{:4.0f}\"\n",
    "    highlight_best = False\n",
    "    use_si_num = True\n",
    "else:\n",
    "    CLUSTERERS = [\n",
    "        \"KMeans\",\n",
    "        \"SpectralClustering\",\n",
    "        \"AC w/ C\",\n",
    "        \"AC w/o C\",\n",
    "        \"AffinityPropagation\",\n",
    "        \"HDBSCAN\",\n",
    "    ]\n",
    "if metric_key.startswith(\"silhouette\"):\n",
    "    show_pc = False\n",
    "    show_fmt = \"{:5.2f}\"\n",
    "\n",
    "print(model_group)\n",
    "\n",
    "test_datasets = []\n",
    "for datagroupname, datagroupset in TEST_DATASETS_GROUPED.items():\n",
    "    test_datasets.extend(datagroupset)\n",
    "\n",
    "best_results = {k: [] for k in test_datasets}\n",
    "best_results_grouped = {k: defaultdict(list) for k in test_datasets}\n",
    "\n",
    "for dummy in [True, False]:\n",
    "    cmds = []\n",
    "    latex_table = r\"% Results for \" + f\"{metric_key}, {backbone}\" + \"\\n\"\n",
    "    now_str = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    latex_table += r\"% Generated \" + now_str + \"\\n\"\n",
    "    latex_table += r\"% Using hparams \" + BEST_PARAMS[\"_version\"] + \"\\n\"\n",
    "    label = backbone\n",
    "    if metric_key == \"AMI\":\n",
    "        latex_table += r\"\\label{tab:\" + label + r\"}\" + \"\\n\"\n",
    "    label = metric_key.replace(\"_\", \"-\") + \":\" + label\n",
    "    latex_table += r\"\\label{tab:\" + label + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"\\resizebox{\\linewidth}{!}{%\" + \"\\n\"\n",
    "    latex_table += r\"\\begin{tabular}{lll\" + r\"r\" * len(test_datasets) + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    latex_table += r\"& \" + f\"{'':<11s} &    \"\n",
    "    for datagroupname, datagroupset in TEST_DATASETS_GROUPED.items():\n",
    "        latex_table += r\" & \\multicolumn{\" + str(len(datagroupset)) + r\"}{c}{\" + datagroupname + r\"}\"\n",
    "    latex_table += r\"\\\\\" + \"\\n\"\n",
    "    icol = 4\n",
    "    for datagroupname, datagroupset in TEST_DATASETS_GROUPED.items():\n",
    "        latex_table += r\"\\cmidrule(l){\" + f\"{icol}-{icol + len(datagroupset) - 1}\" + r\"}\"\n",
    "        icol += len(datagroupset)\n",
    "    latex_table += \"\\n\"\n",
    "    latex_table += r\"& \" + f\"{'Encoder':<11s}\" + r\" & FT \"\n",
    "    for dataset in test_datasets:\n",
    "        latex_table += r\"&\" + \"{:^15s}\".format(DATASET2SH.get(dataset, dataset))\n",
    "    latex_table += r\"\\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\midrule\" + \"\\n\"\n",
    "    print(model_group)\n",
    "    if metric_key == \"num_cluster_pred\":\n",
    "        latex_table += r\"& \\textit{\\textnumero{} targets} & \"\n",
    "        for i_dataset, dataset in enumerate(test_datasets):\n",
    "            sdf = select_rows(test_runs_df, {\"dataset\": dataset}, allow_missing=False)\n",
    "            sdf = sdf[~pd.isna(sdf[\"num_cluster_true\"])]\n",
    "            latex_table += r\"& \"\n",
    "            latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "            latex_table += f\"{sdf.iloc[0]['num_cluster_true'].item():.0f}\"\n",
    "            latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "        latex_table += r\"\\\\\" + \"\\n\"\n",
    "        latex_table += r\"\\midrule\" + \"\\n\"\n",
    "    elif metric_key.endswith(\"_pred\"):\n",
    "        metric_key2 = metric_key.replace(\"_pred\", \"_true\")\n",
    "        clusterername = \"G.T.\"\n",
    "        latex_table += (\n",
    "            r\"\\parbox[t]{2mm}{\\multirow{\"\n",
    "            + str(len(model_group))\n",
    "            + r\"}{*}{\"\n",
    "            + r\"\\scalebox{0.9}{\"\n",
    "            + r\"\\rotatebox[origin=c]{90}{\"\n",
    "            + clusterername\n",
    "            + r\"}}}\"\n",
    "            + r\"}\"\n",
    "        )\n",
    "        latex_table += \"\\n\"\n",
    "        for i_group, model in enumerate(list(model_group)):\n",
    "            model_sh = MODEL2SH.get(model, model)\n",
    "            if model_sh.endswith(\" [FT]\"):\n",
    "                model_sh = f\"{model_sh[:-4]:<10s}\" + r\" & \\checkmark\"\n",
    "            else:\n",
    "                model_sh = f\"{model_sh:<10s}\" + \" &\"\n",
    "            latex_table += f\"& {model_sh:<23s}\"\n",
    "            for i_dataset, dataset in enumerate(test_datasets):\n",
    "                latex_table += \" &\"\n",
    "                filter1 = {\"model\": model, \"dataset\": dataset}\n",
    "                if model == \"timm_vit_base_patch16_224.mae\":\n",
    "                    filter1[\"dim_reducer\"] = \"PCA\"\n",
    "                    filter1[\"pca_variance\"] = 0.95\n",
    "                else:\n",
    "                    filter1[\"dim_reducer_man\"] = \"UMAP\"\n",
    "                    filter1[\"ndim_reduced_man\"] = 50\n",
    "                    filter1[\"dim_reducer_man_metric\"] = \"euclidean\"\n",
    "                sdf = select_rows(test_runs_df, filter1, allow_missing=False)\n",
    "                sdf = sdf[~pd.isna(sdf[metric_key2])]\n",
    "                my_val = np.nanmedian(sdf[metric_key])\n",
    "                if sum(sdf[metric_key2] != my_val) > 0:\n",
    "                    pass\n",
    "                if dummy:\n",
    "                    best_results_grouped[dataset][clusterername].append(my_val)\n",
    "                    continue\n",
    "                is_best_grp = my_val + eps >= np.max(best_results_grouped[dataset][clusterername])\n",
    "                latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "                latex_table += \"     \"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best_grp:\n",
    "                    latex_table += r\"\\tcg{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                latex_table += show_fmt.format(my_val)\n",
    "                if highlight_best:\n",
    "                    latex_table += r\"}\" if is_best_grp else \" \"\n",
    "                latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "\n",
    "            latex_table += r\" \\\\\" + \"\\n\"\n",
    "        latex_table += r\"\\midrule\" + \"\\n\"\n",
    "\n",
    "    first_agg = True\n",
    "    for i_clusterer, clusterer in enumerate(CLUSTERERS):\n",
    "        clusterername = CLUSTERER2SH.get(clusterer, clusterer)\n",
    "        my_override_fields = override_fields.copy()\n",
    "        if first_agg and clusterer == \"AgglomerativeClustering\" and metric_key != \"num_cluster_pred\":\n",
    "            first_agg = False\n",
    "            my_override_fields[\"aggclust_dist_thresh\"] = None\n",
    "            clusterername = \"AC  w/ C\"\n",
    "        elif clusterer == \"AgglomerativeClustering\":\n",
    "            clusterername = \"AC w/o C\"\n",
    "            if \"aggclust_dist_thresh\" in my_override_fields:\n",
    "                del my_override_fields[\"aggclust_dist_thresh\"]\n",
    "\n",
    "        if i_clusterer > 0:\n",
    "            latex_table += r\"\\midrule\" + \"\\n\"\n",
    "\n",
    "        latex_table += (\n",
    "            r\"\\parbox[t]{2mm}{\\multirow{\"\n",
    "            + str(len(model_group))\n",
    "            + r\"}{*}{\"\n",
    "            + r\"\\scalebox{0.9}{\"\n",
    "            + r\"\\rotatebox[origin=c]{90}{\"\n",
    "            + clusterername\n",
    "            + r\"}}}\"\n",
    "            + r\"}\"\n",
    "        )\n",
    "        latex_table += \"\\n\"\n",
    "\n",
    "        for i_group, model in enumerate(list(model_group)):\n",
    "            model_sh = MODEL2SH.get(model, model)\n",
    "            if model_sh.endswith(\" [FT]\"):\n",
    "                model_sh = f\"{model_sh[:-4]:<10s}\" + r\" & \\checkmark\"\n",
    "            else:\n",
    "                model_sh = f\"{model_sh:<10s}\" + \" &\"\n",
    "            latex_table += f\"& {model_sh:<23s}\"\n",
    "            for i_dataset, dataset in enumerate(test_datasets):\n",
    "                latex_table += \" &\"\n",
    "                filter1 = {\"model\": model, \"dataset\": dataset}\n",
    "                filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "                filter2.update(filter1)\n",
    "                filter2.update(my_override_fields)\n",
    "                filter2 = fixup_filter(filter2)\n",
    "                sdf = select_rows(test_runs_df, filter2, allow_missing=False)\n",
    "                if len(sdf) < 1:\n",
    "                    # print(f\"No data for {model}-{dataset}-{clusterer}\\n{filter2}\")\n",
    "                    cmds.append(filter2command(filter2, partition=\"test\"))\n",
    "                    if not dummy:\n",
    "                        # latex_table += r\"\\multicolumn{1}{c}{--}\"\n",
    "                        latex_table += r\"   --  \"\n",
    "                    continue\n",
    "                if len(sdf) > 1:\n",
    "                    if sum(np.abs(sdf[metric_key] - sdf.iloc[0][metric_key]) > 1e-6) > 0:\n",
    "                        print()\n",
    "                        print(\n",
    "                            f\"More than one result with {metric_key} values\",\n",
    "                            list(sdf[metric_key]),\n",
    "                        )\n",
    "                        print(f\"for search {filter2}\")\n",
    "                        dif_cols = find_differing_columns(sdf, config_keys)\n",
    "                        print(f\"columns which differ: {dif_cols}\")\n",
    "                        if dif_cols:\n",
    "                            for col in dif_cols:\n",
    "                                print(f\"  {col}: {list(sdf[col])}\")\n",
    "                my_val = np.nanmedian(sdf[metric_key])\n",
    "                if dummy:\n",
    "                    best_results[dataset].append(my_val)\n",
    "                    best_results_grouped[dataset][clusterername].append(my_val)\n",
    "                    continue\n",
    "                if np.isnan(my_val):\n",
    "                    latex_table += r\"   --  \"\n",
    "                    continue\n",
    "                is_best = my_val + eps >= np.max(best_results[dataset])\n",
    "                if len(best_results[dataset]) > 1:\n",
    "                    is_secd = my_val + eps >= np.sort(best_results[dataset])[-2]\n",
    "                else:\n",
    "                    is_secd = False\n",
    "                is_best_grp = my_val + eps >= np.max(best_results_grouped[dataset][clusterername])\n",
    "                sc_base = np.nanmedian(best_results[dataset])\n",
    "                sc_top = np.max(best_results[dataset])\n",
    "                sc = 100 * max(0, (my_val - sc_base) / (sc_top - sc_base))\n",
    "                if colour_bg:\n",
    "                    latex_table += r\"\\cellcolor{cbg!\" + f\"{sc:.0f}\" + \"}\"\n",
    "                if show_pc:\n",
    "                    my_val = my_val * 100\n",
    "                latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best:\n",
    "                    latex_table += r\"\\tcf{\"\n",
    "                elif is_secd:\n",
    "                    latex_table += r\"\\tcs{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best_grp:\n",
    "                    latex_table += r\"\\tcg{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                latex_table += show_fmt.format(my_val)\n",
    "                if highlight_best:\n",
    "                    latex_table += r\"}\" if is_best or is_secd else \" \"\n",
    "                    latex_table += r\"}\" if is_best_grp else \" \"\n",
    "                latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "            latex_table += r\" \\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\bottomrule\" + \"\\n\"\n",
    "    latex_table += r\"\\end{tabular}\" + \"\\n\"\n",
    "    latex_table += r\"}\" + \"\\n\"\n",
    "\n",
    "print()\n",
    "print(f\"There are {len(cmds)} commands to execute to generate missing datapoints\")\n",
    "if show_commands:\n",
    "    for cmd in cmds:\n",
    "        print(cmd)\n",
    "\n",
    "print()\n",
    "print(\"Done!\")\n",
    "print()\n",
    "print(f\"Here is your results table for {metric_key}, {backbone}:\")\n",
    "print()\n",
    "print()\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_key = \"num_cluster_pred\"  # AMI  num_cluster_pred  silhouette-euclidean_pred  silhouette-og-euclidean_pred\n",
    "clusterers = [\"AC w/o C\", \"AffinityPropagation\", \"HDBSCAN\"]\n",
    "show_pc = False\n",
    "show_fmt = \"{:4.0f}\"\n",
    "show_commands = False\n",
    "highlight_best = False\n",
    "use_si_num = True\n",
    "highlight_best = True\n",
    "colour_bg = True\n",
    "eps = 0.005\n",
    "\n",
    "\n",
    "backbone = \"ViT-B\"  # \"ResNet-50\" \"ViT-B\"\n",
    "model_group = MODEL_GROUPS[backbone]\n",
    "# model_group = RESNET50_MODELS + FT_RESNET50_MODELS\n",
    "model_group = VITB16_MODELS + FT_VITB16_MODELS\n",
    "test_datasets_grouped = TEST_DATASETS_GROUPED\n",
    "\n",
    "print(model_group)\n",
    "\n",
    "\n",
    "######################################\n",
    "# Get the number of clusters for everything\n",
    "\n",
    "model_groups = {\n",
    "    \"---\": [\"none\"],\n",
    "    \"RN50\": RESNET50_MODELS + FT_RESNET50_MODELS,\n",
    "    \"ViT-B\": VITB16_MODELS + FT_VITB16_MODELS,\n",
    "}\n",
    "\n",
    "model_groups_flattened = make_flat_hierarchy_from_dict(model_groups, pad_right=False)\n",
    "model_groups_flattened = np.array(model_groups_flattened)\n",
    "model_groups_flattened = model_groups_flattened[:, -1]\n",
    "\n",
    "test_datasets = []\n",
    "for datagroupname, datagroupset in test_datasets_grouped.items():\n",
    "    test_datasets.extend(datagroupset)\n",
    "\n",
    "\n",
    "print(\"Encoders:\")\n",
    "print(model_groups_flattened)\n",
    "\n",
    "print(\"Datasets:\")\n",
    "print(test_datasets)\n",
    "\n",
    "result_table = build_results_table(\n",
    "    model_groups_flattened,\n",
    "    clusterers,\n",
    "    test_datasets,\n",
    "    metric_keys=[\"num_cluster_pred\", \"num_cluster_true\"],\n",
    ")\n",
    "# Shaped [models, clusterers, datasets]\n",
    "print(\"result_table.shape\", result_table.shape)\n",
    "\n",
    "######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table_min = np.nanmin(result_table[..., 0], axis=1, keepdims=True)\n",
    "result_table_max = np.nanmax(result_table[..., 0], axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table_logratio = np.log(result_table[..., 0] / result_table[..., 1])\n",
    "result_table_logratiomin = np.nanmin(result_table_logratio, axis=(0, 1))\n",
    "result_table_logratiomax = np.nanmax(result_table_logratio, axis=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table_logratioabsmax = np.nanmax(np.abs(result_table_logratio), axis=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table_logratioabsmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "MyWQRN9iqBkI",
    "outputId": "2642efe3-b0ef-46f3-dc70-f2f5c69645d4"
   },
   "outputs": [],
   "source": [
    "clusterers = [\"AC w/o C\", \"AffinityPropagation\", \"HDBSCAN\"]\n",
    "show_fmt = \"{:4.0f}\"\n",
    "show_commands = False\n",
    "highlight_best = False\n",
    "use_si_num = False\n",
    "highlight_best = True\n",
    "colour_bg = True\n",
    "eps = 0.005\n",
    "merge_model_group_column = True\n",
    "\n",
    "\n",
    "######################################\n",
    "# Get the number of clusters for everything\n",
    "\n",
    "model_groups = {\n",
    "    \"\": [\"none\"],\n",
    "    \"RN50\": RESNET50_MODELS + FT_RESNET50_MODELS,\n",
    "    \"ViT-B\": VITB16_MODELS + FT_VITB16_MODELS,\n",
    "}\n",
    "\n",
    "model_groups_flattened = make_flat_hierarchy_from_dict(model_groups, pad_right=False)\n",
    "model_groups_flattened = np.array(model_groups_flattened)\n",
    "model_groups_flattened = model_groups_flattened[:, -1]\n",
    "\n",
    "test_datasets = []\n",
    "for datagroupname, datagroupset in test_datasets_grouped.items():\n",
    "    test_datasets.extend(datagroupset)\n",
    "\n",
    "\n",
    "print(\"Encoders:\")\n",
    "print(model_groups_flattened)\n",
    "\n",
    "print(\"Datasets:\")\n",
    "print(test_datasets)\n",
    "\n",
    "metric_keys = [\"num_cluster_pred\", \"num_cluster_true\"]\n",
    "result_table, cmds = build_results_table(\n",
    "    model_groups_flattened,\n",
    "    clusterers,\n",
    "    test_datasets,\n",
    "    metric_keys=metric_keys,\n",
    "    return_cmds=True,\n",
    ")\n",
    "# Shaped [models, clusterers, datasets]\n",
    "print(\"result_table.shape\", result_table.shape)\n",
    "\n",
    "result_table_logratio = np.log(result_table[..., 0] / result_table[..., 1])\n",
    "result_table_logratiomin = np.nanmin(result_table_logratio, axis=(0, 1))\n",
    "result_table_logratiomax = np.nanmax(result_table_logratio, axis=(0, 1))\n",
    "result_table_logratioabsmax = np.nanmax(np.abs(result_table_logratio), axis=(0, 1))\n",
    "\n",
    "######################################\n",
    "\n",
    "metric_key = metric_keys[0]\n",
    "\n",
    "print(\"Encoders:\")\n",
    "print(model_groups_flattened)\n",
    "\n",
    "print(\"Datasets:\")\n",
    "print(test_datasets)\n",
    "\n",
    "print()\n",
    "print()\n",
    "latex_table = r\"\\begin{landscape}\" + \"\\n\"\n",
    "for i_clusterer, clusterer in enumerate(clusterers):\n",
    "    clusterername = CLUSTERER2SH.get(clusterer, clusterer)\n",
    "\n",
    "    latex_table += r\"\\begin{table}\" + \"\\n\"\n",
    "    latex_table += r\"\\captionsetup{width=.707\\linewidth}\" + \"\\n\"\n",
    "    latex_table += r\"\\caption{\" + \"\\n\"\n",
    "    latex_table += r\"\\textbf{Number of clusters generating using \" + clusterername + \".}\\n\"\n",
    "    latex_table += r\"}\" + \"\\n\"\n",
    "\n",
    "    latex_table += r\"% Results for \" + f\"{metric_key}, {clusterer}\" + \"\\n\"\n",
    "    now_str = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    latex_table += r\"% Generated \" + now_str + \"\\n\"\n",
    "    latex_table += r\"% Using hparams \" + BEST_PARAMS[\"_version\"] + \"\\n\"\n",
    "    label = metric_key.replace(\"_\", \"-\") + \":\" + clusterer\n",
    "    latex_table += r\"\\label{tab:\" + label + r\"}\" + \"\\n\"\n",
    "\n",
    "    latex_table += r\"\\resizebox{\\columnwidth}{!}{%\" + \"\\n\"\n",
    "    latex_table += r\"\\begin{tabular}{\"\n",
    "    if not merge_model_group_column:\n",
    "        latex_table += \"l\"\n",
    "    latex_table += \"ll\" + r\"r\" * len(test_datasets) + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    # Begin dataset group header row\n",
    "    if len(test_datasets_grouped) > 1:\n",
    "        if not merge_model_group_column:\n",
    "            latex_table += r\"& \"\n",
    "        latex_table += f\"{'':<11s}\" + r\" &   \"\n",
    "        for datagroupname, datagroupset in test_datasets_grouped.items():\n",
    "            latex_table += r\" & \\multicolumn{\" + str(len(datagroupset)) + r\"}{c}{\" + datagroupname + r\"}\"\n",
    "        latex_table += r\"\\\\\" + \"\\n\"\n",
    "        icol = 3\n",
    "        if not merge_model_group_column:\n",
    "            icol += 1\n",
    "        for datagroupname, datagroupset in test_datasets_grouped.items():\n",
    "            latex_table += r\"\\cmidrule(l){\" + f\"{icol}-{icol + len(datagroupset) - 1}\" + r\"}\"\n",
    "            icol += len(datagroupset)\n",
    "        latex_table += \"\\n\"\n",
    "    # Begin main header row, with actual dataset names\n",
    "    if merge_model_group_column:\n",
    "        latex_table += r\"\\quad \"\n",
    "    else:\n",
    "        latex_table += r\"Arch. & \"\n",
    "    latex_table += f\"{'Encoder':<11s}\" + r\" & FT \"\n",
    "    for dataset in test_datasets:\n",
    "        latex_table += r\"&\" + \"{:^15s}\".format(DATASET2SH.get(dataset, dataset))\n",
    "    latex_table += r\"\\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\midrule\" + \"\\n\"\n",
    "\n",
    "    # Ground truth number of targets\n",
    "    latex_table += r\"\\textit{\\textnumero{} GT classes} & \"\n",
    "    for i_dataset, dataset in enumerate(test_datasets):\n",
    "        my_val = np.nanmean(result_table[:, :, i_dataset, 1])\n",
    "        latex_table += r\"& \"\n",
    "        latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "        latex_table += show_fmt.format(my_val)\n",
    "        latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "    latex_table += r\"\\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\midrule\" + \"\\n\"\n",
    "\n",
    "    i_model_o = -1\n",
    "    for i_group, group in enumerate(model_groups):\n",
    "        if i_group > 0:\n",
    "            latex_table += r\"\\midrule\" + \"\\n\"\n",
    "        if merge_model_group_column:\n",
    "            if not group:\n",
    "                latex_table += r\"\\quad \"\n",
    "            else:\n",
    "                latex_table += r\"\\textbf{\" + group + r\"} --- \"\n",
    "        elif not group:\n",
    "            latex_table += \"---\" + \"\\n\"\n",
    "        else:\n",
    "            latex_table += group + \"\\n\"\n",
    "        for i_model, model in enumerate(list(model_groups[group])):\n",
    "            i_model_o += 1\n",
    "            model_sh = MODEL2SH.get(model, model)\n",
    "            if model_sh.endswith(\" [FT]\"):\n",
    "                model_sh = f\"{model_sh[:-4]:<10s}\" + r\" & \\checkmark\"\n",
    "            else:\n",
    "                model_sh = f\"{model_sh:<10s}\" + \" &\"\n",
    "            if merge_model_group_column and i_model > 0:\n",
    "                latex_table += r\"\\quad \"\n",
    "            latex_table += f\"{model_sh:<23s}\"\n",
    "            for i_dataset, dataset in enumerate(test_datasets):\n",
    "                latex_table += \" &\"\n",
    "                my_val = result_table[i_model_o, i_clusterer, i_dataset, 0]\n",
    "                if np.isnan(my_val):\n",
    "                    latex_table += r\"   --  \"\n",
    "                    continue\n",
    "                is_best = np.abs(result_table_logratio[i_model_o, i_clusterer, i_dataset]) - eps <= np.nanmin(\n",
    "                    np.abs(result_table_logratio[:, :, i_dataset])\n",
    "                )\n",
    "                is_secd = False\n",
    "                is_best_grp = np.abs(result_table_logratio[i_model_o, i_clusterer, i_dataset]) - eps <= np.nanmin(\n",
    "                    np.abs(result_table_logratio[:, i_clusterer, i_dataset])\n",
    "                )\n",
    "                sc_base = 0\n",
    "                # sc_top = np.max(best_results[dataset])\n",
    "                # sc = 100 * max(0, (my_val - sc_base) / (sc_top - sc_base))\n",
    "                if colour_bg:\n",
    "                    sc = result_table_logratio[i_model_o, i_clusterer, i_dataset]\n",
    "                    sc = 100.0 * sc / result_table_logratioabsmax[i_dataset]\n",
    "                    latex_table += r\"\\cc{\"\n",
    "                    if sc < 0:\n",
    "                        latex_table += r\"cbr!\"\n",
    "                    else:\n",
    "                        latex_table += r\"cbg!\"\n",
    "                    latex_table += f\"{abs(sc):.0f}\" + \"}\"\n",
    "\n",
    "                latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best:\n",
    "                    latex_table += r\"\\tcf{\"\n",
    "                elif is_secd:\n",
    "                    latex_table += r\"\\tcs{\"\n",
    "                else:\n",
    "                    latex_table += \"\"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best_grp:\n",
    "                    latex_table += r\"\\tcg{\"\n",
    "                else:\n",
    "                    latex_table += \"\"\n",
    "                latex_table += show_fmt.format(my_val)\n",
    "                if highlight_best:\n",
    "                    latex_table += r\"}\" if is_best or is_secd else \"\"\n",
    "                    latex_table += r\"}\" if is_best_grp else \"\"\n",
    "                latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "            latex_table += r\" \\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\bottomrule\" + \"\\n\"\n",
    "    latex_table += r\"\\end{tabular}\" + \"\\n\"\n",
    "    latex_table += r\"}\" + \"\\n\"\n",
    "    latex_table += r\"\\end{table}\" + \"\\n\"\n",
    "latex_table += r\"\\end{landscape}\"\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(f\"There are {len(cmds)} commands to execute to generate missing datapoints\")\n",
    "if show_commands:\n",
    "    for cmd in cmds:\n",
    "        print(cmd)\n",
    "\n",
    "print()\n",
    "print(\"Done!\")\n",
    "print()\n",
    "print(f\"Here is your results table for {metric_key}, {backbone}:\")\n",
    "print()\n",
    "print()\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table_logratio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table_logratioabsmax.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table_logratio / result_table_logratioabsmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single clusterer, grouped encoders, grouped datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTERERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "MyWQRN9iqBkI",
    "outputId": "2642efe3-b0ef-46f3-dc70-f2f5c69645d4"
   },
   "outputs": [],
   "source": [
    "clusterer = \"HDBSCAN\"  # \"AC w/o C\"\n",
    "metric_key = \"AMI\"  # AMI  num_cluster_pred  silhouette-euclidean_pred  silhouette-og-euclidean_pred\n",
    "show_pc = True\n",
    "show_fmt = \"{:4.0f}\"\n",
    "show_commands = False\n",
    "highlight_best = True\n",
    "use_si_num = False\n",
    "eps = 0.005\n",
    "override_fields = {\n",
    "    # \"predictions_dir\": \"y_pred\",\n",
    "}\n",
    "\n",
    "model_groups = {\n",
    "    \"---\": [\"none\"],\n",
    "    \"RN50\": RESNET50_MODELS + FT_RESNET50_MODELS,\n",
    "    \"ViT-B\": VITB16_MODELS + FT_VITB16_MODELS,\n",
    "}\n",
    "\n",
    "test_datasets_grouped, dataset2sh = TEST_DATASETS_GROUPED, DATASET2SH\n",
    "# test_datasets_grouped, dataset2sh = {\"in9_bg_challenge\": IN9_DATASETS + [\"in9bggap\"]}, IN92SH  # IN9 special\n",
    "\n",
    "if metric_key == \"num_cluster_pred\":\n",
    "    CLUSTERERS = [\"AC w/o C\", \"AffinityPropagation\", \"HDBSCAN\"]\n",
    "    show_pc = False\n",
    "    show_fmt = \"{:4.0f}\"\n",
    "    highlight_best = False\n",
    "    use_si_num = True\n",
    "else:\n",
    "    CLUSTERERS = [\n",
    "        \"KMeans\",\n",
    "        \"SpectralClustering\",\n",
    "        \"AC w/ C\",\n",
    "        \"AC w/o C\",\n",
    "        \"AffinityPropagation\",\n",
    "        \"HDBSCAN\",\n",
    "    ]\n",
    "if metric_key.startswith(\"silhouette\"):\n",
    "    show_pc = False\n",
    "    show_fmt = \"{:5.2f}\"\n",
    "\n",
    "print(model_groups)\n",
    "\n",
    "test_datasets = []\n",
    "for datagroupname, datagroupset in test_datasets_grouped.items():\n",
    "    test_datasets.extend(datagroupset)\n",
    "\n",
    "best_results = {k: [] for k in test_datasets}\n",
    "best_results_grouped = {k: defaultdict(list) for k in test_datasets}\n",
    "\n",
    "for dummy in [True, False]:\n",
    "    cmds = []\n",
    "    latex_table = r\"% Results for \" + f\"{metric_key}, {clusterer}\" + \"\\n\"\n",
    "    now_str = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    latex_table += r\"% Generated \" + now_str + \"\\n\"\n",
    "    latex_table += r\"% Using hparams \" + BEST_PARAMS[\"_version\"] + \"\\n\"\n",
    "    label = clusterer\n",
    "    if metric_key == \"AMI\":\n",
    "        latex_table += r\"\\label{tab:\" + label + r\"}\" + \"\\n\"\n",
    "    label = metric_key.replace(\"_\", \"-\") + \":\" + label\n",
    "    latex_table += r\"\\label{tab:\" + label + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"\\resizebox{\\columnwidth}{!}{%\" + \"\\n\"\n",
    "    latex_table += r\"\\begin{tabular}{lll\" + r\"r\" * len(test_datasets) + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    # Begin dataset group header row\n",
    "    if len(test_datasets_grouped) > 1:\n",
    "        latex_table += r\"& \" + f\"{'':<11s}\" + r\" &   \"\n",
    "        for datagroupname, datagroupset in test_datasets_grouped.items():\n",
    "            latex_table += r\" & \\multicolumn{\" + str(len(datagroupset)) + r\"}{c}{\" + datagroupname + r\"}\"\n",
    "        latex_table += r\"\\\\\" + \"\\n\"\n",
    "        icol = 4\n",
    "        for datagroupname, datagroupset in test_datasets_grouped.items():\n",
    "            latex_table += r\"\\cmidrule(l){\" + f\"{icol}-{icol + len(datagroupset) - 1}\" + r\"}\"\n",
    "            icol += len(datagroupset)\n",
    "        latex_table += \"\\n\"\n",
    "    # Begin main header row, with actual dataset names\n",
    "    latex_table += r\"Arch. & \" + f\"{'Encoder':<11s}\" + r\" & FT \"\n",
    "    for dataset in test_datasets:\n",
    "        latex_table += r\"&\" + \"{:^15s}\".format(dataset2sh.get(dataset, dataset))\n",
    "    latex_table += r\"\\\\\" + \"\\n\"\n",
    "    # Begin table contents\n",
    "    latex_table += r\"\\midrule\" + \"\\n\"\n",
    "    if metric_key == \"num_cluster_pred\":\n",
    "        latex_table += r\"& Num targets\"\n",
    "        for i_dataset, dataset in enumerate(test_datasets):\n",
    "            sdf = select_rows(test_runs_df, {\"dataset\": dataset}, allow_missing=False)\n",
    "            sdf = sdf[~pd.isna(sdf[\"num_cluster_true\"])]\n",
    "            latex_table += r\"& \"\n",
    "            latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "            latex_table += f\"{sdf.iloc[0]['num_cluster_true'].item()}\"\n",
    "            latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "        latex_table += r\"\\\\\" + \"\\n\"\n",
    "        latex_table += r\"\\midrule\" + \"\\n\"\n",
    "    elif metric_key.endswith(\"_pred\"):\n",
    "        metric_key2 = metric_key.replace(\"_pred\", \"_true\")\n",
    "        clusterername = \"G.T.\"\n",
    "        latex_table += (\n",
    "            r\"\\parbox[t]{2mm}{\\multirow{\"\n",
    "            + str(len(model_groups[group]))\n",
    "            + r\"}{*}{\"\n",
    "            + r\"\\scalebox{0.9}{\"\n",
    "            + r\"\\rotatebox[origin=c]{90}{\"\n",
    "            + clusterername\n",
    "            + r\"}}}\"\n",
    "            + r\"}\"\n",
    "        )\n",
    "        latex_table += \"\\n\"\n",
    "        for i_model, model in enumerate(list(model_groups[group])):\n",
    "            latex_table += f\"& {MODEL2SH[model]:<10s}\"\n",
    "            for i_dataset, dataset in enumerate(test_datasets):\n",
    "                latex_table += \" &\"\n",
    "                filter1 = {\"model\": model, \"dataset\": dataset}\n",
    "                if model == \"timm_vit_base_patch16_224.mae\":\n",
    "                    filter1[\"dim_reducer\"] = \"PCA\"\n",
    "                    filter1[\"pca_variance\"] = 0.95\n",
    "                else:\n",
    "                    filter1[\"dim_reducer_man\"] = \"UMAP\"\n",
    "                    filter1[\"ndim_reduced_man\"] = 50\n",
    "                    filter1[\"dim_reducer_man_metric\"] = \"euclidean\"\n",
    "                sdf = select_rows(test_runs_df, filter1, allow_missing=False)\n",
    "                sdf = sdf[~pd.isna(sdf[metric_key2])]\n",
    "                my_val = np.nanmedian(sdf[metric_key])\n",
    "                if sum(sdf[metric_key2] != my_val) > 0:\n",
    "                    pass\n",
    "                if dummy:\n",
    "                    best_results_grouped[dataset][group].append(my_val)\n",
    "                    continue\n",
    "                is_best_grp = my_val + eps >= np.max(best_results_grouped[dataset][group])\n",
    "                latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "                latex_table += \"     \"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best_grp:\n",
    "                    latex_table += r\"\\tcg{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                latex_table += show_fmt.format(my_val)\n",
    "                if highlight_best:\n",
    "                    latex_table += r\"}\" if is_best_grp else \" \"\n",
    "                latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "\n",
    "            latex_table += r\" \\\\\" + \"\\n\"\n",
    "        latex_table += r\"\\midrule\" + \"\\n\"\n",
    "\n",
    "    first_agg = True\n",
    "    clusterername = CLUSTERER2SH.get(clusterer, clusterer)\n",
    "    my_override_fields = override_fields.copy()\n",
    "    if first_agg and clusterer == \"AgglomerativeClustering\" and metric_key != \"num_cluster_pred\":\n",
    "        first_agg = False\n",
    "        my_override_fields[\"aggclust_dist_thresh\"] = None\n",
    "        clusterername = \"AC  w/ C\"\n",
    "    elif clusterer == \"AgglomerativeClustering\":\n",
    "        clusterername = \"AC w/o C\"\n",
    "        if \"aggclust_dist_thresh\" in my_override_fields:\n",
    "            del my_override_fields[\"aggclust_dist_thresh\"]\n",
    "\n",
    "    for i_group, group in enumerate(model_groups):\n",
    "        if i_group > 0:\n",
    "            latex_table += r\"\\midrule\" + \"\\n\"\n",
    "        latex_table += group + \"\\n\"\n",
    "        for i_model, model in enumerate(list(model_groups[group])):\n",
    "            model_sh = MODEL2SH.get(model, model)\n",
    "            if model_sh.endswith(\" [FT]\"):\n",
    "                model_sh = f\"{model_sh[:-4]:<10s}\" + r\" & \\checkmark\"\n",
    "            else:\n",
    "                model_sh = f\"{model_sh:<10s}\" + \" &\"\n",
    "            latex_table += f\"& {model_sh:<23s}\"\n",
    "            for i_dataset, dataset in enumerate(test_datasets):\n",
    "                latex_table += \" &\"\n",
    "                filter1 = {\"model\": model, \"dataset\": dataset}\n",
    "                filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "                filter2.update(filter1)\n",
    "                filter2.update(my_override_fields)\n",
    "                filter2 = fixup_filter(filter2)\n",
    "                if dataset == \"in9bggap\":\n",
    "                    filter2[\"dataset\"] = \"in9mixedsame\"\n",
    "                sdf = select_rows(test_runs_df, filter2, allow_missing=False)\n",
    "                if len(sdf) < 1:\n",
    "                    # print(f\"No data for {model}-{dataset}-{clusterer}\\n{filter2}\")\n",
    "                    # if dataset == \"imagenet-sketch\":  # not in [\"imagenet\", \"imagenet-sketch\", \"inaturalist\"]:\n",
    "                    cmds.append(filter2command(filter2, partition=\"test\"))\n",
    "                    if not dummy:\n",
    "                        # latex_table += r\"\\multicolumn{1}{c}{--}\"\n",
    "                        latex_table += r\"   --  \"\n",
    "                    continue\n",
    "                if len(sdf) > 1:\n",
    "                    if sum(np.abs(sdf[metric_key] - sdf.iloc[0][metric_key]) > 1e-6) > 0:\n",
    "                        print()\n",
    "                        print(\n",
    "                            f\"More than one result with {metric_key} values\",\n",
    "                            list(sdf[metric_key]),\n",
    "                        )\n",
    "                        print(f\"for search {filter2}\")\n",
    "                        dif_cols = find_differing_columns(sdf, config_keys)\n",
    "                        print(f\"columns which differ: {dif_cols}\")\n",
    "                        if dif_cols:\n",
    "                            for col in dif_cols:\n",
    "                                print(f\"  {col}: {list(sdf[col])}\")\n",
    "                my_val = np.nanmedian(sdf[metric_key])\n",
    "                if dataset == \"in9bggap\":\n",
    "                    filter_mr = dict(filter2, dataset=\"in9mixedrand\")\n",
    "                    sdf = select_rows(test_runs_df, filter_mr, allow_missing=False)\n",
    "                    my_val = my_val - np.nanmedian(sdf[metric_key])\n",
    "                if dummy:\n",
    "                    best_results[dataset].append(my_val)\n",
    "                    best_results_grouped[dataset][group].append(my_val)\n",
    "                    continue\n",
    "                if np.isnan(my_val):\n",
    "                    latex_table += r\"   --  \"\n",
    "                    continue\n",
    "                is_best = my_val + eps >= np.max(best_results[dataset])\n",
    "                if len(best_results[dataset]) > 1:\n",
    "                    is_secd = my_val + eps >= np.sort(best_results[dataset])[-2]\n",
    "                else:\n",
    "                    is_secd = False\n",
    "                is_best_grp = my_val + eps >= np.max(best_results_grouped[dataset][group])\n",
    "                is_best_grp &= len(best_results_grouped[dataset][group]) > 1\n",
    "                sc_base = 0  # np.nanmedian(best_results[dataset])\n",
    "                sc_top = 1  # np.max(best_results[dataset])\n",
    "                sc = 100 * max(0, (my_val - sc_base) / (sc_top - sc_base))\n",
    "                latex_table += r\"\\cellcolor{cbg!\" + f\"{sc:.0f}\" + \"}\"\n",
    "                if show_pc:\n",
    "                    my_val = my_val * 100\n",
    "                latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best:\n",
    "                    latex_table += r\"\\tcf{\"\n",
    "                elif is_secd:\n",
    "                    latex_table += r\"\\tcs{\"\n",
    "                else:\n",
    "                    # latex_table += \"     \"\n",
    "                    pass\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best_grp:\n",
    "                    latex_table += r\"\\tcg{\"\n",
    "                else:\n",
    "                    # latex_table += \"     \"\n",
    "                    pass\n",
    "                latex_table += show_fmt.format(my_val)\n",
    "                if highlight_best:\n",
    "                    latex_table += r\"}\" if is_best or is_secd else \" \"\n",
    "                    latex_table += r\"}\" if is_best_grp else \" \"\n",
    "                latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "            latex_table += r\" \\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\bottomrule\" + \"\\n\"\n",
    "    latex_table += r\"\\end{tabular}\" + \"\\n\"\n",
    "    latex_table += r\"}\" + \"\\n\"\n",
    "\n",
    "print()\n",
    "print(f\"There are {len(cmds)} commands to execute to generate missing datapoints\")\n",
    "if show_commands:\n",
    "    for cmd in cmds:\n",
    "        print(cmd)\n",
    "\n",
    "print()\n",
    "print(\"Done!\")\n",
    "print()\n",
    "print(f\"Here is your results table for {metric_key}, {clusterer}:\")\n",
    "print()\n",
    "print()\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cmd in cmds:\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactored to use a results matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterers = [\n",
    "    \"KMeans\",\n",
    "    \"SpectralClustering\",\n",
    "    \"AC w/ C\",\n",
    "    \"AC w/o C\",\n",
    "    \"AffinityPropagation\",\n",
    "    \"HDBSCAN\",\n",
    "]\n",
    "clusterers = [\"KMeans\"]\n",
    "metric_key = \"silhouette-euclidean_true\"  # AMI  AMI_clus  num_cluster_pred  silhouette-euclidean_pred  silhouette-og-euclidean_pred\n",
    "use_rank = False\n",
    "show_pc = True\n",
    "show_fmt = \"{:4.0f}\"\n",
    "highlight_best = True\n",
    "use_si_num = False\n",
    "eps = 0.005\n",
    "merge_model_group_column = True\n",
    "\n",
    "# override_fields = {\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50, \"dim_reducer\": \"None\", \"pca_variance\": None, \"ndim_reduced\": None, \"zscore\": False}\n",
    "override_fields = {\n",
    "    \"dim_reducer_man\": \"None\",\n",
    "    \"ndim_reduced_man\": None,\n",
    "    \"dim_reducer\": \"PCA\",\n",
    "    \"pca_variance\": 0.9,\n",
    "    \"ndim_reduced\": None,\n",
    "    \"zscore\": True,\n",
    "}\n",
    "# override_fields = {}\n",
    "fixed_sc_base = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterers = [\"SpectralClustering\"]\n",
    "metric_key = \"AMI\"  # AMI  AMI_clus  num_cluster_pred  silhouette-euclidean_pred  silhouette-og-euclidean_pred\n",
    "use_rank = False\n",
    "show_pc = True\n",
    "show_fmt = \"{:4.0f}\"\n",
    "highlight_best = True\n",
    "use_si_num = False\n",
    "eps = 0.005\n",
    "merge_model_group_column = True\n",
    "\n",
    "override_fields = {}\n",
    "fixed_sc_base = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterers = [\"LouvainCommunities\"]\n",
    "metric_key = \"AMI\"  # AMI  AMI_clus  num_cluster_pred  silhouette-euclidean_pred  silhouette-og-euclidean_pred\n",
    "use_rank = False\n",
    "show_pc = True\n",
    "show_fmt = \"{:4.0f}\"\n",
    "highlight_best = True\n",
    "use_si_num = False\n",
    "eps = 0.005\n",
    "merge_model_group_column = True\n",
    "\n",
    "override_fields = {}\n",
    "fixed_sc_base = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterers = [\n",
    "    \"KMeans\",\n",
    "    \"SpectralClustering\",\n",
    "    \"AC w/ C\",\n",
    "    \"AC w/o C\",\n",
    "    \"AffinityPropagation\",\n",
    "    \"HDBSCAN\",\n",
    "]\n",
    "metric_key = \"AMI\"  # AMI  AMI_clus  num_cluster_pred  silhouette-euclidean_pred  silhouette-og-euclidean_pred\n",
    "use_rank = False\n",
    "show_pc = True\n",
    "show_fmt = \"{:4.0f}\"\n",
    "highlight_best = True\n",
    "use_si_num = False\n",
    "eps = 0.005\n",
    "merge_model_group_column = True\n",
    "\n",
    "override_fields = {}\n",
    "fixed_sc_base = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "MyWQRN9iqBkI",
    "outputId": "2642efe3-b0ef-46f3-dc70-f2f5c69645d4"
   },
   "outputs": [],
   "source": [
    "# override_fields = {\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50, \"dim_reducer\": \"None\", \"pca_variance\": None, \"ndim_reduced\": None, \"zscore\": False}\n",
    "# override_fields = {\"dim_reducer_man\": \"None\", \"ndim_reduced_man\": None, \"dim_reducer\": \"PCA\", \"pca_variance\": 0.9, \"ndim_reduced\": None, \"zscore\": True}\n",
    "# fixed_sc_base = 0\n",
    "\n",
    "if metric_key == \"num_cluster_pred\":\n",
    "    clusterers = [\"AC w/o C\", \"AffinityPropagation\", \"HDBSCAN\"]\n",
    "    show_pc = False\n",
    "    show_fmt = \"{:4.0f}\"\n",
    "    highlight_best = False\n",
    "    use_si_num = True\n",
    "\n",
    "if metric_key.startswith(\"silhouette\"):\n",
    "    show_pc = False\n",
    "    show_fmt = \"{:5.2f}\"\n",
    "\n",
    "\n",
    "model_groups = {\n",
    "    \"\": [\"none\"],\n",
    "    \"RN50\": RESNET50_MODELS + FT_RESNET50_MODELS,\n",
    "    \"ViT-B\": VITB16_MODELS + FT_VITB16_MODELS,\n",
    "}\n",
    "\n",
    "test_datasets_grouped, dataset2sh = TEST_DATASETS_GROUPED, DATASET2SH\n",
    "\n",
    "\n",
    "if False:\n",
    "    # IN9 table\n",
    "    model_groups = {\n",
    "        # \"\": [\"none\"],\n",
    "        \"RN50\": RESNET50_MODELS[1:] + FT_RESNET50_MODELS,\n",
    "        \"ViT-B\": VITB16_MODELS[1:] + FT_VITB16_MODELS,\n",
    "    }\n",
    "    test_datasets_grouped, dataset2sh = {\"in9_bg_challenge\": IN9_DATASETS + [\"in9bggap\"]}, IN92SH\n",
    "\n",
    "\n",
    "if len(clusterers) == 1:\n",
    "    clustererstr = clusterers[0]\n",
    "    if metric_key.endswith(\"_true\"):\n",
    "        clustererstr = \"GT\"\n",
    "else:\n",
    "    clustererstr = f\"{len(clusterers)}c-avg\"\n",
    "\n",
    "model_groups_flattened = make_flat_hierarchy_from_dict(model_groups, pad_right=False)\n",
    "model_groups_flattened = np.array(model_groups_flattened)\n",
    "model_groups_flattened = model_groups_flattened[:, -1]\n",
    "\n",
    "test_datasets = []\n",
    "for datagroupname, datagroupset in test_datasets_grouped.items():\n",
    "    test_datasets.extend(datagroupset)\n",
    "\n",
    "# test_datasets = [d for d in test_datasets if \"inaturalist\" not in d]\n",
    "# test_datasets = [d for d in test_datasets if \"bioscan\" not in d]\n",
    "\n",
    "print(\"Encoders:\")\n",
    "print(model_groups_flattened)\n",
    "print()\n",
    "print(\"Datasets:\")\n",
    "print(test_datasets)\n",
    "print()\n",
    "print(\"Clusterers:\")\n",
    "print(clusterers)\n",
    "print()\n",
    "\n",
    "result_table, cmds = build_results_table(\n",
    "    model_groups_flattened,\n",
    "    clusterers,\n",
    "    test_datasets,\n",
    "    metric_keys=metric_key,\n",
    "    override_fields=override_fields,\n",
    "    return_cmds=True,\n",
    ")\n",
    "# Shaped [models, clusterers, datasets]\n",
    "print(\"result_table.shape\", result_table.shape)\n",
    "\n",
    "# Remove clusterer-dataset combos which are NaN for any model\n",
    "result_table[:, np.any(np.isnan(result_table), axis=0)] = np.nan\n",
    "\n",
    "# Take mean over clusterers\n",
    "result_table = np.nanmean(result_table, axis=1)\n",
    "# Shaped [models, datasets]\n",
    "\n",
    "print(\"result_table.shape\", result_table.shape)\n",
    "\n",
    "if use_rank:\n",
    "    result_table_r = np.argsort(result_table, -1)[::-1, :] + 1\n",
    "else:\n",
    "    result_table_r = result_table\n",
    "\n",
    "\n",
    "print(model_groups)\n",
    "\n",
    "\n",
    "best_results = {k: [] for k in test_datasets}\n",
    "best_results_grouped = {k: defaultdict(list) for k in test_datasets}\n",
    "\n",
    "for dummy in [True, False]:\n",
    "    latex_table = r\"% Results for \" + f\"{metric_key}, {clustererstr}\" + \"\\n\"\n",
    "    now_str = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    latex_table += r\"% Generated \" + now_str + \"\\n\"\n",
    "    latex_table += r\"% Using hparams \" + BEST_PARAMS[\"_version\"] + \"\\n\"\n",
    "    label = clustererstr\n",
    "    if metric_key == \"AMI\":\n",
    "        latex_table += r\"\\label{tab:\" + label + r\"}\" + \"\\n\"\n",
    "    label = metric_key.replace(\"_\", \"-\") + \":\" + label\n",
    "    latex_table += r\"\\label{tab:\" + label + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"\\resizebox{\\columnwidth}{!}{%\" + \"\\n\"\n",
    "    latex_table += r\"\\begin{tabular}{\"\n",
    "    if not merge_model_group_column:\n",
    "        latex_table += \"l\"\n",
    "    latex_table += \"ll\" + r\"r\" * len(test_datasets) + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    # Begin dataset group header row\n",
    "    if len(test_datasets_grouped) > 1:\n",
    "        if not merge_model_group_column:\n",
    "            latex_table += r\"& \"\n",
    "        latex_table += f\"{'':<11s}\" + r\" &   \"\n",
    "        for datagroupname, datagroupset in test_datasets_grouped.items():\n",
    "            latex_table += r\" & \\multicolumn{\" + str(len(datagroupset)) + r\"}{c}{\" + datagroupname + r\"}\"\n",
    "        latex_table += r\"\\\\\" + \"\\n\"\n",
    "        icol = 3\n",
    "        if not merge_model_group_column:\n",
    "            icol += 1\n",
    "        for datagroupname, datagroupset in test_datasets_grouped.items():\n",
    "            latex_table += r\"\\cmidrule(l){\" + f\"{icol}-{icol + len(datagroupset) - 1}\" + r\"}\"\n",
    "            icol += len(datagroupset)\n",
    "        latex_table += \"\\n\"\n",
    "    # Begin main header row, with actual dataset names\n",
    "    if merge_model_group_column:\n",
    "        latex_table += r\"\\quad \"\n",
    "    else:\n",
    "        latex_table += r\"Arch. & \"\n",
    "    latex_table += f\"{'Encoder':<11s}\" + r\" & FT \"\n",
    "    for dataset in test_datasets:\n",
    "        latex_table += r\"&\" + \"{:^15s}\".format(dataset2sh.get(dataset, dataset))\n",
    "    latex_table += r\"\\\\\" + \"\\n\"\n",
    "    # Begin table contents\n",
    "    latex_table += r\"\\midrule\" + \"\\n\"\n",
    "    i_model_o = -1\n",
    "    if metric_key == \"num_cluster_pred\":\n",
    "        latex_table += r\"& Num targets\"\n",
    "        for i_dataset, dataset in enumerate(test_datasets):\n",
    "            sdf = select_rows(test_runs_df, {\"dataset\": dataset}, allow_missing=False)\n",
    "            sdf = sdf[~pd.isna(sdf[\"num_cluster_true\"])]\n",
    "            latex_table += r\"& \"\n",
    "            latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "            latex_table += f\"{sdf.iloc[0]['num_cluster_true'].item()}\"\n",
    "            latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "        latex_table += r\"\\\\\" + \"\\n\"\n",
    "        latex_table += r\"\\midrule\" + \"\\n\"\n",
    "    elif metric_key.endswith(\"_pred\"):\n",
    "        metric_key2 = metric_key.replace(\"_pred\", \"_true\")\n",
    "        clusterername = \"G.T.\"\n",
    "        latex_table += (\n",
    "            r\"\\parbox[t]{2mm}{\\multirow{\"\n",
    "            + str(len(model_groups[group]))\n",
    "            + r\"}{*}{\"\n",
    "            + r\"\\scalebox{0.9}{\"\n",
    "            + r\"\\rotatebox[origin=c]{90}{\"\n",
    "            + clusterername\n",
    "            + r\"}}}\"\n",
    "            + r\"}\"\n",
    "        )\n",
    "        latex_table += \"\\n\"\n",
    "        for i_model, model in enumerate(list(model_groups[group])):\n",
    "            i_model_o += 1\n",
    "            latex_table += f\"& {MODEL2SH[model]:<10s}\"\n",
    "            for i_dataset, dataset in enumerate(test_datasets):\n",
    "                latex_table += \" &\"\n",
    "                my_val = result_table[i_model_o, i_dataset]\n",
    "                if sum(sdf[metric_key2] != my_val) > 0:\n",
    "                    pass\n",
    "                if dummy:\n",
    "                    best_results_grouped[dataset][group].append(my_val)\n",
    "                    continue\n",
    "                is_best_grp = my_val + eps >= np.max(best_results_grouped[dataset][group])\n",
    "                latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "                latex_table += \"     \"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best_grp:\n",
    "                    latex_table += r\"\\tcg{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                latex_table += show_fmt.format(my_val)\n",
    "                if highlight_best:\n",
    "                    latex_table += r\"}\" if is_best_grp else \" \"\n",
    "                latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "\n",
    "            latex_table += r\" \\\\\" + \"\\n\"\n",
    "        latex_table += r\"\\midrule\" + \"\\n\"\n",
    "\n",
    "    i_model_o = -1\n",
    "    for i_group, group in enumerate(model_groups):\n",
    "        if i_group > 0:\n",
    "            latex_table += r\"\\midrule\" + \"\\n\"\n",
    "        if merge_model_group_column:\n",
    "            if not group:\n",
    "                latex_table += r\"\\quad \"\n",
    "            else:\n",
    "                latex_table += r\"\\textbf{\" + group + r\"} --- \"\n",
    "        elif not group:\n",
    "            latex_table += \"---\" + \"\\n\"\n",
    "        else:\n",
    "            latex_table += group + \"\\n\"\n",
    "        for i_model, model in enumerate(list(model_groups[group])):\n",
    "            i_model_o += 1\n",
    "            model_sh = MODEL2SH.get(model, model)\n",
    "            if model_sh.endswith(\" [FT]\"):\n",
    "                model_sh = f\"{model_sh[:-4]:<10s}\" + r\" & \\checkmark\"\n",
    "            else:\n",
    "                model_sh = f\"{model_sh:<10s}\" + \" &\"\n",
    "            if merge_model_group_column and i_model > 0:\n",
    "                latex_table += r\"\\quad \"\n",
    "            latex_table += f\"{model_sh:<23s}\"\n",
    "            for i_dataset, dataset in enumerate(test_datasets):\n",
    "                latex_table += \" &\"\n",
    "                my_val = result_table[i_model_o, i_dataset]\n",
    "                if dummy:\n",
    "                    best_results[dataset].append(my_val)\n",
    "                    best_results_grouped[dataset][group].append(my_val)\n",
    "                    continue\n",
    "                if np.isnan(my_val):\n",
    "                    latex_table += r\"   --  \"\n",
    "                    continue\n",
    "                is_best = my_val + eps >= np.max(best_results[dataset])\n",
    "                if len(best_results[dataset]) > 1:\n",
    "                    is_secd = my_val + eps >= np.sort(best_results[dataset])[-2]\n",
    "                else:\n",
    "                    is_secd = False\n",
    "                is_best_grp = my_val + eps >= np.max(best_results_grouped[dataset][group])\n",
    "                is_best_grp &= len(best_results_grouped[dataset][group]) > 1\n",
    "                sc_base = np.nanmedian(best_results[dataset])\n",
    "                if fixed_sc_base is not None:\n",
    "                    sc_base = fixed_sc_base\n",
    "                sc_top = np.max(best_results[dataset])\n",
    "                sc = 100 * max(0, (my_val - sc_base) / (sc_top - sc_base))\n",
    "                if sc_top >= sc_base:\n",
    "                    latex_table += r\"\\cc{cbg!\" + f\"{sc:.0f}\" + \"}\"\n",
    "                if show_pc:\n",
    "                    my_val = my_val * 100\n",
    "                latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best:\n",
    "                    latex_table += r\"\\tcf{\"\n",
    "                elif is_secd:\n",
    "                    latex_table += r\"\\tcs{\"\n",
    "                else:\n",
    "                    # latex_table += \"     \"\n",
    "                    pass\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best_grp:\n",
    "                    latex_table += r\"\\tcg{\"\n",
    "                else:\n",
    "                    # latex_table += \"     \"\n",
    "                    pass\n",
    "                latex_table += show_fmt.format(my_val)\n",
    "                if highlight_best:\n",
    "                    latex_table += r\"}\" if is_best or is_secd else \" \"\n",
    "                    latex_table += r\"}\" if is_best_grp else \" \"\n",
    "                latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "            latex_table += r\" \\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\bottomrule\" + \"\\n\"\n",
    "    latex_table += r\"\\end{tabular}\" + \"\\n\"\n",
    "    latex_table += r\"}\" + \"\\n\"\n",
    "\n",
    "print()\n",
    "print(f\"There are {len(cmds)} commands to execute to generate missing datapoints\")\n",
    "\n",
    "print()\n",
    "print(\"Done!\")\n",
    "print()\n",
    "print(f\"Here is your results table for {metric_key}, {clustererstr}:\")\n",
    "print()\n",
    "print()\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for cmd in cmds:\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusterer correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_key = \"AMI\"  # num_cluster_pred  silhouette-euclidean_pred  silhouette-og-euclidean_pred\n",
    "clusterers = [\n",
    "    \"KMeans\",\n",
    "    \"SpectralClustering\",\n",
    "    \"AC w/ C\",\n",
    "    \"AC w/o C\",\n",
    "    \"AffinityPropagation\",\n",
    "    \"HDBSCAN\",\n",
    "]\n",
    "\n",
    "model_groups = {\n",
    "    \"---\": [\"none\"],\n",
    "    \"RN50\": RESNET50_MODELS + FT_RESNET50_MODELS,\n",
    "    \"ViT-B\": VITB16_MODELS + FT_VITB16_MODELS,\n",
    "}\n",
    "\n",
    "test_datasets_grouped = TEST_DATASETS_GROUPED\n",
    "\n",
    "if len(clusterers) == 1:\n",
    "    clustererstr = clusterers[0]\n",
    "else:\n",
    "    clustererstr = f\"{len(clusterers)}c-avg\"\n",
    "\n",
    "model_groups_flattened = make_flat_hierarchy_from_dict(model_groups, pad_right=False)\n",
    "model_groups_flattened = np.array(model_groups_flattened)\n",
    "model_groups_flattened = model_groups_flattened[:, -1]\n",
    "\n",
    "test_datasets = []\n",
    "for datagroupname, datagroupset in test_datasets_grouped.items():\n",
    "    test_datasets.extend(datagroupset)\n",
    "\n",
    "result_table, cmds = build_results_table(\n",
    "    model_groups_flattened,\n",
    "    clusterers,\n",
    "    test_datasets,\n",
    "    metric_keys=metric_key,\n",
    "    return_cmds=True,\n",
    ")\n",
    "print(f\"There are {len(cmds)} commands to execute to generate missing datapoints\")\n",
    "print()\n",
    "print(\"result_table.shape\", result_table.shape)  # Shaped [models, clusterers, datasets]\n",
    "\n",
    "xx, yy, zz = np.meshgrid(\n",
    "    [MODEL2SH_ARCH[k] for k in model_groups_flattened],\n",
    "    clusterers,\n",
    "    test_datasets,\n",
    "    indexing=\"ij\",\n",
    ")\n",
    "print(\"xx.shape\", xx.shape)\n",
    "\n",
    "result_table = np.swapaxes(result_table, 1, 2)  # Shaped [models, datasets, clusterers]\n",
    "xx = np.swapaxes(xx, 1, 2)\n",
    "yy = np.swapaxes(yy, 1, 2)\n",
    "zz = np.swapaxes(zz, 1, 2)\n",
    "result_table = np.reshape(result_table, (-1, result_table.shape[-1]))  # Shaped [models * datasets, clusterers]\n",
    "xx = np.reshape(xx, (-1, xx.shape[-1]))\n",
    "yy = np.reshape(yy, (-1, yy.shape[-1]))\n",
    "zz = np.reshape(zz, (-1, zz.shape[-1]))\n",
    "print(\"result_table.shape\", result_table.shape)\n",
    "print(\"xx.shape\", xx.shape)\n",
    "\n",
    "result_df = pd.DataFrame(data=result_table, columns=[CLUSTERER2SH.get(c, c) for c in clusterers])\n",
    "result_df[\"encoder\"] = xx[:, 0]\n",
    "result_df[\"dataset\"] = zz[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cmd in cmds:\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seaborn.set_context(\"paper\", rc={\"axes.labelsize\": 12})\n",
    "seaborn.set_context(\"paper\", font_scale=1.4)\n",
    "seaborn.pairplot(result_df)\n",
    "\n",
    "plt.savefig(os.path.join(FIGS_DIR, f\"scatter-clusterers_{metric_key}.pdf\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seaborn.set_context(\"paper\", font_scale=1.4)\n",
    "seaborn.set_context(\"paper\", rc={\"axes.labelsize\": 25})\n",
    "palette = {v: MODEL2COLORRGB[k] for k, v in MODEL2SH_ARCH.items()}\n",
    "seaborn.pairplot(result_df, diag_kind=\"hist\")\n",
    "\n",
    "plt.savefig(\n",
    "    os.path.join(FIGS_DIR, f\"scatter-clusterers_{metric_key}_biglab.pdf\"),\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.set_context(\"paper\", font_scale=1.4)\n",
    "# seaborn.set_context(\"paper\", rc={\"axes.labelsize\": 22})\n",
    "palette = {v: MODEL2COLORRGB[k] for k, v in MODEL2SH_ARCH.items()}\n",
    "seaborn.pairplot(result_df, hue=\"encoder\", palette=palette, diag_kind=\"hist\")\n",
    "\n",
    "plt.savefig(\n",
    "    os.path.join(FIGS_DIR, f\"scatter-clusterers_{metric_key}_col-enc.pdf\"),\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seaborn.set_context(\"paper\", rc={\"axes.labelsize\": 12})\n",
    "seaborn.set_context(\"paper\", font_scale=1.4)\n",
    "seaborn.pairplot(result_df, hue=\"dataset\")\n",
    "\n",
    "plt.savefig(\n",
    "    os.path.join(FIGS_DIR, f\"scatter-clusterers_{metric_key}_col-dataset.pdf\"),\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_key = \"AMI\"  # num_cluster_pred  silhouette-euclidean_pred  silhouette-og-euclidean_pred\n",
    "clusterers = [\n",
    "    \"KMeans\",\n",
    "    \"SpectralClustering\",\n",
    "    \"AC w/ C\",\n",
    "    \"AC w/o C\",\n",
    "    \"AffinityPropagation\",\n",
    "    \"HDBSCAN\",\n",
    "]\n",
    "\n",
    "model_groups = {\n",
    "    \"---\": [\"none\"],\n",
    "    \"RN50\": RESNET50_MODELS + FT_RESNET50_MODELS,\n",
    "    \"ViT-B\": VITB16_MODELS + FT_VITB16_MODELS,\n",
    "}\n",
    "\n",
    "test_datasets_grouped = TEST_DATASETS_GROUPED\n",
    "\n",
    "if len(clusterers) == 1:\n",
    "    clustererstr = clusterers[0]\n",
    "else:\n",
    "    clustererstr = f\"{len(clusterers)}c-avg\"\n",
    "\n",
    "model_groups_flattened = make_flat_hierarchy_from_dict(model_groups, pad_right=False)\n",
    "model_groups_flattened = np.array(model_groups_flattened)\n",
    "model_groups_flattened = model_groups_flattened[:, -1]\n",
    "\n",
    "test_datasets = []\n",
    "for datagroupname, datagroupset in test_datasets_grouped.items():\n",
    "    test_datasets.extend(datagroupset)\n",
    "\n",
    "result_table, cmds = build_results_table(\n",
    "    model_groups_flattened,\n",
    "    clusterers,\n",
    "    test_datasets,\n",
    "    metric_keys=metric_key,\n",
    "    return_cmds=True,\n",
    ")\n",
    "print(f\"There are {len(cmds)} commands to execute to generate missing datapoints\")\n",
    "print()\n",
    "print(\"result_table.shape\", result_table.shape)  # Shaped [models, clusterers, datasets]\n",
    "\n",
    "xx, yy, zz = np.meshgrid(\n",
    "    [MODEL2SH_ARCH[k] for k in model_groups_flattened],\n",
    "    clusterers,\n",
    "    test_datasets,\n",
    "    indexing=\"ij\",\n",
    ")\n",
    "print(\"xx.shape\", xx.shape)\n",
    "\n",
    "result_table = np.swapaxes(result_table, 0, 2)  # Shaped [datasets, models, clusterers]\n",
    "xx = np.swapaxes(xx, 0, 2)\n",
    "yy = np.swapaxes(yy, 0, 2)\n",
    "zz = np.swapaxes(zz, 0, 2)\n",
    "result_table = np.reshape(result_table, (-1, result_table.shape[-1]))  # Shaped [datasets * models, clusterers]\n",
    "xx = np.reshape(xx, (-1, xx.shape[-1]))\n",
    "yy = np.reshape(yy, (-1, yy.shape[-1]))\n",
    "zz = np.reshape(zz, (-1, zz.shape[-1]))\n",
    "print(\"result_table.shape\", result_table.shape)\n",
    "print(\"xx.shape\", xx.shape)\n",
    "\n",
    "result_df = pd.DataFrame(data=result_table, columns=model_groups_flattened)\n",
    "result_df[\"encoder\"] = xx[:, 0]\n",
    "result_df[\"clusterer\"] = yy[:, 0]\n",
    "result_df[\"dataset\"] = zz[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seaborn.set_context(\"paper\", rc={\"axes.labelsize\": 12})\n",
    "seaborn.set_context(\"paper\", font_scale=1.4)\n",
    "seaborn.pairplot(result_df)\n",
    "\n",
    "# plt.savefig(os.path.join(FIGS_DIR, f\"scatter-encoders_{metric_key}.pdf\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWpBwhP8MOm0"
   },
   "source": [
    "## Correlation between AMI and Silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Gp5BaVDxG2Il",
    "outputId": "2ea12a0e-f294-47fa-8830-d59fbfd547b5"
   },
   "outputs": [],
   "source": [
    "metric_key1 = \"silhouette-euclidean_pred\"  # silhouette-euclidean_pred | silhouette-og-euclidean_pred\n",
    "metric_key2 = \"AMI\"\n",
    "\n",
    "override_fields = {}\n",
    "\n",
    "backbones = [\"ResNet-50\", \"ViT-B\"]\n",
    "CLUSTERERS = [\n",
    "    \"KMeans\",\n",
    "    \"SpectralClustering\",\n",
    "    \"AC w/ C\",\n",
    "    \"AC w/o C\",\n",
    "    \"AffinityPropagation\",\n",
    "    \"HDBSCAN\",\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(1, len(backbones), sharey=True, figsize=(6, 3))\n",
    "\n",
    "for i_backbone, backbone in enumerate(backbones):\n",
    "    my_valx_method = {clusterer: [] for clusterer in CLUSTERERS}\n",
    "    my_valy_method = {clusterer: [] for clusterer in CLUSTERERS}\n",
    "\n",
    "    print(backbone)\n",
    "    print(CLUSTERERS)\n",
    "    print(TEST_DATASETS)\n",
    "    print(MODEL_GROUPS[backbone])\n",
    "    print()\n",
    "\n",
    "    for i_clusterer, clusterer in enumerate(CLUSTERERS):\n",
    "        for i_dataset, dataset in enumerate(TEST_DATASETS):\n",
    "            for i_model, model in enumerate(list(MODEL_GROUPS[backbone])):\n",
    "                if model in [\n",
    "                    \"timm_vit_base_patch16_224.mae\",\n",
    "                    \"mae_pretrain_vit_base_global\",\n",
    "                ]:\n",
    "                    # print(f\"Skipping {model}\")\n",
    "                    continue\n",
    "                    pass\n",
    "                if model.startswith(\"random\"):\n",
    "                    continue\n",
    "                    pass\n",
    "                filter1 = {\"model\": model, \"dataset\": dataset}\n",
    "                filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "                filter2.update(filter1)\n",
    "                filter2.update(override_fields)\n",
    "                filter2 = fixup_filter(filter2)\n",
    "                sdf = select_rows(test_runs_df, filter2, allow_missing=False)\n",
    "                my_valx_method[clusterer].append(np.nanmedian(sdf[metric_key1]))\n",
    "                my_valy_method[clusterer].append(np.nanmedian(sdf[metric_key2]))\n",
    "\n",
    "    my_valx_method = {k: np.array(v) for k, v in my_valx_method.items()}\n",
    "    my_valy_method = {k: np.array(v) for k, v in my_valy_method.items()}\n",
    "    my_valx_overall = np.concatenate([my_valx_method[clusterer] for clusterer in CLUSTERERS])\n",
    "    my_valy_overall = np.concatenate([my_valy_method[clusterer] for clusterer in CLUSTERERS])\n",
    "    my_cols = np.concatenate(\n",
    "        [\n",
    "            np.tile(\n",
    "                CLUSTERER2COLORRGB.get(clusterer, (0.0, 0.0, 0.0)),\n",
    "                [len(my_valx_method[clusterer]), 1],\n",
    "            )\n",
    "            for clusterer in CLUSTERERS\n",
    "        ]\n",
    "    )\n",
    "    indices = np.arange(len(my_valx_overall))\n",
    "    np.random.shuffle(indices)\n",
    "    ax[i_backbone].scatter(\n",
    "        my_valx_overall[indices],\n",
    "        my_valy_overall[indices],\n",
    "        color=my_cols[indices],\n",
    "        s=20,\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    ax[i_backbone].set_xlabel(r\"$S$\" if metric_key1.startswith(\"silhouette\") else metric_key1)\n",
    "    if i_backbone == 0:\n",
    "        ax[i_backbone].set_ylabel(metric_key2)\n",
    "    ax[i_backbone].set_xlim(-1.05, 1.05)\n",
    "    ax[i_backbone].set_ylim(-0.05, max(max(my_valy_overall), 0.95))\n",
    "    ax[i_backbone].set_title(backbone)\n",
    "    print(f\"{backbone:<20s} Correlation coef\")\n",
    "    cors = []\n",
    "    for clusterer in CLUSTERERS:\n",
    "        sel = (~np.isnan(my_valx_method[clusterer])) & (~np.isnan(my_valy_method[clusterer]))\n",
    "        cor = np.corrcoef(my_valx_method[clusterer][sel], my_valy_method[clusterer][sel])[0, 1]\n",
    "        cors.append(cor)\n",
    "        print(f\"{clusterer:<20s} {cor:.4f}\")\n",
    "    print(f\"{'Average':<20s} {np.nanmean(cors):.4f}\")\n",
    "    sel = (~np.isnan(my_valx_overall)) & (~np.isnan(my_valy_overall))\n",
    "    cor = np.corrcoef(my_valx_overall[sel], my_valy_overall[sel])[0, 1]\n",
    "    print(f\"{'Overall':<20s} {cor:.4f}\")\n",
    "    print()\n",
    "    ax[i_backbone].text(-0.85, 0.85, f\"$r={cor:.2f}$\")\n",
    "    ax[i_backbone].text(-0.85, 0.75, r\"$\\bar{r}=\" + f\"{np.mean(cors):.2f}$\")\n",
    "\n",
    "label_fn = lambda c, marker: plt.plot([], [], color=c, ls=\"None\", marker=marker, linewidth=6)[0]  # noqa:E731\n",
    "handles = [label_fn(CLUSTERER2COLORRGB.get(clusterer), \"o\") for clusterer in CLUSTERERS]\n",
    "data_labels = [CLUSTERER2SH.get(c, c) for c in CLUSTERERS]\n",
    "ax[1].legend(handles, data_labels, loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "fig.savefig(\n",
    "    os.path.join(FIGS_DIR, f\"scatter__{metric_key1}__{metric_key2}.pdf\"),\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "MuURKU9mMXIN",
    "outputId": "72833c72-a908-43cd-a4f9-d0030733fb0d"
   },
   "outputs": [],
   "source": [
    "exclude_random = True\n",
    "\n",
    "metric_key1 = \"AMI\"\n",
    "\n",
    "backbones = [\"ResNet-50\", \"ViT-B\"]\n",
    "CLUSTERERS = [\n",
    "    \"KMeans\",\n",
    "    \"SpectralClustering\",\n",
    "    \"AC w/ C\",\n",
    "    \"AC w/o C\",\n",
    "    \"AffinityPropagation\",\n",
    "    \"HDBSCAN\",\n",
    "]\n",
    "test_datasets = TEST_DATASETS\n",
    "\n",
    "figenc, axenc = plt.subplots(1, 2, figsize=(6, 2))\n",
    "figclus, axclus = plt.subplots(1, 2, figsize=(6, 2))\n",
    "\n",
    "for i_backbone, backbone in enumerate(backbones):\n",
    "    model_group = MODEL_GROUPS[backbone]\n",
    "    if exclude_random:\n",
    "        model_group = [m for m in model_group if not m.startswith(\"random\")]\n",
    "\n",
    "    result_table = build_results_table(\n",
    "        model_group,\n",
    "        CLUSTERERS,\n",
    "        test_datasets,\n",
    "        metric_keys=metric_key1,\n",
    "    )\n",
    "    result_table[np.isnan(result_table)] = -100.0\n",
    "\n",
    "    print(backbone)\n",
    "    print(model_group)\n",
    "\n",
    "    # RANK PER ENCODER - go through each dataset, look at each clusterer,\n",
    "    # and determine the rank of each encoder in that setting\n",
    "    print(list(model_group))\n",
    "    ranks_encoders = np.nan * np.ones((len(model_group), len(CLUSTERERS), len(test_datasets)))\n",
    "    for i_dataset, dataset in enumerate(test_datasets):\n",
    "        for i_clusterer, clusterer in enumerate(CLUSTERERS):\n",
    "            cluster_data = result_table[:, i_clusterer, i_dataset]\n",
    "            if np.all(cluster_data == cluster_data[0]) or np.all(np.isnan(cluster_data)):\n",
    "                print(f\"Skipping {dataset} {clusterer} (all same)\")\n",
    "                continue\n",
    "            if np.any(cluster_data == -100.0):\n",
    "                print(f\"Skipping {dataset} {clusterer} (incomplete)\")\n",
    "                continue\n",
    "            rank = np.argsort(cluster_data)[::-1]\n",
    "            ranks_encoders[:, i_clusterer, i_dataset] = 1 + rank.argsort()\n",
    "    mean_rank_encoders = np.nanmean(ranks_encoders, axis=(1, 2))\n",
    "    std_rank_encoders = np.nanstd(ranks_encoders, axis=(1, 2))\n",
    "    # order = np.argsort(mean_rank_encoders)\n",
    "    order = np.arange(len(model_group))\n",
    "\n",
    "    for i_plot, i_model in enumerate(order):\n",
    "        axenc[i_backbone].barh(\n",
    "            i_plot,\n",
    "            mean_rank_encoders[i_model],\n",
    "            xerr=std_rank_encoders[i_model],\n",
    "            align=\"center\",\n",
    "            alpha=0.6,\n",
    "            ecolor=\"black\",\n",
    "            color=MODEL2COLORRGB.get(model_group[i_model], (0.0, 0.0, 0.0)),\n",
    "            capsize=2,\n",
    "            zorder=10,\n",
    "        )\n",
    "\n",
    "    axenc[i_backbone].invert_yaxis()\n",
    "    axenc[i_backbone].set_yticks([])\n",
    "    axenc[i_backbone].set_yticklabels([])\n",
    "    axenc[i_backbone].set_xticks(np.arange(1, 1 + len(model_group)))\n",
    "    axenc[i_backbone].set_xlim([0, 0.5 + len(model_group)])\n",
    "    axenc[i_backbone].xaxis.grid(True, zorder=1, alpha=0.5)\n",
    "    axenc[i_backbone].set_title(backbone)\n",
    "\n",
    "    # RANK PER CLUSTERER - go through each dataset, look at each encoder,\n",
    "    # and determine the rank of each clusterer in that setting\n",
    "\n",
    "    print(CLUSTERERS)\n",
    "    ranks_clusterers = np.nan * np.ones((len(model_group), len(CLUSTERERS), len(test_datasets)))\n",
    "    for i_dataset, dataset in enumerate(test_datasets):\n",
    "        for i_encoder, encoder in enumerate(model_group):\n",
    "            encoder_data = result_table[i_encoder, :, i_dataset]\n",
    "            if np.all(encoder_data == encoder_data[0]) or np.all(np.isnan(encoder_data)):\n",
    "                print(f\"Skipping {dataset} {encoder} (all same)\")\n",
    "                continue\n",
    "            if np.any(encoder_data == -100.0):\n",
    "                print(f\"Skipping {dataset} {encoder} (incomplete)\")\n",
    "                continue\n",
    "            rank = np.argsort(encoder_data)[::-1]\n",
    "            ranks_clusterers[i_encoder, :, i_dataset] = 1 + rank.argsort()\n",
    "    mean_rank_clusters = np.nanmean(ranks_clusterers, axis=(0, 2))\n",
    "    std_rank_clusters = np.nanstd(ranks_clusterers, axis=(0, 2))\n",
    "    # order = np.argsort(mean_rank_clusters)\n",
    "    order = np.arange(len(CLUSTERERS))\n",
    "\n",
    "    for i_plot, i_clusterer in enumerate(order):\n",
    "        axclus[i_backbone].barh(\n",
    "            i_plot,\n",
    "            mean_rank_clusters[i_clusterer],\n",
    "            xerr=std_rank_clusters[i_clusterer],\n",
    "            align=\"center\",\n",
    "            alpha=0.6,\n",
    "            ecolor=\"black\",\n",
    "            color=CLUSTERER2COLORSTR.get(CLUSTERERS[i_clusterer], (0.0, 0.0, 0.0)),\n",
    "            capsize=2,\n",
    "            zorder=10,\n",
    "        )\n",
    "\n",
    "    axclus[i_backbone].invert_yaxis()\n",
    "    axclus[i_backbone].set_yticks([])\n",
    "    axclus[i_backbone].set_yticklabels([])\n",
    "    axclus[i_backbone].set_xticks(np.arange(1, 1 + len(CLUSTERERS)))\n",
    "    axclus[i_backbone].set_xlim([0, 0.6 + len(CLUSTERERS)])\n",
    "    axclus[i_backbone].xaxis.grid(True, zorder=1, alpha=0.5)\n",
    "    axclus[i_backbone].set_title(backbone)\n",
    "\n",
    "    axclus[i_backbone].set_xlabel(\"Rank\")\n",
    "    axenc[i_backbone].set_xlabel(\"Rank\")\n",
    "\n",
    "label_fn = lambda c, ls: plt.plot([], [], color=c, ls=ls, linewidth=3)[0]  # noqa:E731\n",
    "\n",
    "model_names = list(MODEL_GROUPS[\"ResNet-50\"]) + MODEL_GROUPS[\"ViT-B\"][-2:]\n",
    "if exclude_random:\n",
    "    model_names = [m for m in model_names if not m.startswith(\"random\")]\n",
    "handles_enc = [label_fn(MODEL2COLORRGB[idx], \"-\") for idx in model_names]\n",
    "axenc[1].legend(\n",
    "    handles_enc,\n",
    "    [MODEL2SH[x] for x in model_names],\n",
    "    loc=\"center left\",\n",
    "    bbox_to_anchor=(1, 0.5),\n",
    ")\n",
    "\n",
    "handles_clus = [label_fn(CLUSTERER2COLORRGB[clusterer], \"-\") for clusterer in CLUSTERERS]\n",
    "axclus[1].legend(handles_clus, CLUSTERERS, loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "figenc.savefig(os.path.join(FIGS_DIR, \"ranking_enc.pdf\"), bbox_inches=\"tight\")\n",
    "figclus.savefig(os.path.join(FIGS_DIR, \"ranking_clus.pdf\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With grouped datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "MuURKU9mMXIN",
    "outputId": "72833c72-a908-43cd-a4f9-d0030733fb0d"
   },
   "outputs": [],
   "source": [
    "metric_key1 = \"AMI\"\n",
    "\n",
    "backbones = [\"ResNet-50\", \"ViT-B\"]\n",
    "CLUSTERERS = [\n",
    "    \"KMeans\",\n",
    "    \"SpectralClustering\",\n",
    "    \"AC w/ C\",\n",
    "    \"AC w/o C\",\n",
    "    \"AffinityPropagation\",\n",
    "    \"HDBSCAN\",\n",
    "]\n",
    "\n",
    "for test_group, test_datasets in TEST_DATASETS_GROUPED.items():\n",
    "    figenc, axenc = plt.subplots(1, 2, figsize=(6, 1.6))\n",
    "    figclus, axclus = plt.subplots(1, 2, figsize=(6, 2))\n",
    "\n",
    "    for i_backbone, backbone in enumerate(backbones):\n",
    "        result_table = build_results_table(\n",
    "            MODEL_GROUPS[backbone],\n",
    "            CLUSTERERS,\n",
    "            test_datasets,\n",
    "            metric_keys=metric_key1,\n",
    "        )\n",
    "        result_table[np.isnan(result_table)] = -100.0\n",
    "\n",
    "        print(backbone)\n",
    "        print(MODEL_GROUPS[backbone])\n",
    "\n",
    "        # RANK PER ENCODER - go through each dataset, look at each clusterer,\n",
    "        # and determine the rank of each encoder in that setting\n",
    "        print(list(MODEL_GROUPS[backbone]))\n",
    "        ranks_encoders = np.nan * np.ones((len(MODEL_GROUPS[backbone]), len(CLUSTERERS), len(test_datasets)))\n",
    "        for i_dataset, dataset in enumerate(test_datasets):\n",
    "            for i_clusterer, clusterer in enumerate(CLUSTERERS):\n",
    "                cluster_data = result_table[:, i_clusterer, i_dataset]\n",
    "                if np.all(cluster_data == cluster_data[0]) or np.all(np.isnan(cluster_data)):\n",
    "                    print(f\"Skipping {dataset} {clusterer} (all same)\")\n",
    "                    continue\n",
    "                if np.any(cluster_data == -100.0):\n",
    "                    print(f\"Skipping {dataset} {clusterer} (incomplete)\")\n",
    "                    continue\n",
    "                rank = np.argsort(cluster_data)[::-1]\n",
    "                ranks_encoders[:, i_clusterer, i_dataset] = 1 + rank.argsort()\n",
    "        mean_rank_encoders = np.nanmean(ranks_encoders, axis=(1, 2))\n",
    "        std_rank_encoders = np.nanstd(ranks_encoders, axis=(1, 2))\n",
    "        # order = np.argsort(mean_rank_encoders)\n",
    "        order = np.arange(len(MODEL_GROUPS[backbone]))\n",
    "\n",
    "        for i_plot, i_model in enumerate(order):\n",
    "            axenc[i_backbone].barh(\n",
    "                i_plot,\n",
    "                mean_rank_encoders[i_model],\n",
    "                xerr=std_rank_encoders[i_model],\n",
    "                align=\"center\",\n",
    "                alpha=0.6,\n",
    "                ecolor=\"black\",\n",
    "                color=MODEL2COLORRGB.get(MODEL_GROUPS[backbone][i_model], (0.0, 0.0, 0.0)),\n",
    "                capsize=2,\n",
    "                zorder=10,\n",
    "            )\n",
    "\n",
    "        axenc[i_backbone].invert_yaxis()\n",
    "        axenc[i_backbone].set_yticks([])\n",
    "        axenc[i_backbone].set_yticklabels([])\n",
    "        axenc[i_backbone].set_xticks(np.arange(1, 1 + len(MODEL_GROUPS[backbone])))\n",
    "        axenc[i_backbone].set_xlim([0, 0.5 + len(MODEL_GROUPS[backbone])])\n",
    "        axenc[i_backbone].xaxis.grid(True, zorder=1, alpha=0.5)\n",
    "        axenc[i_backbone].set_title(f\"{DATASETGROUP2TITLE.get(test_group, test_group)}, {backbone}\")\n",
    "        axenc[i_backbone].set_xlabel(\"Rank\")\n",
    "\n",
    "        # RANK PER CLUSTERER - go through each dataset, look at each encoder,\n",
    "        # and determine the rank of each clusterer in that setting\n",
    "\n",
    "        print(CLUSTERERS)\n",
    "        ranks_clusterers = np.nan * np.ones((len(MODEL_GROUPS[backbone]), len(CLUSTERERS), len(test_datasets)))\n",
    "        for i_dataset, dataset in enumerate(test_datasets):\n",
    "            for i_encoder, encoder in enumerate(MODEL_GROUPS[backbone]):\n",
    "                encoder_data = result_table[i_encoder, :, i_dataset]\n",
    "                if np.all(encoder_data == encoder_data[0]) or np.all(np.isnan(encoder_data)):\n",
    "                    print(f\"Skipping {dataset} {encoder} (all same)\")\n",
    "                    continue\n",
    "                if np.any(encoder_data == -100.0):\n",
    "                    print(f\"Skipping {dataset} {encoder} (incomplete)\")\n",
    "                    continue\n",
    "                rank = np.argsort(encoder_data)[::-1]\n",
    "                ranks_clusterers[i_encoder, :, i_dataset] = 1 + rank.argsort()\n",
    "        mean_rank_clusters = np.nanmean(ranks_clusterers, axis=(0, 2))\n",
    "        std_rank_clusters = np.nanstd(ranks_clusterers, axis=(0, 2))\n",
    "        # order = np.argsort(mean_rank_clusters)\n",
    "        order = np.arange(len(CLUSTERERS))\n",
    "\n",
    "        for i_plot, i_clusterer in enumerate(order):\n",
    "            axclus[i_backbone].barh(\n",
    "                i_plot,\n",
    "                mean_rank_clusters[i_clusterer],\n",
    "                xerr=std_rank_clusters[i_clusterer],\n",
    "                align=\"center\",\n",
    "                alpha=0.6,\n",
    "                ecolor=\"black\",\n",
    "                color=CLUSTERER2COLORSTR.get(CLUSTERERS[i_clusterer], (0.0, 0.0, 0.0)),\n",
    "                capsize=2,\n",
    "                zorder=10,\n",
    "            )\n",
    "\n",
    "        axclus[i_backbone].invert_yaxis()\n",
    "        axclus[i_backbone].set_yticks([])\n",
    "        axclus[i_backbone].set_yticklabels([])\n",
    "        axclus[i_backbone].set_xticks(np.arange(1, 1 + len(CLUSTERERS)))\n",
    "        axclus[i_backbone].set_xlim([0, 0.6 + len(CLUSTERERS)])\n",
    "        axclus[i_backbone].xaxis.grid(True, zorder=1, alpha=0.5)\n",
    "        axclus[i_backbone].set_title(f\"{DATASETGROUP2TITLE.get(test_group, test_group)}, {backbone}\")\n",
    "        axclus[i_backbone].set_xlabel(\"Rank\")\n",
    "\n",
    "    label_fn = lambda c, ls: plt.plot([], [], color=c, ls=ls, linewidth=3)[0]  # noqa:E731\n",
    "\n",
    "    model_names = list(MODEL_GROUPS[\"ResNet-50\"]) + MODEL_GROUPS[\"ViT-B\"][-2:]\n",
    "    handles_enc = [label_fn(MODEL2COLORRGB[idx], \"-\") for idx in model_names]\n",
    "    axenc[1].legend(\n",
    "        handles_enc,\n",
    "        [MODEL2SH[x] for x in model_names],\n",
    "        loc=\"center left\",\n",
    "        bbox_to_anchor=(1, 0.5),\n",
    "    )\n",
    "\n",
    "    handles_clus = [label_fn(CLUSTERER2COLORRGB[clusterer], \"-\") for clusterer in CLUSTERERS]\n",
    "    axclus[1].legend(handles_clus, CLUSTERERS, loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "    figenc.savefig(os.path.join(FIGS_DIR, f\"ranking_enc__{test_group}.pdf\"), bbox_inches=\"tight\")\n",
    "    figclus.savefig(os.path.join(FIGS_DIR, f\"ranking_clus__{test_group}.pdf\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder rankings and Delta v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ranking_encoders(\n",
    "    model_group,\n",
    "    clusterers,\n",
    "    test_datasets,\n",
    "    metric_key=\"AMI\",\n",
    "    ax=None,\n",
    "    use_rank=True,\n",
    "    hide_ft=False,\n",
    "):\n",
    "    show_error = use_rank\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    if exclude_random:\n",
    "        model_group = [m for m in model_group if not m.startswith(\"random\")]\n",
    "\n",
    "    print(\"Encoders:\")\n",
    "    print(model_group)\n",
    "\n",
    "    print(\"Datasets:\")\n",
    "    print(test_datasets)\n",
    "\n",
    "    result_table = build_results_table(\n",
    "        model_group,\n",
    "        clusterers,\n",
    "        test_datasets,\n",
    "        metric_keys=metric_key,\n",
    "    )\n",
    "    # Shaped [models, clusterers, datasets]\n",
    "\n",
    "    # Remove clusterer-dataset combos which are NaN for any model\n",
    "    result_table[:, np.any(np.isnan(result_table), axis=0)] = np.nan\n",
    "\n",
    "    # Take mean over clusterers\n",
    "    result_table = np.nanmean(result_table, axis=1)\n",
    "    # Shaped [models, datasets]\n",
    "\n",
    "    print(result_table.shape)\n",
    "\n",
    "    # Scale up to be a percentage\n",
    "    result_table *= 100.0\n",
    "\n",
    "    if use_rank:\n",
    "        result_table_r = np.argsort(result_table, -1)[::-1, :] + 1\n",
    "    else:\n",
    "        result_table_r = result_table\n",
    "\n",
    "    # Take mean and stdev over samples\n",
    "    mu = np.mean(result_table_r, axis=-1)\n",
    "    sd = np.std(result_table_r, axis=-1)\n",
    "\n",
    "    # Do statistical tests\n",
    "    print()\n",
    "    print(\"Signed rank tests:\")\n",
    "    print(f\"{result_table.shape[1]} samples\")\n",
    "    print()\n",
    "    jj = np.argsort(mu)\n",
    "    print(\"Ordering:\")\n",
    "    for i in jj:\n",
    "        print(f\"  {model_group[i] + ' ':.<32s} {mu[i]}\")\n",
    "\n",
    "    idx_low = jj[0]\n",
    "    print(f\"Lowest {metric_key}: {model_group[idx_low]}\")\n",
    "    for i in jj[1:]:\n",
    "        wtest = scipy.stats.wilcoxon(result_table[idx_low, :], result_table[i, :], method=\"exact\")\n",
    "        print(f\"  vs {model_group[i] + ' ':.<32s} Wilcoxon pvalue={wtest.pvalue}\")\n",
    "\n",
    "    idx_high = jj[-1]\n",
    "    print(f\"Highest {metric_key}: {model_group[idx_high]}\")\n",
    "    for i in jj[:-1]:\n",
    "        wtest = scipy.stats.wilcoxon(result_table[idx_high, :], result_table[i, :], method=\"exact\")\n",
    "        print(f\"  vs {model_group[i] + ' ':.<32s} Wilcoxon pvalue={wtest.pvalue}\")\n",
    "\n",
    "    for model in model_group:\n",
    "        if model in FT_MODELS:\n",
    "            continue\n",
    "\n",
    "    # order = np.argsort(mean_rank_encoders)\n",
    "    order = np.arange(len(model_group))\n",
    "\n",
    "    for i_plot, i_model in enumerate(order):\n",
    "        ax.barh(\n",
    "            i_plot,\n",
    "            mu[i_model],\n",
    "            xerr=sd[i_model],\n",
    "            align=\"center\",\n",
    "            alpha=0.6,\n",
    "            ecolor=\"black\",\n",
    "            color=MODEL2COLORRGB.get(model_group[i_model], (0.0, 0.0, 0.0)),\n",
    "            capsize=4,\n",
    "            zorder=10,\n",
    "            hatch=\"//\" if model_group[i_model] in FT_MODELS else None,\n",
    "        )\n",
    "        if show_error:\n",
    "            ax.plot(\n",
    "                mu[i_model],\n",
    "                i_plot,\n",
    "                \"ok\",\n",
    "                markerfacecolor=\"none\",\n",
    "                zorder=11,\n",
    "            )\n",
    "\n",
    "    labels = [MODEL2SH.get(c, c) for c in model_group]\n",
    "    if hide_ft:\n",
    "        labels = [m.replace(\" [FT]\", \"\") for m in labels]\n",
    "    # ax.tick_params(axis=\"x\", labelsize=12)\n",
    "    # ax.tick_params(axis=\"y\", labelsize=12)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_yticks(np.arange(len(model_group)))\n",
    "    ax.set_yticklabels(labels)\n",
    "    if use_rank:\n",
    "        ax.set_xticks(np.arange(1, 1 + len(model_group)))\n",
    "        ax.set_xlim([0, 0.5 + len(model_group)])\n",
    "        ax.xaxis.grid(True, zorder=1, alpha=0.5)\n",
    "\n",
    "    if use_rank:\n",
    "        ax.set_xlabel(\"Rank\", fontsize=12)\n",
    "    else:\n",
    "        ax.set_xlabel(metric_key, fontsize=12)\n",
    "\n",
    "    if False:\n",
    "        label_fn = lambda c, ls: ax.plot([], [], color=c, ls=ls, linewidth=3)[0]  # noqa:E731\n",
    "        handles_enc = [label_fn(MODEL2COLORRGB[idx], \"-\") for idx in model_group]\n",
    "        ax.legend(\n",
    "            handles_enc,\n",
    "            [MODEL2SH[x] for x in model_names],\n",
    "            loc=\"center left\",\n",
    "            bbox_to_anchor=(1, 0.5),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_encoders_vs_sup(\n",
    "    model_group,\n",
    "    clusterers,\n",
    "    test_datasets,\n",
    "    metric_key=\"AMI\",\n",
    "    ax=None,\n",
    "    use_rank=False,\n",
    "    hide_ft=False,\n",
    "):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    if exclude_random:\n",
    "        model_group = [m for m in model_group if not m.startswith(\"random\")]\n",
    "\n",
    "    print(\"Encoders:\")\n",
    "    print(model_group)\n",
    "\n",
    "    print(\"Datasets:\")\n",
    "    print(test_datasets)\n",
    "\n",
    "    result_table = build_results_table(\n",
    "        model_group,\n",
    "        clusterers,\n",
    "        test_datasets,\n",
    "        metric_keys=metric_key,\n",
    "    )\n",
    "    # Shaped [models, clusterers, datasets]\n",
    "\n",
    "    # Remove clusterer-dataset combos which are NaN for any model\n",
    "    for ix, iy, iz in zip(*np.where(np.isnan(result_table))):\n",
    "        print(f\"    Missing value for {model_group[ix]}  {clusterers[iy]}  {test_datasets[iz]}\")\n",
    "    select = np.any(np.isnan(result_table), axis=0)\n",
    "    for iy, iz in zip(*np.where(select)):\n",
    "        print(f\"    Removing datapoints for all models for  {clusterers[iy]}  {test_datasets[iz]}\")\n",
    "    result_table[:, select] = np.nan\n",
    "\n",
    "    # Take mean over clusterers\n",
    "    result_table = np.nanmean(result_table, axis=1)\n",
    "    # Shaped [models, datasets]\n",
    "\n",
    "    print(result_table.shape)\n",
    "\n",
    "    # Scale up to be a percentage\n",
    "    result_table *= 100.0\n",
    "\n",
    "    # Compare to baseline model\n",
    "    print(f\"Comparing vs {model_group[0]}\")\n",
    "    result_table = result_table - result_table[[0], :]\n",
    "\n",
    "    if use_rank:\n",
    "        result_table_r = np.argsort(result_table, -1)[::-1, :] + 1\n",
    "    else:\n",
    "        result_table_r = result_table\n",
    "\n",
    "    # Take mean and stdev over samples\n",
    "    mu = np.mean(result_table_r, axis=-1)\n",
    "    sd = np.std(result_table_r, axis=-1)\n",
    "\n",
    "    # Do statistical tests\n",
    "    if False:\n",
    "        # Old version of tests, from when comparing ranks\n",
    "        print()\n",
    "        print(\"Signed rank tests:\")\n",
    "        print(f\"{result_table.shape[1]} samples\")\n",
    "        print()\n",
    "        jj = np.argsort(mu)\n",
    "        print(\"Ordering:\")\n",
    "        for i in jj:\n",
    "            print(f\"  {model_group[i] + ' ':.<32s} {mu[i]}\")\n",
    "\n",
    "        idx_low = jj[0]\n",
    "        print(f\"Lowest {metric_key}: {model_group[idx_low]}\")\n",
    "        for i in jj[1:]:\n",
    "            wtest = scipy.stats.wilcoxon(result_table[idx_low, :], result_table[i, :], method=\"exact\")\n",
    "            print(f\"  vs {model_group[i] + ' ':.<32s} Wilcoxon pvalue={wtest.pvalue}\")\n",
    "\n",
    "        idx_high = jj[-1]\n",
    "        print(f\"Highest {metric_key}: {model_group[idx_high]}\")\n",
    "        for i in jj[:-1]:\n",
    "            wtest = scipy.stats.wilcoxon(result_table[idx_high, :], result_table[i, :], method=\"exact\")\n",
    "            print(f\"  vs {model_group[i] + ' ':.<32s} Wilcoxon pvalue={wtest.pvalue}\")\n",
    "\n",
    "    print()\n",
    "    print(\"Paired t-tests:\")\n",
    "    print(f\"{result_table.shape[1]} samples\")\n",
    "    print()\n",
    "    for i in range(1, result_table.shape[0]):\n",
    "        ttest = scipy.stats.ttest_rel(result_table[0, :], result_table[i, :])\n",
    "        print(f\"  {model_group[0]:<10s} vs {model_group[i] + ' ':.<32s} {mu[i]:+6.2f}  Paired t-test pvalue={ttest.pvalue}\")\n",
    "\n",
    "    for model in model_group:\n",
    "        if model in FT_MODELS:\n",
    "            continue\n",
    "\n",
    "    order = np.arange(1, len(model_group))\n",
    "    for i_model in order:\n",
    "        ax.barh(\n",
    "            i_model,\n",
    "            mu[i_model],\n",
    "            xerr=sd[i_model],\n",
    "            align=\"center\",\n",
    "            alpha=0.6,\n",
    "            ecolor=\"black\",\n",
    "            # color=MODEL2COLORRGB.get(model_group[i_model], (0.0, 0.0, 0.0)),\n",
    "            color=\"tab:red\" if mu[i_model] < 0 else \"tab:green\",\n",
    "            capsize=4,\n",
    "            zorder=10,\n",
    "            # hatch=\"//\" if model_group[i_model] in FT_MODELS else None,\n",
    "        )\n",
    "        ax.plot(\n",
    "            mu[i_model],\n",
    "            i_model,\n",
    "            \"ok\",\n",
    "            markerfacecolor=\"none\",\n",
    "            zorder=11,\n",
    "        )\n",
    "\n",
    "    labels = [MODEL2SH.get(model_group[a], model_group[a]) for a in order]\n",
    "    if hide_ft:\n",
    "        labels = [m.replace(\" [FT]\", \"\") for m in labels]\n",
    "    # ax.tick_params(axis=\"x\", labelsize=12)\n",
    "    # ax.tick_params(axis=\"y\", labelsize=12)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_yticks(order)\n",
    "    ax.set_yticklabels(labels)\n",
    "    if use_rank:\n",
    "        ax.set_xticks(np.arange(1, 1 + len(model_group)))\n",
    "        ax.set_xlim([0, 0.5 + len(model_group)])\n",
    "        ax.xaxis.grid(True, zorder=1, alpha=0.5)\n",
    "    else:\n",
    "        # ax.set_xlim([0, 0.5 + len(model_group)])\n",
    "        ax.grid(\"x\")\n",
    "        # ax.set_xlim([-5, 5])\n",
    "        ymin, ymax = ax.get_ylim()\n",
    "        ax.vlines([0], ymin, ymax, \"k\")\n",
    "        ax.set_ylim([ymin, ymax])\n",
    "\n",
    "    if use_rank:\n",
    "        ax.set_xlabel(\"Rank\", fontsize=12)\n",
    "    else:\n",
    "        ax.set_xlabel(f\"Δ{metric_key} (p.p.)\", fontsize=12)\n",
    "\n",
    "    if False:\n",
    "        label_fn = lambda c, ls: ax.plot([], [], color=c, ls=ls, linewidth=3)[0]  # noqa:E731\n",
    "        handles_enc = [label_fn(MODEL2COLORRGB[idx], \"-\") for idx in model_group]\n",
    "        ax.legend(\n",
    "            handles_enc,\n",
    "            [MODEL2SH[x] for x in model_names],\n",
    "            loc=\"center left\",\n",
    "            bbox_to_anchor=(1, 0.5),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exclude_random = True\n",
    "metric_key = \"AMI\"\n",
    "clusterers = [\n",
    "    \"KMeans\",\n",
    "    \"SpectralClustering\",\n",
    "    \"AC w/ C\",\n",
    "    \"AC w/o C\",\n",
    "    \"AffinityPropagation\",\n",
    "    \"HDBSCAN\",\n",
    "]\n",
    "test_datasets = TEST_DATASETS\n",
    "# model_group = RESNET50_MODELS + VITB16_MODELS\n",
    "# model_group = RESNET50_MODELS + FT_RESNET50_MODELS + VITB16_MODELS + FT_VITB16_MODELS\n",
    "# model_group = RESNET50_MODELS + FT_RESNET50_MODELS\n",
    "# model_group = VITB16_MODELS + FT_VITB16_MODELS\n",
    "\n",
    "model_group = RESNET50_MODELS\n",
    "# model_group = RESNET50_MODELS_INTERLEAVED\n",
    "# model_group = VITB16_MODELS_INTERLEAVED\n",
    "\n",
    "hf = plt.figure(figsize=(6, 4))\n",
    "\n",
    "plot_encoders_vs_sup(model_group, clusterers, test_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Vertical orientation of ranking or delta subplots\n",
    "\n",
    "# plotting_fn, plotname = plot_ranking_encoders, \"rank\"\n",
    "plotting_fn, plotname = plot_encoders_vs_sup, \"delta\"\n",
    "\n",
    "exclude_random = True\n",
    "metric_key = \"AMI\"\n",
    "clusterers = [\n",
    "    \"KMeans\",\n",
    "    \"SpectralClustering\",\n",
    "    \"AC w/ C\",\n",
    "    \"AC w/o C\",\n",
    "    \"AffinityPropagation\",\n",
    "    \"HDBSCAN\",\n",
    "]\n",
    "# clusterers = [\"AC w/o C\"]\n",
    "test_datasets_grouped = TEST_DATASETS_GROUPED\n",
    "\n",
    "model_groups = {\n",
    "    \"ResNet-50\": [m for m in RESNET50_MODELS if not m.startswith(\"random\")],\n",
    "    \"ViT-B\": [m for m in VITB16_MODELS if not m.startswith(\"random\")],\n",
    "}\n",
    "fname_suffix = \"\"\n",
    "hide_ft = False\n",
    "xtickvalues = [-40, -20, 0, 20] if plotname == \"delta\" else None\n",
    "\n",
    "if False:\n",
    "    model_groups = {\n",
    "        \"ResNet-50\": [m for m in RESNET50_MODELS_INTERLEAVED if not m.startswith(\"random\")],\n",
    "        \"ViT-B\": [m for m in VITB16_MODELS_INTERLEAVED if not m.startswith(\"random\")],\n",
    "    }\n",
    "    fname_suffix = \"_FT\"\n",
    "    hide_ft = False\n",
    "\n",
    "if False:\n",
    "    model_groups = {\n",
    "        \"ResNet-50\": [\"resnet50\"] + FT_RESNET50_MODELS,\n",
    "        \"ViT-B\": [\"vitb16\"] + FT_VITB16_MODELS,\n",
    "    }\n",
    "    fname_suffix = \"_FTonly\"\n",
    "    hide_ft = True\n",
    "    xtickvalues = None\n",
    "\n",
    "\n",
    "nmodelgroups = len(model_groups)\n",
    "ndatagroups = len(test_datasets_grouped)\n",
    "\n",
    "sharex = plotname == \"delta\"\n",
    "fig, axs = plt.subplots(\n",
    "    ndatagroups,\n",
    "    nmodelgroups,\n",
    "    figsize=(nmodelgroups * 3, ndatagroups * 2),\n",
    "    sharex=sharex,\n",
    ")\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "\n",
    "for i_backbone, backbone in enumerate(model_groups):\n",
    "    for i_domain, domain in enumerate(test_datasets_grouped):\n",
    "        print()\n",
    "        print(f\"{backbone}  {domain}\")\n",
    "        print()\n",
    "        ax = axs[i_domain, i_backbone]\n",
    "        plotting_fn(\n",
    "            model_groups[backbone],\n",
    "            clusterers,\n",
    "            test_datasets_grouped[domain],\n",
    "            metric_key=metric_key,\n",
    "            ax=ax,\n",
    "            hide_ft=hide_ft,\n",
    "        )\n",
    "        if i_backbone == 0:\n",
    "            ax.set_ylabel(domain)\n",
    "        if i_domain < ndatagroups - 1:\n",
    "            if not sharex:\n",
    "                ax.set_xticklabels([])\n",
    "            ax.set_xlabel(\"\")\n",
    "\n",
    "if xtickvalues is not None:\n",
    "    print(\"Overriding xtick values\")\n",
    "    ax.set_xticks(xtickvalues)\n",
    "\n",
    "if len(clusterers) == 1:\n",
    "    clusterer_suffix = clusterers[0].replace(\" \", \"\").replace(\"/\", \"\")\n",
    "else:\n",
    "    clusterer_suffix = f\"{len(clusterers)}c-avg\"\n",
    "fname = f\"enc_{plotname}_{metric_key}_bydomain{fname_suffix}_{clusterer_suffix}.pdf\"\n",
    "print(f\"Saving to {fname}\")\n",
    "fig.savefig(os.path.join(FIGS_DIR, fname), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertical orientation of subplots\n",
    "exclude_random = True\n",
    "metric_key = \"AMI\"\n",
    "clusterers = [\n",
    "    \"KMeans\",\n",
    "    \"SpectralClustering\",\n",
    "    \"AC w/ C\",\n",
    "    \"AC w/o C\",\n",
    "    \"AffinityPropagation\",\n",
    "    \"HDBSCAN\",\n",
    "]\n",
    "# clusterers = [\"AC w/o C\"]\n",
    "test_datasets_grouped = TEST_DATASETS_GROUPED\n",
    "\n",
    "\n",
    "model_groups = {\n",
    "    \"ResNet-50\": [m for m in RESNET50_MODELS if not m.startswith(\"random\")],\n",
    "    \"ViT-B\": [m for m in VITB16_MODELS if not m.startswith(\"random\")],\n",
    "}\n",
    "fname_suffix = \"\"\n",
    "hide_ft = False\n",
    "\n",
    "if False:\n",
    "    model_groups = {\n",
    "        \"ResNet-50\": [m for m in RESNET50_MODELS_INTERLEAVED if not m.startswith(\"random\")],\n",
    "        \"ViT-B\": [m for m in VITB16_MODELS_INTERLEAVED if not m.startswith(\"random\")],\n",
    "    }\n",
    "    fname_suffix = \"_FT\"\n",
    "    hide_ft = False\n",
    "\n",
    "if False:\n",
    "    model_groups = {\n",
    "        \"ResNet-50\": [\"resnet50\"] + FT_RESNET50_MODELS,\n",
    "        \"ViT-B\": [\"vitb16\"] + FT_VITB16_MODELS,\n",
    "    }\n",
    "    fname_suffix = \"_FTonly\"\n",
    "    hide_ft = True\n",
    "\n",
    "\n",
    "nmodelgroups = len(model_groups)\n",
    "ndatagroups = len(test_datasets_grouped)\n",
    "\n",
    "fig, axs = plt.subplots(ndatagroups, nmodelgroups, figsize=(nmodelgroups * 3, ndatagroups * 2), sharex=True)\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "\n",
    "for i_backbone, backbone in enumerate(model_groups):\n",
    "    for i_domain, domain in enumerate(test_datasets_grouped):\n",
    "        print()\n",
    "        print(f\"{backbone}  {domain}\")\n",
    "        print()\n",
    "        ax = axs[i_domain, i_backbone]\n",
    "        plot_encoders_vs_sup(\n",
    "            model_groups[backbone],\n",
    "            clusterers,\n",
    "            test_datasets_grouped[domain],\n",
    "            ax=ax,\n",
    "            hide_ft=hide_ft,\n",
    "        )\n",
    "        if i_backbone == 0:\n",
    "            ax.set_ylabel(domain, fontsize=12)\n",
    "        if i_domain < ndatagroups - 1:\n",
    "            # ax.set_xticklabels([])\n",
    "            ax.set_xlabel(\"\")\n",
    "        if i_domain == 0:\n",
    "            ax.set_title(backbone)\n",
    "\n",
    "fig.savefig(\n",
    "    os.path.join(FIGS_DIR, \"enc_delta_AMI_bydomain\" + fname_suffix + \".pdf\"),\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Horizontal orientation of ranking subplots\n",
    "\n",
    "# plotting_fn, plotname = plot_ranking_encoders, \"rank\"\n",
    "plotting_fn, plotname = plot_encoders_vs_sup, \"delta\"\n",
    "\n",
    "exclude_random = True\n",
    "metric_key = \"AMI\"\n",
    "# clusterers = [\"KMeans\", \"AC w/ C\", \"AC w/o C\", \"AffinityPropagation\"]\n",
    "clusterers = [\n",
    "    \"KMeans\",\n",
    "    \"SpectralClustering\",\n",
    "    \"AC w/ C\",\n",
    "    \"AC w/o C\",\n",
    "    \"AffinityPropagation\",\n",
    "    \"HDBSCAN\",\n",
    "]\n",
    "# clusterers = [\"AC w/o C\"]\n",
    "test_datasets_grouped = TEST_DATASETS_GROUPED\n",
    "\n",
    "model_groups = {\n",
    "    \"ResNet-50\": [m for m in RESNET50_MODELS if not m.startswith(\"random\")],\n",
    "    \"ViT-B\": [m for m in VITB16_MODELS if not m.startswith(\"random\")],\n",
    "}\n",
    "fname_suffix = \"\"\n",
    "hide_ft = False\n",
    "xtickvalues = [-40, -20, 0, 20] if plotname == \"delta\" else None\n",
    "\n",
    "if False:\n",
    "    model_groups = {\n",
    "        \"ResNet-50\": [m for m in RESNET50_MODELS_INTERLEAVED if not m.startswith(\"random\")],\n",
    "        \"ViT-B\": [m for m in VITB16_MODELS_INTERLEAVED if not m.startswith(\"random\")],\n",
    "    }\n",
    "    fname_suffix = \"_FT\"\n",
    "    hide_ft = False\n",
    "\n",
    "if True:\n",
    "    model_groups = {\n",
    "        \"ResNet-50\": [\"resnet50\"] + FT_RESNET50_MODELS,\n",
    "        \"ViT-B\": [\"vitb16\"] + FT_VITB16_MODELS,\n",
    "    }\n",
    "    fname_suffix = \"_FTonly\"\n",
    "    hide_ft = True\n",
    "    xtickvalues = None\n",
    "\n",
    "\n",
    "nmodelgroups = len(model_groups)\n",
    "ndatagroups = len(test_datasets_grouped)\n",
    "\n",
    "sharex = plotname == \"delta\"\n",
    "fig, axs = plt.subplots(\n",
    "    nmodelgroups,\n",
    "    ndatagroups,\n",
    "    figsize=(ndatagroups * 2, nmodelgroups * 1.75),\n",
    "    sharex=sharex,\n",
    ")\n",
    "axs = axs.T\n",
    "\n",
    "for i_domain, domain in enumerate(test_datasets_grouped):\n",
    "    for i_backbone, backbone in enumerate(model_groups):\n",
    "        print()\n",
    "        print(f\"{backbone}  {domain}\")\n",
    "        print()\n",
    "        ax = axs[i_domain, i_backbone]\n",
    "        plotting_fn(\n",
    "            model_groups[backbone],\n",
    "            clusterers,\n",
    "            test_datasets_grouped[domain],\n",
    "            metric_key=metric_key,\n",
    "            ax=ax,\n",
    "            hide_ft=hide_ft,\n",
    "        )\n",
    "        if i_domain > 0:\n",
    "            ax.set_yticklabels([])\n",
    "        if i_backbone == 0:\n",
    "            ax.set_title(domain)\n",
    "        if i_backbone < nmodelgroups - 1:\n",
    "            if not sharex:\n",
    "                ax.set_xticklabels([])\n",
    "            ax.set_xlabel(\"\")\n",
    "        if i_domain == 0:\n",
    "            ax.set_ylabel(backbone, fontsize=12)\n",
    "\n",
    "if xtickvalues is not None:\n",
    "    print(\"Overriding xtick values\")\n",
    "    ax.set_xticks(xtickvalues)\n",
    "\n",
    "if len(clusterers) == 1:\n",
    "    clusterer_suffix = clusterers[0].replace(\" \", \"\").replace(\"/\", \"\")\n",
    "else:\n",
    "    clusterer_suffix = f\"{len(clusterers)}c-avg\"\n",
    "fname = f\"horiz_enc_{plotname}_{metric_key}_bydomain{fname_suffix}_{clusterer_suffix}.pdf\"\n",
    "print(f\"Saving to {fname}\")\n",
    "fig.savefig(os.path.join(FIGS_DIR, fname), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare clusterers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_rank = True\n",
    "exclude_random = True\n",
    "metric_key = \"AMI\"\n",
    "clusterers = [\n",
    "    \"KMeans\",\n",
    "    \"SpectralClustering\",\n",
    "    \"AC w/ C\",\n",
    "    \"AC w/o C\",\n",
    "    \"AffinityPropagation\",\n",
    "    \"HDBSCAN\",\n",
    "]\n",
    "\n",
    "test_datasets = [d for d in TEST_DATASETS if not d.startswith(\"in9onlybg\")]\n",
    "model_group = RESNET50_MODELS + VITB16_MODELS\n",
    "# model_group = RESNET50_MODELS + FT_RESNET50_MODELS + VITB16_MODELS + FT_VITB16_MODELS\n",
    "\n",
    "\n",
    "show_error = use_rank\n",
    "\n",
    "hf = plt.figure(figsize=(4, 3))\n",
    "\n",
    "if exclude_random:\n",
    "    model_group = [m for m in model_group if not m.startswith(\"random\")]\n",
    "\n",
    "print(\"Encoders:\")\n",
    "print(model_group)\n",
    "\n",
    "print(\"Datasets:\")\n",
    "print(test_datasets)\n",
    "\n",
    "result_table = build_results_table(\n",
    "    model_group,\n",
    "    clusterers,\n",
    "    test_datasets,\n",
    "    metric_keys=metric_key,\n",
    ")\n",
    "# Shaped [models, clusterers, datasets]\n",
    "\n",
    "\n",
    "# Note clusterer-dataset combos which are NaN for any model, which we will remove\n",
    "for ix, iy, iz in zip(*np.where(np.isnan(result_table))):\n",
    "    print(f\"    Missing value for {model_group[ix]}  {clusterers[iy]}  {test_datasets[iz]}\")\n",
    "\n",
    "result_table = np.swapaxes(result_table, 1, 2)\n",
    "# Shaped [models, datasets, clusterers]\n",
    "\n",
    "result_table = np.reshape(result_table, [-1, len(clusterers)])\n",
    "# Shaped [models x datasets, clusterers]\n",
    "print(result_table.shape)\n",
    "\n",
    "# Remove samples which are all NaN\n",
    "select = ~np.any(np.isnan(result_table), axis=-1)\n",
    "result_table = result_table[select]\n",
    "\n",
    "# Scale up to be a percentage\n",
    "result_table *= 100.0\n",
    "\n",
    "print(result_table.shape)\n",
    "if use_rank:\n",
    "    # Add small amount of random noise so tie breaks are allocated\n",
    "    # equally and first in the array doesn't have priority.\n",
    "    # noise = 1e-9 * np.random.randn(*result_table.shape)\n",
    "    # noise = 0\n",
    "    # BAD VERSION!\n",
    "    # result_table_r = np.argsort(result_table + noise, -1)[:, ::-1] + 1\n",
    "    # Correct version\n",
    "    result_table_r = result_table.shape[1] - np.argsort(np.argsort(result_table, axis=1), axis=1)\n",
    "    print(result_table_r.shape)\n",
    "else:\n",
    "    result_table_r = result_table\n",
    "\n",
    "# Take mean and stdev over samples\n",
    "mu = np.mean(result_table_r, axis=0)\n",
    "sd = np.std(result_table_r, axis=0)\n",
    "\n",
    "\n",
    "# Do statistical tests\n",
    "print()\n",
    "print(f\"{result_table.shape[0]} samples\")\n",
    "print()\n",
    "jj = np.argsort(mu)\n",
    "print(\"Ordering:\")\n",
    "for i in jj:\n",
    "    print(f\"  {clusterers[i]:<20s} = {mu[i]}\")\n",
    "\n",
    "idx_low = jj[0]\n",
    "print(f\"Lowest {metric_key}: {clusterers[idx_low]}\")\n",
    "for i in jj[1:]:\n",
    "    wtest = scipy.stats.wilcoxon(result_table[:, idx_low], result_table[:, i], method=\"exact\")\n",
    "    print(f\"  vs {clusterers[i]:<20s}  pvalue={wtest.pvalue}\")\n",
    "\n",
    "idx_high = jj[-1]\n",
    "print(f\"Highest {metric_key}: {clusterers[idx_high]}\")\n",
    "for i in jj[:-1]:\n",
    "    wtest = scipy.stats.wilcoxon(result_table[:, idx_high], result_table[:, i], method=\"exact\")\n",
    "    print(f\"  vs {clusterers[i]:<20s}  pvalue={wtest.pvalue}\")\n",
    "\n",
    "clusterers = np.asarray(clusterers)\n",
    "idx1 = np.where(np.asarray(clusterers) == \"AC w/o C\")[0][0]\n",
    "idx2 = np.where(np.asarray(clusterers) == \"AffinityPropagation\")[0][0]\n",
    "wtest = scipy.stats.wilcoxon(result_table[:, idx1], result_table[:, idx2], method=\"exact\")\n",
    "print(f\"{metric_key} for {clusterers[idx1]} vs {clusterers[idx2]:<20s}  pvalue={wtest.pvalue}\")\n",
    "\n",
    "# order = np.argsort(mean_rank_clusters)\n",
    "order = np.arange(len(clusterers))\n",
    "\n",
    "for i_plot, i_clusterer in enumerate(order):\n",
    "    plt.barh(\n",
    "        i_plot,\n",
    "        mu[i_clusterer],\n",
    "        xerr=sd[i_clusterer] if show_error else None,\n",
    "        align=\"center\",\n",
    "        alpha=0.6,\n",
    "        ecolor=\"black\",\n",
    "        color=CLUSTERER2COLORSTR.get(clusterers[i_clusterer], (0.0, 0.0, 0.0)),\n",
    "        capsize=4,\n",
    "        zorder=10,\n",
    "    )\n",
    "    if show_error:\n",
    "        plt.plot(\n",
    "            mu[i_clusterer],\n",
    "            i_plot,\n",
    "            \"ok\",\n",
    "            markerfacecolor=\"none\",\n",
    "        )\n",
    "\n",
    "labels = [CLUSTERER2SH.get(c, c) for c in clusterers]\n",
    "ax = plt.gca()\n",
    "ax.tick_params(axis=\"x\", labelsize=12)\n",
    "ax.tick_params(axis=\"y\", labelsize=12)\n",
    "ax.invert_yaxis()\n",
    "ax.set_yticks(np.arange(len(clusterers)))\n",
    "ax.set_yticklabels(labels)\n",
    "if use_rank:\n",
    "    ax.set_xticks(np.arange(1, 1 + len(clusterers)))\n",
    "    ax.set_xlim([0, 0.75 + len(clusterers)])\n",
    "else:\n",
    "    XLIM = [np.min(mu), np.max(mu)]\n",
    "    XLIM = XLIM + 0.075 * np.array([-1, 1]) * (XLIM[1] - XLIM[0])\n",
    "    ax.set_xlim(XLIM)\n",
    "ax.xaxis.grid(True, zorder=1, alpha=0.5)\n",
    "\n",
    "if use_rank:\n",
    "    ax.set_xlabel(\"Rank\", fontsize=15)\n",
    "else:\n",
    "    ax.set_xlabel(metric_key, fontsize=15)\n",
    "\n",
    "if False:\n",
    "    # Show legend\n",
    "    label_fn = lambda c, ls: plt.plot([], [], color=c, ls=ls, linewidth=3)[0]  # noqa:E731\n",
    "    handles_clus = [label_fn(CLUSTERER2COLORRGB[clusterer], \"-\") for clusterer in clusterers]\n",
    "    ax.legend(handles_clus, labels, loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "prfx = \"ranking\" if use_rank else metric_key\n",
    "hf.savefig(os.path.join(FIGS_DIR, f\"{prfx}_clus_overall.pdf\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plot sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from zs_ssl_clustering.io import sanitize_filename\n",
    "\n",
    "\n",
    "def get_pred_path(row):\n",
    "    \"\"\"\n",
    "    Generate path to y_pred file.\n",
    "    \"\"\"\n",
    "    run_id = row[\"name\"].split(\"__\")[-1]\n",
    "    fname = f\"{row['partition']}-{row['dataset_name']}__{row['model']}__{run_id}.npz\"\n",
    "    fname = sanitize_filename(fname)\n",
    "    fname = os.path.join(\n",
    "        row[\"predictions_dir\"],\n",
    "        sanitize_filename(row[\"partition\"] + f\"__z{float(row['zoom_ratio'])}\"),\n",
    "        fname,\n",
    "    )\n",
    "    return fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms.functional import crop, get_dimensions\n",
    "from torchvision.utils import _log_api_usage_once\n",
    "\n",
    "\n",
    "def center_squaring(img):\n",
    "    \"\"\"Crops the given image at the center.\n",
    "    If the image is torch Tensor, it is expected\n",
    "    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions.\n",
    "    If image size is smaller than output size along any edge, image is padded with 0 and then center cropped.\n",
    "\n",
    "    Args:\n",
    "        img (PIL Image or Tensor): Image to be cropped.\n",
    "        output_size (sequence or int): (height, width) of the crop box. If int or sequence with single int,\n",
    "            it is used for both directions.\n",
    "\n",
    "    Returns:\n",
    "        PIL Image or Tensor: Cropped image.\n",
    "    \"\"\"\n",
    "    if not torch.jit.is_scripting() and not torch.jit.is_tracing():\n",
    "        _log_api_usage_once(center_squaring)\n",
    "\n",
    "    _, image_height, image_width = get_dimensions(img)\n",
    "\n",
    "    if image_height == image_width:\n",
    "        return img\n",
    "\n",
    "    crop_height = crop_width = min(image_height, image_width)\n",
    "\n",
    "    crop_top = int(round((image_height - crop_height) / 2.0))\n",
    "    crop_left = int(round((image_width - crop_width) / 2.0))\n",
    "    return crop(img, crop_top, crop_left, crop_height, crop_width)\n",
    "\n",
    "\n",
    "class CenterSquaring(torch.nn.Module):\n",
    "    \"\"\"Crops the given image to the center square.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        _log_api_usage_once(self)\n",
    "\n",
    "    def forward(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image or Tensor): Image to be cropped.\n",
    "\n",
    "        Returns:\n",
    "            PIL Image or Tensor: Cropped image.\n",
    "        \"\"\"\n",
    "        return center_squaring(img)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zs_ssl_clustering import datasets\n",
    "\n",
    "\n",
    "def show_samples(row, nsamp=12, ds=None, save=False, clusterer=\"\", nclusters=None, skip_existing=None):\n",
    "    if skip_existing is None:\n",
    "        skip_existing = save\n",
    "\n",
    "    if clusterer:\n",
    "        clusterer = clusterer.replace(\"/\", \"\").replace(\" \", \"\")\n",
    "    else:\n",
    "        clusterer = row[\"clusterer_name\"]\n",
    "\n",
    "    output_dir = \"../samples\"\n",
    "    output_fname = f\"samples__{row['dataset_name']}__{row['model']}__{clusterer}.png\"\n",
    "    output_fname = os.path.join(output_dir, output_fname)\n",
    "    if skip_existing and os.path.exists(output_fname):\n",
    "        print(f\"Output {output_fname} already exists. Skipping.\")\n",
    "        return\n",
    "\n",
    "    if ds is None:\n",
    "        dses = datasets.fetch_image_dataset(row[\"dataset_name\"], transform_eval=CenterSquaring())\n",
    "        if row[\"partition\"] == \"train\":\n",
    "            ds = dses[0]\n",
    "        elif row[\"partition\"] == \"test\":\n",
    "            ds = dses[-1]\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    y_pred = np.load(\"../\" + get_pred_path(row))[\"y_pred\"]\n",
    "\n",
    "    u_labels, label_count = np.unique(y_pred, return_counts=True)\n",
    "    # Remove clusters with very few samples in the cluster\n",
    "    # u_labels = u_labels[label_count >= nsamp]\n",
    "\n",
    "    if nclusters is None:\n",
    "        nclusters = len(u_labels)\n",
    "    else:\n",
    "        nclusters = min(nclusters, len(u_labels))\n",
    "\n",
    "    fig, axs = plt.subplots(nclusters, nsamp, figsize=(nsamp / 2, nclusters / 2))\n",
    "\n",
    "    for i_label, label in enumerate(u_labels[:nclusters]):\n",
    "        indices = np.where(y_pred == label)[0]\n",
    "        np.random.default_rng(seed=label).shuffle(indices)\n",
    "        for i in range(nsamp):\n",
    "            if i < len(indices):\n",
    "                idx = indices[i]\n",
    "                axs[i_label, i].imshow(ds[idx][0].convert(\"RGB\"))\n",
    "            axs[i_label, i].axis(\"off\")\n",
    "\n",
    "    if save:\n",
    "        print(f\"Saving to {output_fname}\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        plt.savefig(output_fname, bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "def fetch_row(dataset, model, clusterer):\n",
    "    override_fields = {\n",
    "        \"predictions_dir\": \"y_pred\",\n",
    "    }\n",
    "    if clusterer == \"HDBSCAN\" and dataset in [\"celeba\", \"utkface\"]:\n",
    "        override_fields[\"min_samples\"] = 2\n",
    "    filter1 = {\"model\": model, \"dataset\": dataset}\n",
    "    filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "    filter2.update(filter1)\n",
    "    filter2.update(override_fields)\n",
    "    filter2 = fixup_filter(filter2)\n",
    "    sdf = select_rows(test_runs_df, filter2, allow_missing=False)\n",
    "    if len(sdf) < 1:\n",
    "        print(f\"No data for {filter2}\")\n",
    "        print(filter2command(filter2, partition=\"test\"))\n",
    "        return\n",
    "    elif len(sdf) > 1:\n",
    "        perf = sdf.iloc[0][\"AMI\"]\n",
    "        if sum(sdf[\"AMI\"] != perf) > 0:\n",
    "            print()\n",
    "            print(\"More than one result with AMIs:\", list(sdf[\"AMI\"]))\n",
    "            print(f\"for search {filter2}\")\n",
    "            dif_cols = find_differing_columns(sdf, config_keys)\n",
    "            print(f\"columns which differ: {dif_cols}\")\n",
    "            if dif_cols:\n",
    "                for col in dif_cols:\n",
    "                    print(f\"  {col}: {list(sdf[col])}\")\n",
    "        return\n",
    "    return sdf.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"svhn\"\n",
    "model = \"mocov3_resnet50\"\n",
    "clusterer = \"AC w/ C\"\n",
    "\n",
    "override_fields = {\n",
    "    \"predictions_dir\": \"y_pred\",\n",
    "    # \"aggclust_dist_thresh\": None,  # Use this to flip between unknown/known num clusters for Agglom\n",
    "}\n",
    "filter1 = {\"model\": model, \"dataset\": dataset}\n",
    "filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "filter2.update(filter1)\n",
    "filter2.update(override_fields)\n",
    "filter2 = fixup_filter(filter2)\n",
    "sdf = select_rows(test_runs_df, filter2, allow_missing=False)\n",
    "if len(sdf) < 1:\n",
    "    print(f\"No data for {filter2}\")\n",
    "    print(filter2command(filter2, partition=\"test\"))\n",
    "elif len(sdf) > 1:\n",
    "    perf = sdf.iloc[0][\"AMI\"]\n",
    "    if sum(sdf[\"AMI\"] != perf) > 0:\n",
    "        print()\n",
    "        print(\"More than one result with AMIs:\", list(sdf[\"AMI\"]))\n",
    "        print(f\"for search {filter2}\")\n",
    "        dif_cols = find_differing_columns(sdf, config_keys)\n",
    "        print(f\"columns which differ: {dif_cols}\")\n",
    "        if dif_cols:\n",
    "            for col in dif_cols:\n",
    "                print(f\"  {col}: {list(sdf[col])}\")\n",
    "else:\n",
    "    display(sdf)\n",
    "    row = sdf.iloc[0]\n",
    "    print(\n",
    "        row[\"name\"].split(\"__\")[-1],\n",
    "        \"\\n\" + row[\"name\"],\n",
    "        \"\\n  \" + row[\"dataset_name\"],\n",
    "        \"\\n  \" + row[\"model\"],\n",
    "        \"\\n  \" + row[\"clusterer_name\"],\n",
    "        f\"\\n  AMI={row['AMI']}\",\n",
    "        f\"\\n  S_reduced={row['silhouette-euclidean_pred']}\",\n",
    "        f\"\\n  S_originl={row['silhouette-og-euclidean_pred']}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.load(\"../\" + get_pred_path(row))[\"y_pred\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.fetch_image_dataset(row[\"dataset_name\"])[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.where(y_pred == 0)[0]\n",
    "np.random.default_rng(seed=0).shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 3\n",
    "nsamp = 10\n",
    "\n",
    "indices = np.where(y_pred == label)[0]\n",
    "np.random.default_rng(seed=label).shuffle(indices)\n",
    "\n",
    "fig, axs = plt.subplots(1, nsamp, figsize=(6, 2))\n",
    "\n",
    "for i in range(10):\n",
    "    idx = indices[i]\n",
    "    axs[i].imshow(ds[idx][0])\n",
    "    axs[i].axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamp = 10\n",
    "\n",
    "u_labels = np.unique(y_pred)\n",
    "\n",
    "fig, axs = plt.subplots(len(u_labels), nsamp, figsize=(len(u_labels) / 2, nsamp / 2))\n",
    "\n",
    "for i_label, label in enumerate(u_labels):\n",
    "    indices = np.where(y_pred == label)[0]\n",
    "    np.random.default_rng(seed=label).shuffle(indices)\n",
    "    for i in range(10):\n",
    "        idx = indices[i]\n",
    "        axs[i_label, i].imshow(ds[idx][0])\n",
    "        axs[i_label, i].axis(\"off\")\n",
    "\n",
    "# plt.savefig(f\"{row['dataset_name']}_{row['model']}_{row['clusterer_name']}.png\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.fetch_image_dataset(\"flowers102\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.fetch_image_dataset(\"flowers102\", transform_eval=CenterSquaring())[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"svhn\"\n",
    "model = \"mocov3_resnet50\"\n",
    "clusterer = \"AC w/ C\"\n",
    "\n",
    "row = fetch_row(dataset, model, clusterer)\n",
    "print(\n",
    "    row[\"name\"].split(\"__\")[-1],\n",
    "    \"\\n\" + row[\"name\"],\n",
    "    \"\\n  \" + row[\"dataset_name\"],\n",
    "    \"\\n  \" + row[\"model\"],\n",
    "    \"\\n  \" + row[\"clusterer_name\"],\n",
    "    f\"\\n  AMI        = {row['AMI']}\",\n",
    "    f\"\\n  S_reduced  = {row['silhouette-euclidean_pred']}\",\n",
    "    f\"\\n  S_original = {row['silhouette-og-euclidean_pred']}\",\n",
    ")\n",
    "show_samples(row, save=True, clusterer=clusterer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for clusterer in [\"AC w/ C\"]:  # , \"AC w/o C\"]:\n",
    "    for model in [\"mocov3_resnet50\", \"mocov3_vit_base\", \"dino_resnet50\", \"dino_vitb16\"]:\n",
    "        for dataset in [\n",
    "            \"mnist\",\n",
    "            \"fashionmnist\",\n",
    "            \"svhn\",\n",
    "            \"cifar10\",\n",
    "            \"cifar100\",\n",
    "            \"flowers102\",\n",
    "            \"aircraft\",\n",
    "        ]:\n",
    "            print()\n",
    "            print(f\"{dataset:<16s} {model:<32s} {clusterer}\")\n",
    "            row = fetch_row(dataset, model, clusterer)\n",
    "            if row is None:\n",
    "                print(\"No data with y_pred for\", dataset, model, clusterer)\n",
    "                continue\n",
    "            print(\n",
    "                row[\"name\"].split(\"__\")[-1],\n",
    "                \"\\n\" + row[\"name\"],\n",
    "                \"\\n  \" + row[\"dataset_name\"],\n",
    "                \"\\n  \" + row[\"model\"],\n",
    "                \"\\n  \" + row[\"clusterer_name\"],\n",
    "                f\"\\n  AMI        = {row['AMI']}\",\n",
    "                f\"\\n  S_reduced  = {row['silhouette-euclidean_pred']}\",\n",
    "                f\"\\n  S_original = {row['silhouette-og-euclidean_pred']}\",\n",
    "            )\n",
    "            fig = show_samples(row, save=True, clusterer=clusterer, nclusters=150)\n",
    "            # plt.show()\n",
    "            print(\"\\n\\nStopping early!\")\n",
    "            break\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for clusterer in [\"AC w/o C\"]:\n",
    "    for model in RESNET50_MODELS + VITB16_MODELS:\n",
    "        for dataset in [\n",
    "            \"mnist\",\n",
    "            \"fashionmnist\",\n",
    "            \"svhn\",\n",
    "            \"cifar10\",\n",
    "            \"cifar100\",\n",
    "            \"flowers102\",\n",
    "            \"aircraft\",\n",
    "        ]:\n",
    "            print()\n",
    "            print(f\"{dataset:<16s} {model:<32s} {clusterer}\")\n",
    "            row = fetch_row(dataset, model, clusterer)\n",
    "            if row is None:\n",
    "                print(\"No data with y_pred for\", dataset, model, clusterer)\n",
    "                continue\n",
    "            print(\n",
    "                row[\"name\"].split(\"__\")[-1],\n",
    "                \"\\n\" + row[\"name\"],\n",
    "                \"\\n  \" + row[\"dataset_name\"],\n",
    "                \"\\n  \" + row[\"model\"],\n",
    "                \"\\n  \" + row[\"clusterer_name\"],\n",
    "                f\"\\n  AMI        = {row['AMI']}\",\n",
    "                f\"\\n  S_reduced  = {row['silhouette-euclidean_pred']}\",\n",
    "                f\"\\n  S_original = {row['silhouette-og-euclidean_pred']}\",\n",
    "            )\n",
    "            fig = show_samples(row, save=True, clusterer=clusterer, nclusters=150)\n",
    "            # plt.show()\n",
    "            print(\"\\n\\nStopping early!\")\n",
    "            break\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clusterer in [\"AC w/o C\"]:\n",
    "    for model in RESNET50_MODELS + VITB16_MODELS:\n",
    "        for dataset in [\"inaturalist\"]:\n",
    "            print()\n",
    "            print(f\"{dataset:<16s} {model:<32s} {clusterer}\")\n",
    "            row = fetch_row(dataset, model, clusterer)\n",
    "            if row is None:\n",
    "                print(\"No data with y_pred for\", dataset, model, clusterer)\n",
    "                continue\n",
    "            print(\n",
    "                row[\"name\"].split(\"__\")[-1],\n",
    "                \"\\n\" + row[\"name\"],\n",
    "                \"\\n  \" + row[\"dataset_name\"],\n",
    "                \"\\n  \" + row[\"model\"],\n",
    "                \"\\n  \" + row[\"clusterer_name\"],\n",
    "                f\"\\n  AMI        = {row['AMI']}\",\n",
    "                f\"\\n  S_reduced  = {row['silhouette-euclidean_pred']}\",\n",
    "                f\"\\n  S_original = {row['silhouette-og-euclidean_pred']}\",\n",
    "            )\n",
    "            fig = show_samples(row, save=True, clusterer=clusterer, nclusters=150)\n",
    "            # plt.show()\n",
    "            plt.close()\n",
    "            print(\"\\n\\nStopping early!\")\n",
    "            break\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for clusterer in [\"AC w/o C\"]:\n",
    "    for model in RESNET50_MODELS + VITB16_MODELS:\n",
    "        for dataset in TEST_DATASETS:\n",
    "            print()\n",
    "            print(f\"{dataset:<16s} {model:<32s} {clusterer}\")\n",
    "            row = fetch_row(dataset, model, clusterer)\n",
    "            if row is None:\n",
    "                print(\"No data with y_pred for\", dataset, model, clusterer)\n",
    "                continue\n",
    "            print(\n",
    "                row[\"name\"].split(\"__\")[-1],\n",
    "                \"\\n\" + row[\"name\"],\n",
    "                \"\\n  \" + row[\"dataset_name\"],\n",
    "                \"\\n  \" + row[\"model\"],\n",
    "                \"\\n  \" + row[\"clusterer_name\"],\n",
    "                f\"\\n  AMI        = {row['AMI']}\",\n",
    "                f\"\\n  S_reduced  = {row['silhouette-euclidean_pred']}\",\n",
    "                f\"\\n  S_original = {row['silhouette-og-euclidean_pred']}\",\n",
    "            )\n",
    "            try:\n",
    "                fig = show_samples(row, save=True, clusterer=clusterer, nclusters=150)\n",
    "            except Exception:\n",
    "                print(f\"{dataset} not found\")\n",
    "            try:\n",
    "                plt.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "            print(\"\\n\\nStopping early!\")\n",
    "            break\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Breakdown information about datasets with multiple labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### CelebA attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeba_test = torchvision.datasets.CelebA(\n",
    "    os.path.expanduser(\"~/Datasets\"),\n",
    "    target_type=\"attr\",\n",
    "    split=\"test\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeba_test.attr_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "MyWQRN9iqBkI",
    "outputId": "2642efe3-b0ef-46f3-dc70-f2f5c69645d4"
   },
   "outputs": [],
   "source": [
    "metric_key = \"AMI\"  # AMI  num_cluster_pred  silhouette-euclidean_pred  silhouette-og-euclidean_pred\n",
    "show_pc = True\n",
    "show_fmt = \"{:4.0f}\"\n",
    "show_commands = False\n",
    "highlight_best = True\n",
    "use_si_num = False\n",
    "eps = 0.005\n",
    "override_fields = {\n",
    "    \"predictions_dir\": \"y_pred\",\n",
    "}\n",
    "\n",
    "backbone = \"ResNet-50\"  # \"ResNet-50\" or \"ViT-B\"\n",
    "\n",
    "if metric_key == \"num_cluster_pred\":\n",
    "    CLUSTERERS = [\"AC w/o C\", \"AffinityPropagation\", \"HDBSCAN\"]\n",
    "    show_pc = False\n",
    "    show_fmt = \"{:4.0f}\"\n",
    "    highlight_best = False\n",
    "    use_si_num = True\n",
    "else:\n",
    "    CLUSTERERS = [\n",
    "        \"KMeans\",\n",
    "        \"SpectralClustering\",\n",
    "        \"AC w/ C\",\n",
    "        \"AC w/o C\",\n",
    "        \"AffinityPropagation\",\n",
    "        \"HDBSCAN\",\n",
    "    ]\n",
    "if metric_key.startswith(\"silhouette\"):\n",
    "    show_pc = False\n",
    "    show_fmt = \"{:5.2f}\"\n",
    "\n",
    "print(MODEL_GROUPS)\n",
    "\n",
    "dataset = \"celeba\"\n",
    "\n",
    "TEST_ATTRS = [\"Identity\"] + celeba_test.attr_names[:-1]\n",
    "TEST_ATTRS = [\n",
    "    \"Identity\",\n",
    "    \"Attractive\",\n",
    "    \"Bald\",\n",
    "    \"Eyeglasses\",\n",
    "    \"Heavy_Makeup\",\n",
    "    \"Male\",\n",
    "    \"No_Beard\",\n",
    "    \"Wearing_Lipstick\",\n",
    "]\n",
    "print(TEST_ATTRS)\n",
    "best_results = {k: [] for k in TEST_ATTRS}\n",
    "best_results_grouped = {k: defaultdict(list) for k in TEST_ATTRS}\n",
    "\n",
    "for dummy in [True, False]:\n",
    "    cmds = []\n",
    "    latex_table = r\"% Results for \" + f\"{dataset} breakdown, {metric_key}, {backbone}\" + \"\\n\"\n",
    "    now_str = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    latex_table += r\"% Generated \" + now_str + \"\\n\"\n",
    "    latex_table += r\"% Using hparams \" + BEST_PARAMS[\"_version\"] + \"\\n\"\n",
    "    label = backbone\n",
    "    if metric_key == \"AMI\":\n",
    "        latex_table += r\"\\label{tab:\" + label + r\"}\" + \"\\n\"\n",
    "    label = metric_key.replace(\"_\", \"-\") + \":\" + label\n",
    "    latex_table += r\"\\label{tab:\" + label + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"\\resizebox{\\textwidth}{!}{%\" + \"\\n\"\n",
    "    latex_table += r\"\\begin{tabular}{ll\" + r\"r\" * len(TEST_ATTRS) + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    latex_table += r\"& \" + f\"{'Encoder':<11s}\"\n",
    "    for attr in TEST_ATTRS:\n",
    "        latex_table += r\"&\" + f\"{attr.replace('_', ' '):^15s}\"\n",
    "    latex_table += r\"\\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    print(MODEL_GROUPS[backbone])\n",
    "    if metric_key == \"num_cluster_pred\":\n",
    "        latex_table += r\"& Num targets\"\n",
    "        for i_attr, attr in enumerate(TEST_ATTRS):\n",
    "            sdf = select_rows(test_runs_df, {\"dataset\": dataset}, allow_missing=False)\n",
    "            sdf = sdf[~pd.isna(sdf[\"num_cluster_true\"])]\n",
    "            latex_table += r\"& \"\n",
    "            latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "            latex_table += f\"{sdf.iloc[0]['num_cluster_true'].item()}\"\n",
    "            latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "        latex_table += r\"\\\\\" + \"\\n\"\n",
    "        latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    elif metric_key.endswith(\"_pred\"):\n",
    "        metric_key2 = metric_key.replace(\"_pred\", \"_true\")\n",
    "        clusterername = \"G.T.\"\n",
    "        latex_table += (\n",
    "            r\"\\parbox[t]{2mm}{\\multirow{\"\n",
    "            + str(len(MODEL_GROUPS[backbone]))\n",
    "            + r\"}{*}{\"\n",
    "            + r\"\\scalebox{0.9}{\"\n",
    "            + r\"\\rotatebox[origin=c]{90}{\"\n",
    "            + clusterername\n",
    "            + r\"}}}\"\n",
    "            + r\"}\"\n",
    "        )\n",
    "        latex_table += \"\\n\"\n",
    "        for i_group, model in enumerate(list(MODEL_GROUPS[backbone])):\n",
    "            latex_table += f\"& {MODEL2SH[model]:<10s}\"\n",
    "            for i_attr, attr in enumerate(TEST_ATTRS):\n",
    "                latex_table += \" &\"\n",
    "                filter1 = {\"model\": model, \"dataset\": dataset}\n",
    "                if model == \"timm_vit_base_patch16_224.mae\":\n",
    "                    filter1[\"dim_reducer\"] = \"PCA\"\n",
    "                    filter1[\"pca_variance\"] = 0.95\n",
    "                else:\n",
    "                    filter1[\"dim_reducer_man\"] = \"UMAP\"\n",
    "                    filter1[\"ndim_reduced_man\"] = 50\n",
    "                    filter1[\"dim_reducer_man_metric\"] = \"euclidean\"\n",
    "                sdf = select_rows(test_runs_df, filter1, allow_missing=False)\n",
    "                sdf = sdf[~pd.isna(sdf[metric_key2])]\n",
    "                my_val = np.nanmedian(sdf[metric_key])\n",
    "                if sum(sdf[metric_key2] != my_val) > 0:\n",
    "                    pass\n",
    "                if dummy:\n",
    "                    best_results_grouped[attr][clusterername].append(my_val)\n",
    "                    continue\n",
    "                is_best_grp = my_val + eps >= np.max(best_results_grouped[attr][clusterername])\n",
    "                latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "                latex_table += \"     \"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best_grp:\n",
    "                    latex_table += r\"\\tcg{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                latex_table += show_fmt.format(my_val)\n",
    "                if highlight_best:\n",
    "                    latex_table += r\"}\" if is_best_grp else \" \"\n",
    "                latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "\n",
    "            latex_table += r\" \\\\\" + \"\\n\"\n",
    "        latex_table += r\"\\toprule\" + \"\\n\"\n",
    "\n",
    "    first_agg = True\n",
    "    for i_clusterer, clusterer in enumerate(CLUSTERERS):\n",
    "        clusterername = CLUSTERER2SH.get(clusterer, clusterer)\n",
    "        my_override_fields = override_fields.copy()\n",
    "        if first_agg and clusterer == \"AgglomerativeClustering\" and metric_key != \"num_cluster_pred\":\n",
    "            first_agg = False\n",
    "            my_override_fields[\"aggclust_dist_thresh\"] = None\n",
    "            clusterername = \"AC  w/ C\"\n",
    "        elif clusterer == \"AgglomerativeClustering\":\n",
    "            clusterername = \"AC w/o C\"\n",
    "            if \"aggclust_dist_thresh\" in my_override_fields:\n",
    "                del my_override_fields[\"aggclust_dist_thresh\"]\n",
    "\n",
    "        if i_clusterer > 0:\n",
    "            latex_table += r\"\\midrule\" + \"\\n\"\n",
    "\n",
    "        latex_table += (\n",
    "            r\"\\parbox[t]{2mm}{\\multirow{\"\n",
    "            + str(len(MODEL_GROUPS[backbone]))\n",
    "            + r\"}{*}{\"\n",
    "            + r\"\\scalebox{0.9}{\"\n",
    "            + r\"\\rotatebox[origin=c]{90}{\"\n",
    "            + clusterername\n",
    "            + r\"}}}\"\n",
    "            + r\"}\"\n",
    "        )\n",
    "        latex_table += \"\\n\"\n",
    "\n",
    "        for i_group, model in enumerate(list(MODEL_GROUPS[backbone])):\n",
    "            latex_table += f\"& {MODEL2SH[model]:<10s}\"\n",
    "            filter1 = {\"model\": model, \"dataset\": dataset}\n",
    "            filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "            filter2.update(filter1)\n",
    "            filter2.update(my_override_fields)\n",
    "            filter2 = fixup_filter(filter2)\n",
    "            sdf = select_rows(test_runs_df, filter2, allow_missing=False)\n",
    "            if len(sdf) < 1:\n",
    "                print(f\"No data for {model}-{dataset}-{clusterer}\\n{filter2}\")\n",
    "                cmds.append(filter2command(filter2, partition=\"test\"))\n",
    "                if not dummy:\n",
    "                    # latex_table += r\"\\multicolumn{1}{c}{--}\"\n",
    "                    latex_table += r\"   --  \"\n",
    "                continue\n",
    "            if len(sdf) > 1:\n",
    "                if sum(np.abs(sdf[metric_key] - sdf.iloc[0][metric_key]) > 1e-6) > 0:\n",
    "                    print()\n",
    "                    print(\n",
    "                        f\"More than one result with {metric_key} values\",\n",
    "                        list(sdf[metric_key]),\n",
    "                    )\n",
    "                    print(f\"for search {filter2}\")\n",
    "                    dif_cols = find_differing_columns(sdf, config_keys)\n",
    "                    print(f\"columns which differ: {dif_cols}\")\n",
    "                    if dif_cols:\n",
    "                        for col in dif_cols:\n",
    "                            print(f\"  {col}: {list(sdf[col])}\")\n",
    "            y_pred = np.load(\"../\" + get_pred_path(sdf.iloc[0]))[\"y_pred\"]\n",
    "            for i_attr, attr in enumerate(TEST_ATTRS):\n",
    "                latex_table += \" &\"\n",
    "                if metric_key.lower() != \"ami\":\n",
    "                    raise NotImplementedError()\n",
    "                if attr.lower() == \"identity\":\n",
    "                    my_val = sklearn.metrics.adjusted_mutual_info_score(celeba_test.identity[:, 0], y_pred)\n",
    "                else:\n",
    "                    i_attr = np.where(np.asarray(celeba_test.attr_names) == attr)[0][0]\n",
    "                    my_val = sklearn.metrics.adjusted_mutual_info_score(celeba_test.attr[:, i_attr], y_pred)\n",
    "                if dummy:\n",
    "                    best_results[attr].append(my_val)\n",
    "                    best_results_grouped[attr][clusterername].append(my_val)\n",
    "                    continue\n",
    "                if np.isnan(my_val):\n",
    "                    latex_table += r\"   --  \"\n",
    "                    continue\n",
    "                is_best = my_val + eps >= np.max(best_results[attr])\n",
    "                if len(best_results[attr]) > 1:\n",
    "                    is_secd = my_val + eps >= np.sort(best_results[attr])[-2]\n",
    "                else:\n",
    "                    is_secd = False\n",
    "                is_best_grp = my_val + eps >= np.max(best_results_grouped[attr][clusterername])\n",
    "                sc_base = np.nanmedian(best_results[attr])\n",
    "                sc_top = np.max(best_results[attr])\n",
    "                sc = 100 * max(0, (my_val - sc_base) / (sc_top - sc_base))\n",
    "                latex_table += r\"\\cellcolor{cbg!\" + f\"{sc:.0f}\" + \"}\"\n",
    "                if show_pc:\n",
    "                    my_val = my_val * 100\n",
    "                latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best:\n",
    "                    latex_table += r\"\\tcf{\"\n",
    "                elif is_secd:\n",
    "                    latex_table += r\"\\tcs{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best_grp:\n",
    "                    latex_table += r\"\\tcg{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                latex_table += show_fmt.format(my_val)\n",
    "                if highlight_best:\n",
    "                    latex_table += r\"}\" if is_best or is_secd else \" \"\n",
    "                    latex_table += r\"}\" if is_best_grp else \" \"\n",
    "                latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "            latex_table += r\" \\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\bottomrule\" + \"\\n\"\n",
    "    latex_table += r\"\\end{tabular}\" + \"\\n\"\n",
    "    latex_table += r\"}\" + \"\\n\"\n",
    "\n",
    "print()\n",
    "print(f\"There are {len(cmds)} commands to execute to generate missing datapoints\")\n",
    "if show_commands:\n",
    "    for cmd in cmds:\n",
    "        print(cmd)\n",
    "\n",
    "print()\n",
    "print(\"Done!\")\n",
    "print()\n",
    "print(f\"Here is your {dataset} results table for {metric_key}, {backbone}:\")\n",
    "print()\n",
    "print()\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTKFace breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zs_ssl_clustering.datasets\n",
    "\n",
    "utkface_test = zs_ssl_clustering.datasets.fetch_image_dataset(\"utkface\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utkface_test.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_names = [\"age\", \"gender\", \"race\"]\n",
    "attrs = utkface_test.metadata[[\"age\", \"gender_id\", \"race_id\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "MyWQRN9iqBkI",
    "outputId": "2642efe3-b0ef-46f3-dc70-f2f5c69645d4"
   },
   "outputs": [],
   "source": [
    "metric_key = \"AMI\"  # AMI  num_cluster_pred  silhouette-euclidean_pred  silhouette-og-euclidean_pred\n",
    "show_pc = True\n",
    "show_fmt = \"{:4.1f}\"\n",
    "show_commands = False\n",
    "highlight_best = True\n",
    "use_si_num = False\n",
    "eps = 0.0005\n",
    "override_fields = {\n",
    "    \"predictions_dir\": \"y_pred\",\n",
    "}\n",
    "\n",
    "backbone = \"ViT-B\"  # \"ResNet-50\" or \"ViT-B\"\n",
    "\n",
    "if metric_key == \"num_cluster_pred\":\n",
    "    CLUSTERERS = [\"AC w/o C\", \"AffinityPropagation\", \"HDBSCAN\"]\n",
    "    show_pc = False\n",
    "    show_fmt = \"{:4.0f}\"\n",
    "    highlight_best = False\n",
    "    use_si_num = True\n",
    "else:\n",
    "    CLUSTERERS = [\n",
    "        \"KMeans\",\n",
    "        \"SpectralClustering\",\n",
    "        \"AC w/ C\",\n",
    "        \"AC w/o C\",\n",
    "        \"AffinityPropagation\",\n",
    "        \"HDBSCAN\",\n",
    "    ]\n",
    "if metric_key.startswith(\"silhouette\"):\n",
    "    show_pc = False\n",
    "    show_fmt = \"{:5.2f}\"\n",
    "\n",
    "print(MODEL_GROUPS)\n",
    "\n",
    "dataset = \"utkface\"\n",
    "\n",
    "TEST_ATTRS = attr_names\n",
    "print(TEST_ATTRS)\n",
    "best_results = {k: [] for k in TEST_ATTRS}\n",
    "best_results_grouped = {k: defaultdict(list) for k in TEST_ATTRS}\n",
    "\n",
    "for dummy in [True, False]:\n",
    "    cmds = []\n",
    "    latex_table = r\"% Results for \" + f\"{dataset} breakdown, {metric_key}, {backbone}\" + \"\\n\"\n",
    "    now_str = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    latex_table += r\"% Generated \" + now_str + \"\\n\"\n",
    "    latex_table += r\"% Using hparams \" + BEST_PARAMS[\"_version\"] + \"\\n\"\n",
    "    label = backbone\n",
    "    if metric_key == \"AMI\":\n",
    "        latex_table += r\"\\label{tab:\" + label + r\"}\" + \"\\n\"\n",
    "    label = metric_key.replace(\"_\", \"-\") + \":\" + label\n",
    "    latex_table += r\"\\label{tab:\" + label + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"%\\resizebox{\\textwidth}{!}{%\" + \"\\n\"  # Disabled\n",
    "    latex_table += r\"\\begin{tabular}{ll\" + r\"r\" * len(TEST_ATTRS) + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    latex_table += r\"& \" + f\"{'Encoder':<11s}\"\n",
    "    for attr in TEST_ATTRS:\n",
    "        latex_table += r\"&\" + f\"{attr.replace('_', ' '):^15s}\"\n",
    "    latex_table += r\"\\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    print(MODEL_GROUPS[backbone])\n",
    "    if metric_key == \"num_cluster_pred\":\n",
    "        latex_table += r\"& Num targets\"\n",
    "        for i_attr, attr in enumerate(TEST_ATTRS):\n",
    "            sdf = select_rows(test_runs_df, {\"dataset\": dataset}, allow_missing=False)\n",
    "            sdf = sdf[~pd.isna(sdf[\"num_cluster_true\"])]\n",
    "            latex_table += r\"& \"\n",
    "            latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "            latex_table += f\"{sdf.iloc[0]['num_cluster_true'].item()}\"\n",
    "            latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "        latex_table += r\"\\\\\" + \"\\n\"\n",
    "        latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    elif metric_key.endswith(\"_pred\"):\n",
    "        metric_key2 = metric_key.replace(\"_pred\", \"_true\")\n",
    "        clusterername = \"G.T.\"\n",
    "        latex_table += (\n",
    "            r\"\\parbox[t]{2mm}{\\multirow{\"\n",
    "            + str(len(MODEL_GROUPS[backbone]))\n",
    "            + r\"}{*}{\"\n",
    "            + r\"\\scalebox{0.9}{\"\n",
    "            + r\"\\rotatebox[origin=c]{90}{\"\n",
    "            + clusterername\n",
    "            + r\"}}}\"\n",
    "            + r\"}\"\n",
    "        )\n",
    "        latex_table += \"\\n\"\n",
    "        for i_group, model in enumerate(list(MODEL_GROUPS[backbone])):\n",
    "            latex_table += f\"& {MODEL2SH[model]:<10s}\"\n",
    "            for i_attr, attr in enumerate(TEST_ATTRS):\n",
    "                latex_table += \" &\"\n",
    "                filter1 = {\"model\": model, \"dataset\": dataset}\n",
    "                if model == \"timm_vit_base_patch16_224.mae\":\n",
    "                    filter1[\"dim_reducer\"] = \"PCA\"\n",
    "                    filter1[\"pca_variance\"] = 0.95\n",
    "                else:\n",
    "                    filter1[\"dim_reducer_man\"] = \"UMAP\"\n",
    "                    filter1[\"ndim_reduced_man\"] = 50\n",
    "                    filter1[\"dim_reducer_man_metric\"] = \"euclidean\"\n",
    "                sdf = select_rows(test_runs_df, filter1, allow_missing=False)\n",
    "                sdf = sdf[~pd.isna(sdf[metric_key2])]\n",
    "                my_val = np.nanmedian(sdf[metric_key])\n",
    "                if sum(sdf[metric_key2] != my_val) > 0:\n",
    "                    pass\n",
    "                if dummy:\n",
    "                    best_results_grouped[attr][clusterername].append(my_val)\n",
    "                    continue\n",
    "                is_best_grp = my_val + eps >= np.max(best_results_grouped[attr][clusterername])\n",
    "                latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "                latex_table += \"     \"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best_grp:\n",
    "                    latex_table += r\"\\tcg{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                latex_table += show_fmt.format(my_val)\n",
    "                if highlight_best:\n",
    "                    latex_table += r\"}\" if is_best_grp else \" \"\n",
    "                latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "\n",
    "            latex_table += r\" \\\\\" + \"\\n\"\n",
    "        latex_table += r\"\\toprule\" + \"\\n\"\n",
    "\n",
    "    first_agg = True\n",
    "    for i_clusterer, clusterer in enumerate(CLUSTERERS):\n",
    "        clusterername = CLUSTERER2SH.get(clusterer, clusterer)\n",
    "        my_override_fields = override_fields.copy()\n",
    "        if first_agg and clusterer == \"AgglomerativeClustering\" and metric_key != \"num_cluster_pred\":\n",
    "            first_agg = False\n",
    "            my_override_fields[\"aggclust_dist_thresh\"] = None\n",
    "            clusterername = \"AC  w/ C\"\n",
    "        elif clusterer == \"AgglomerativeClustering\":\n",
    "            clusterername = \"AC w/o C\"\n",
    "            if \"aggclust_dist_thresh\" in my_override_fields:\n",
    "                del my_override_fields[\"aggclust_dist_thresh\"]\n",
    "\n",
    "        if i_clusterer > 0:\n",
    "            latex_table += r\"\\midrule\" + \"\\n\"\n",
    "\n",
    "        latex_table += (\n",
    "            r\"\\parbox[t]{2mm}{\\multirow{\"\n",
    "            + str(len(MODEL_GROUPS[backbone]))\n",
    "            + r\"}{*}{\"\n",
    "            + r\"\\scalebox{0.9}{\"\n",
    "            + r\"\\rotatebox[origin=c]{90}{\"\n",
    "            + clusterername\n",
    "            + r\"}}}\"\n",
    "            + r\"}\"\n",
    "        )\n",
    "        latex_table += \"\\n\"\n",
    "\n",
    "        for i_group, model in enumerate(list(MODEL_GROUPS[backbone])):\n",
    "            latex_table += f\"& {MODEL2SH[model]:<10s}\"\n",
    "            filter1 = {\"model\": model, \"dataset\": dataset}\n",
    "            filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "            filter2.update(filter1)\n",
    "            filter2.update(my_override_fields)\n",
    "            filter2 = fixup_filter(filter2)\n",
    "            sdf = select_rows(test_runs_df, filter2, allow_missing=False)\n",
    "            if len(sdf) < 1:\n",
    "                print(f\"No data for {model}-{dataset}-{clusterer}\\n{filter2}\")\n",
    "                cmds.append(filter2command(filter2, partition=\"test\"))\n",
    "                if not dummy:\n",
    "                    # latex_table += r\"\\multicolumn{1}{c}{--}\"\n",
    "                    latex_table += r\"   --  \"\n",
    "                continue\n",
    "            if len(sdf) > 1:\n",
    "                if sum(np.abs(sdf[metric_key] - sdf.iloc[0][metric_key]) > 1e-6) > 0:\n",
    "                    print()\n",
    "                    print(\n",
    "                        f\"More than one result with {metric_key} values\",\n",
    "                        list(sdf[metric_key]),\n",
    "                    )\n",
    "                    print(f\"for search {filter2}\")\n",
    "                    dif_cols = find_differing_columns(sdf, config_keys)\n",
    "                    print(f\"columns which differ: {dif_cols}\")\n",
    "                    if dif_cols:\n",
    "                        for col in dif_cols:\n",
    "                            print(f\"  {col}: {list(sdf[col])}\")\n",
    "            y_pred = np.load(\"../\" + get_pred_path(sdf.iloc[0]))[\"y_pred\"]\n",
    "            for i_attr, attr in enumerate(TEST_ATTRS):\n",
    "                latex_table += \" &\"\n",
    "                if metric_key.lower() == \"ami\":\n",
    "                    my_val = sklearn.metrics.adjusted_mutual_info_score(attrs[:, i_attr], y_pred)\n",
    "                elif metric_key.lower() == \"nmi\":\n",
    "                    my_val = sklearn.metrics.normalized_mutual_info_score(attrs[:, i_attr], y_pred)\n",
    "                elif metric_key.lower() == \"mi\":\n",
    "                    my_val = sklearn.metrics.mutual_info_score(attrs[:, i_attr], y_pred)\n",
    "                elif metric_key.lower() == \"rand_score\":\n",
    "                    my_val = sklearn.metrics.rand_score(attrs[:, i_attr], y_pred)\n",
    "                else:\n",
    "                    raise NotImplementedError()\n",
    "                if dummy:\n",
    "                    best_results[attr].append(my_val)\n",
    "                    best_results_grouped[attr][clusterername].append(my_val)\n",
    "                    continue\n",
    "                if np.isnan(my_val):\n",
    "                    latex_table += r\"   --  \"\n",
    "                    continue\n",
    "                is_best = my_val + eps >= np.max(best_results[attr])\n",
    "                if len(best_results[attr]) > 1:\n",
    "                    is_secd = my_val + eps >= np.sort(best_results[attr])[-2]\n",
    "                else:\n",
    "                    is_secd = False\n",
    "                is_best_grp = my_val + eps >= np.max(best_results_grouped[attr][clusterername])\n",
    "                sc_base = np.nanmedian(best_results[attr])\n",
    "                sc_top = np.max(best_results[attr])\n",
    "                sc = 100 * max(0, (my_val - sc_base) / (sc_top - sc_base))\n",
    "                latex_table += r\"\\cellcolor{cbg!\" + f\"{sc:.0f}\" + \"}\"\n",
    "                if show_pc:\n",
    "                    my_val = my_val * 100\n",
    "                latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best:\n",
    "                    latex_table += r\"\\tcf{\"\n",
    "                elif is_secd:\n",
    "                    latex_table += r\"\\tcs{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best_grp:\n",
    "                    latex_table += r\"\\tcg{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                latex_table += show_fmt.format(my_val)\n",
    "                if highlight_best:\n",
    "                    latex_table += r\"}\" if is_best or is_secd else \" \"\n",
    "                    latex_table += r\"}\" if is_best_grp else \" \"\n",
    "                latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "            latex_table += r\" \\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\bottomrule\" + \"\\n\"\n",
    "    latex_table += r\"\\end{tabular}\" + \"\\n\"\n",
    "    latex_table += r\"%}\" + \"\\n\"  # Disabled\n",
    "\n",
    "print()\n",
    "print(f\"There are {len(cmds)} commands to execute to generate missing datapoints\")\n",
    "if show_commands:\n",
    "    for cmd in cmds:\n",
    "        print(cmd)\n",
    "\n",
    "print()\n",
    "print(\"Done!\")\n",
    "print()\n",
    "print(f\"Here is your {dataset} results table for {metric_key}, {backbone}:\")\n",
    "print()\n",
    "print()\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ImageNet-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenetr_test = datasets.fetch_image_dataset(\"imagenet-r\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artforms = [os.path.basename(fname[0]).split(\"_\")[0] for fname in imagenetr_test.imgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_artform, artform_ids = np.unique(artforms, return_inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(u_artform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(imagenetr_test.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_artform_ids = imagenetr_test.targets + artform_ids * 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(class_artform_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs = np.stack([imagenetr_test.targets, artform_ids, class_artform_ids], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "MyWQRN9iqBkI",
    "outputId": "2642efe3-b0ef-46f3-dc70-f2f5c69645d4"
   },
   "outputs": [],
   "source": [
    "metric_key = \"AMI\"  # AMI  num_cluster_pred  silhouette-euclidean_pred  silhouette-og-euclidean_pred\n",
    "show_pc = True\n",
    "show_fmt = \"{:4.0f}\"\n",
    "show_commands = False\n",
    "highlight_best = True\n",
    "use_si_num = False\n",
    "eps = 0.005\n",
    "override_fields = {\n",
    "    \"predictions_dir\": \"y_pred\",\n",
    "}\n",
    "\n",
    "backbone = \"ResNet-50\"  # \"ResNet-50\" or \"ViT-B\"\n",
    "\n",
    "if metric_key == \"num_cluster_pred\":\n",
    "    CLUSTERERS = [\"AC w/o C\", \"AffinityPropagation\", \"HDBSCAN\"]\n",
    "    show_pc = False\n",
    "    show_fmt = \"{:4.0f}\"\n",
    "    highlight_best = False\n",
    "    use_si_num = True\n",
    "else:\n",
    "    CLUSTERERS = [\n",
    "        \"KMeans\",\n",
    "        \"SpectralClustering\",\n",
    "        \"AC w/ C\",\n",
    "        \"AC w/o C\",\n",
    "        \"AffinityPropagation\",\n",
    "        \"HDBSCAN\",\n",
    "    ]\n",
    "if metric_key.startswith(\"silhouette\"):\n",
    "    show_pc = False\n",
    "    show_fmt = \"{:5.2f}\"\n",
    "\n",
    "print(MODEL_GROUPS)\n",
    "\n",
    "dataset = \"imagenet-r\"\n",
    "\n",
    "TEST_ATTRS = [\"Class\", \"Artform\", \"Both\"]\n",
    "print(TEST_ATTRS)\n",
    "best_results = {k: [] for k in TEST_ATTRS}\n",
    "best_results_grouped = {k: defaultdict(list) for k in TEST_ATTRS}\n",
    "\n",
    "for dummy in [True, False]:\n",
    "    cmds = []\n",
    "    latex_table = r\"% Results for \" + f\"{dataset} breakdown, {metric_key}, {backbone}\" + \"\\n\"\n",
    "    now_str = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    latex_table += r\"% Generated \" + now_str + \"\\n\"\n",
    "    latex_table += r\"% Using hparams \" + BEST_PARAMS[\"_version\"] + \"\\n\"\n",
    "    label = backbone\n",
    "    if metric_key == \"AMI\":\n",
    "        latex_table += r\"\\label{tab:\" + label + r\"}\" + \"\\n\"\n",
    "    label = metric_key.replace(\"_\", \"-\") + \":\" + label\n",
    "    latex_table += r\"\\label{tab:\" + label + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"%\\resizebox{\\textwidth}{!}{%\" + \"\\n\"  # Disabled\n",
    "    latex_table += r\"\\begin{tabular}{ll\" + r\"r\" * len(TEST_ATTRS) + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    latex_table += r\"& \" + f\"{'Encoder':<11s}\"\n",
    "    for attr in TEST_ATTRS:\n",
    "        latex_table += r\"&\" + f\"{attr.replace('_', ' '):^15s}\"\n",
    "    latex_table += r\"\\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    print(MODEL_GROUPS[backbone])\n",
    "    if metric_key == \"num_cluster_pred\":\n",
    "        latex_table += r\"& Num targets\"\n",
    "        for i_attr, attr in enumerate(TEST_ATTRS):\n",
    "            sdf = select_rows(test_runs_df, {\"dataset\": dataset}, allow_missing=False)\n",
    "            sdf = sdf[~pd.isna(sdf[\"num_cluster_true\"])]\n",
    "            latex_table += r\"& \"\n",
    "            latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "            latex_table += f\"{sdf.iloc[0]['num_cluster_true'].item()}\"\n",
    "            latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "        latex_table += r\"\\\\\" + \"\\n\"\n",
    "        latex_table += r\"\\midrule\" + \"\\n\"\n",
    "    elif metric_key.endswith(\"_pred\"):\n",
    "        metric_key2 = metric_key.replace(\"_pred\", \"_true\")\n",
    "        clusterername = \"G.T.\"\n",
    "        latex_table += (\n",
    "            r\"\\parbox[t]{2mm}{\\multirow{\"\n",
    "            + str(len(MODEL_GROUPS[backbone]))\n",
    "            + r\"}{*}{\"\n",
    "            + r\"\\scalebox{0.9}{\"\n",
    "            + r\"\\rotatebox[origin=c]{90}{\"\n",
    "            + clusterername\n",
    "            + r\"}}}\"\n",
    "            + r\"}\"\n",
    "        )\n",
    "        latex_table += \"\\n\"\n",
    "        for i_group, model in enumerate(list(MODEL_GROUPS[backbone])):\n",
    "            latex_table += f\"& {MODEL2SH[model]:<10s}\"\n",
    "            for i_attr, attr in enumerate(TEST_ATTRS):\n",
    "                latex_table += \" &\"\n",
    "                filter1 = {\"model\": model, \"dataset\": dataset}\n",
    "                if model == \"timm_vit_base_patch16_224.mae\":\n",
    "                    filter1[\"dim_reducer\"] = \"PCA\"\n",
    "                    filter1[\"pca_variance\"] = 0.95\n",
    "                else:\n",
    "                    filter1[\"dim_reducer_man\"] = \"UMAP\"\n",
    "                    filter1[\"ndim_reduced_man\"] = 50\n",
    "                    filter1[\"dim_reducer_man_metric\"] = \"euclidean\"\n",
    "                sdf = select_rows(test_runs_df, filter1, allow_missing=False)\n",
    "                sdf = sdf[~pd.isna(sdf[metric_key2])]\n",
    "                my_val = np.nanmedian(sdf[metric_key])\n",
    "                if sum(sdf[metric_key2] != my_val) > 0:\n",
    "                    pass\n",
    "                if dummy:\n",
    "                    best_results_grouped[attr][clusterername].append(my_val)\n",
    "                    continue\n",
    "                is_best_grp = my_val + eps >= np.max(best_results_grouped[attr][clusterername])\n",
    "                latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "                latex_table += \"     \"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best_grp:\n",
    "                    latex_table += r\"\\tcg{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                latex_table += show_fmt.format(my_val)\n",
    "                if highlight_best:\n",
    "                    latex_table += r\"}\" if is_best_grp else \" \"\n",
    "                latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "\n",
    "            latex_table += r\" \\\\\" + \"\\n\"\n",
    "        latex_table += r\"\\midrule\" + \"\\n\"\n",
    "\n",
    "    first_agg = True\n",
    "    for i_clusterer, clusterer in enumerate(CLUSTERERS):\n",
    "        clusterername = CLUSTERER2SH.get(clusterer, clusterer)\n",
    "        my_override_fields = override_fields.copy()\n",
    "        if first_agg and clusterer == \"AgglomerativeClustering\" and metric_key != \"num_cluster_pred\":\n",
    "            first_agg = False\n",
    "            my_override_fields[\"aggclust_dist_thresh\"] = None\n",
    "            clusterername = \"AC  w/ C\"\n",
    "        elif clusterer == \"AgglomerativeClustering\":\n",
    "            clusterername = \"AC w/o C\"\n",
    "            if \"aggclust_dist_thresh\" in my_override_fields:\n",
    "                del my_override_fields[\"aggclust_dist_thresh\"]\n",
    "\n",
    "        if i_clusterer > 0:\n",
    "            latex_table += r\"\\midrule\" + \"\\n\"\n",
    "\n",
    "        latex_table += (\n",
    "            r\"\\parbox[t]{2mm}{\\multirow{\"\n",
    "            + str(len(MODEL_GROUPS[backbone]))\n",
    "            + r\"}{*}{\"\n",
    "            + r\"\\scalebox{0.9}{\"\n",
    "            + r\"\\rotatebox[origin=c]{90}{\"\n",
    "            + clusterername\n",
    "            + r\"}}}\"\n",
    "            + r\"}\"\n",
    "        )\n",
    "        latex_table += \"\\n\"\n",
    "\n",
    "        for i_group, model in enumerate(list(MODEL_GROUPS[backbone])):\n",
    "            latex_table += f\"& {MODEL2SH[model]:<10s}\"\n",
    "            filter1 = {\"model\": model, \"dataset\": dataset}\n",
    "            filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "            filter2.update(filter1)\n",
    "            filter2.update(my_override_fields)\n",
    "            filter2 = fixup_filter(filter2)\n",
    "            sdf = select_rows(test_runs_df, filter2, allow_missing=False)\n",
    "            if len(sdf) < 1:\n",
    "                print(f\"No data for {model}-{dataset}-{clusterer}\\n{filter2}\")\n",
    "                cmds.append(filter2command(filter2, partition=\"test\"))\n",
    "                if not dummy:\n",
    "                    # latex_table += r\"\\multicolumn{1}{c}{--}\"\n",
    "                    latex_table += r\"   --  \"\n",
    "                continue\n",
    "            if len(sdf) > 1:\n",
    "                if sum(np.abs(sdf[metric_key] - sdf.iloc[0][metric_key]) > 1e-6) > 0:\n",
    "                    print()\n",
    "                    print(\n",
    "                        f\"More than one result with {metric_key} values\",\n",
    "                        list(sdf[metric_key]),\n",
    "                    )\n",
    "                    print(f\"for search {filter2}\")\n",
    "                    dif_cols = find_differing_columns(sdf, config_keys)\n",
    "                    print(f\"columns which differ: {dif_cols}\")\n",
    "                    if dif_cols:\n",
    "                        for col in dif_cols:\n",
    "                            print(f\"  {col}: {list(sdf[col])}\")\n",
    "            y_pred = np.load(\"../\" + get_pred_path(sdf.iloc[0]))[\"y_pred\"]\n",
    "            for i_attr, attr in enumerate(TEST_ATTRS):\n",
    "                latex_table += \" &\"\n",
    "                if metric_key.lower() != \"ami\":\n",
    "                    raise NotImplementedError()\n",
    "                my_val = sklearn.metrics.adjusted_mutual_info_score(attrs[:, i_attr], y_pred)\n",
    "                if dummy:\n",
    "                    best_results[attr].append(my_val)\n",
    "                    best_results_grouped[attr][clusterername].append(my_val)\n",
    "                    continue\n",
    "                if np.isnan(my_val):\n",
    "                    latex_table += r\"   --  \"\n",
    "                    continue\n",
    "                is_best = my_val + eps >= np.max(best_results[attr])\n",
    "                if len(best_results[attr]) > 1:\n",
    "                    is_secd = my_val + eps >= np.sort(best_results[attr])[-2]\n",
    "                else:\n",
    "                    is_secd = False\n",
    "                is_best_grp = my_val + eps >= np.max(best_results_grouped[attr][clusterername])\n",
    "                sc_base = np.nanmedian(best_results[attr])\n",
    "                sc_top = np.max(best_results[attr])\n",
    "                sc = 100 * max(0, (my_val - sc_base) / (sc_top - sc_base))\n",
    "                latex_table += r\"\\cellcolor{cbg!\" + f\"{sc:.0f}\" + \"}\"\n",
    "                if show_pc:\n",
    "                    my_val = my_val * 100\n",
    "                latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best:\n",
    "                    latex_table += r\"\\tcf{\"\n",
    "                elif is_secd:\n",
    "                    latex_table += r\"\\tcs{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best_grp:\n",
    "                    latex_table += r\"\\tcg{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                latex_table += show_fmt.format(my_val)\n",
    "                if highlight_best:\n",
    "                    latex_table += r\"}\" if is_best or is_secd else \" \"\n",
    "                    latex_table += r\"}\" if is_best_grp else \" \"\n",
    "                latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "            latex_table += r\" \\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\bottomrule\" + \"\\n\"\n",
    "    latex_table += r\"\\end{tabular}\" + \"\\n\"\n",
    "    latex_table += r\"%}\" + \"\\n\"  # Disabled\n",
    "\n",
    "print()\n",
    "print(f\"There are {len(cmds)} commands to execute to generate missing datapoints\")\n",
    "if show_commands:\n",
    "    for cmd in cmds:\n",
    "        print(cmd)\n",
    "\n",
    "print()\n",
    "print(\"Done!\")\n",
    "print()\n",
    "print(f\"Here is your {dataset} results table for {metric_key}, {backbone}:\")\n",
    "print()\n",
    "print()\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_attr_ami_table(models, clusterers, dataset, attrs, return_cmds=False, verbosity=0):\n",
    "    result_table = np.nan * np.ones((len(models), len(clusterers), attrs.shape[1]))\n",
    "    cmds = []\n",
    "\n",
    "    for i_model, model in enumerate(models):\n",
    "        print(f\"[{i_model + 1}/{len(models)}] {model}\")\n",
    "        for i_clusterer, clusterer in enumerate(clusterers):\n",
    "            filter1 = {\"model\": model, \"dataset\": dataset}\n",
    "            filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "            filter2.update(filter1)\n",
    "            filter2.update(override_fields)\n",
    "            filter2 = fixup_filter(filter2)\n",
    "            sdf = select_rows(test_runs_df, filter2, allow_missing=False)\n",
    "            if len(sdf) < 1:\n",
    "                if verbosity >= 1:\n",
    "                    print(f\"No data for {model}-{dataset}-{clusterer}\\n{filter2}\")\n",
    "                cmds.append(filter2command(filter2, partition=\"test\"))\n",
    "                continue\n",
    "            y_pred = np.load(\"../\" + get_pred_path(sdf.iloc[0]))[\"y_pred\"]\n",
    "\n",
    "            for i_attr in range(attrs.shape[1]):\n",
    "                my_val = sklearn.metrics.adjusted_mutual_info_score(attrs[:, i_attr], y_pred)\n",
    "                result_table[i_model, i_clusterer, i_attr] = my_val\n",
    "\n",
    "    if return_cmds:\n",
    "        return result_table, cmds\n",
    "    else:\n",
    "        return result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"imagenet-r\"\n",
    "# clusterers = ['KMeans', 'AC w/ C', 'AC w/o C', 'AffinityPropagation', 'HDBSCAN']\n",
    "metric_key = \"AMI\"  # AMI  num_cluster_pred  silhouette-euclidean_pred  silhouette-og-euclidean_pred\n",
    "use_rank = False\n",
    "show_pc = True\n",
    "show_fmt = \"{:4.0f}\"\n",
    "show_commands = False\n",
    "highlight_best = True\n",
    "use_si_num = False\n",
    "eps = 0.005\n",
    "\n",
    "if metric_key == \"num_cluster_pred\":\n",
    "    clusterers = [\"AC w/o C\", \"AffinityPropagation\", \"HDBSCAN\"]\n",
    "    show_pc = False\n",
    "    show_fmt = \"{:4.0f}\"\n",
    "    highlight_best = False\n",
    "    use_si_num = True\n",
    "else:\n",
    "    clusterers = [\n",
    "        \"KMeans\",\n",
    "        \"SpectralClustering\",\n",
    "        \"AC w/ C\",\n",
    "        \"AC w/o C\",\n",
    "        \"AffinityPropagation\",\n",
    "        \"HDBSCAN\",\n",
    "    ]\n",
    "if metric_key.startswith(\"silhouette\"):\n",
    "    show_pc = False\n",
    "    show_fmt = \"{:5.2f}\"\n",
    "\n",
    "\n",
    "model_groups = {\n",
    "    # \"---\": [\"none\"],\n",
    "    \"RN50\": RESNET50_MODELS[1:] + FT_RESNET50_MODELS,\n",
    "    \"ViT-B\": VITB16_MODELS[1:] + FT_VITB16_MODELS,\n",
    "}\n",
    "\n",
    "\n",
    "if len(clusterers) == 1:\n",
    "    clustererstr = clusterers[0]\n",
    "else:\n",
    "    clustererstr = f\"{len(clusterers)}c-avg\"\n",
    "\n",
    "model_groups_flattened = make_flat_hierarchy_from_dict(model_groups, pad_right=False)\n",
    "model_groups_flattened = np.array(model_groups_flattened)\n",
    "model_groups_flattened = model_groups_flattened[:, -1]\n",
    "\n",
    "\n",
    "print(\"Encoders:\")\n",
    "print(model_groups_flattened)\n",
    "\n",
    "result_table = build_attr_ami_table(\n",
    "    model_groups_flattened,\n",
    "    clusterers,\n",
    "    dataset,\n",
    "    attrs=attrs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table_actual = result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_ATTRS = [\"Class\", \"Artform\", \"Both\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "MyWQRN9iqBkI",
    "outputId": "2642efe3-b0ef-46f3-dc70-f2f5c69645d4"
   },
   "outputs": [],
   "source": [
    "print(\"Encoders:\")\n",
    "print(model_groups_flattened)\n",
    "\n",
    "print(\"Datasets:\")\n",
    "print(test_datasets)\n",
    "\n",
    "result_table = copy.deepcopy(result_table_actual)\n",
    "\n",
    "# Shaped [models, clusterers, datasets]\n",
    "print(\"result_table.shape\", result_table.shape)\n",
    "\n",
    "# Remove clusterer-dataset combos which are NaN for any model\n",
    "result_table[:, np.any(np.isnan(result_table), axis=0)] = np.nan\n",
    "\n",
    "# Take mean over clusterers\n",
    "result_table = np.nanmean(result_table, axis=1)\n",
    "# Shaped [models, datasets]\n",
    "\n",
    "print(\"result_table.shape\", result_table.shape)\n",
    "\n",
    "# Scale up to be a percentage\n",
    "# result_table *= 100.0\n",
    "\n",
    "\n",
    "if use_rank:\n",
    "    result_table_r = np.argsort(result_table, -1)[::-1, :] + 1\n",
    "else:\n",
    "    result_table_r = result_table\n",
    "\n",
    "# Take mean and stdev over samples\n",
    "mu = np.mean(result_table_r, axis=-1)\n",
    "sd = np.std(result_table_r, axis=-1)\n",
    "print(\"mu.shape\", mu.shape)\n",
    "\n",
    "\n",
    "print(model_groups)\n",
    "\n",
    "\n",
    "print(TEST_ATTRS)\n",
    "best_results = {k: [] for k in TEST_ATTRS}\n",
    "best_results_grouped = {k: defaultdict(list) for k in TEST_ATTRS}\n",
    "\n",
    "for dummy in [True, False]:\n",
    "    cmds = []\n",
    "    latex_table = r\"% Results for \" + f\"{metric_key}, {clustererstr}\" + \"\\n\"\n",
    "    now_str = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    latex_table += r\"% Generated \" + now_str + \"\\n\"\n",
    "    latex_table += r\"% Using hparams \" + BEST_PARAMS[\"_version\"] + \"\\n\"\n",
    "    label = clustererstr\n",
    "    if metric_key == \"AMI\":\n",
    "        latex_table += r\"\\label{tab:\" + label + r\"}\" + \"\\n\"\n",
    "    label = metric_key.replace(\"_\", \"-\") + \":\" + label\n",
    "    latex_table += r\"\\label{tab:\" + label + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"%\\resizebox{\\columnwidth}{!}{%\" + \"\\n\"  # Disabled\n",
    "    latex_table += r\"\\begin{tabular}{ll\" + r\"r\" * len(TEST_ATTRS) + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    latex_table += r\"& \" + f\"{'Encoder':<11s}\"\n",
    "    for attr in TEST_ATTRS:\n",
    "        latex_table += r\"&\" + f\"{attr.replace('_', ' '):^15s}\"\n",
    "    latex_table += r\"\\\\\" + \"\\n\"\n",
    "    # Begin table contents\n",
    "    latex_table += r\"\\midrule\" + \"\\n\"\n",
    "    i_model_o = -1\n",
    "    for i_group, group in enumerate(model_groups):\n",
    "        if i_group > 0:\n",
    "            latex_table += r\"\\midrule\" + \"\\n\"\n",
    "        latex_table += group + \"\\n\"\n",
    "        for i_model, model in enumerate(list(model_groups[group])):\n",
    "            i_model_o += 1\n",
    "            model_sh = MODEL2SH.get(model, model)\n",
    "            if model_sh.endswith(\" [FT]\"):\n",
    "                model_sh = f\"{model_sh[:-4]:<10s}\" + r\" & \\checkmark\"\n",
    "            else:\n",
    "                model_sh = f\"{model_sh:<10s}\" + \" &\"\n",
    "            latex_table += f\"& {model_sh:<23s}\"\n",
    "            # for i_dataset, dataset in enumerate(test_datasets):\n",
    "            for i_attr, attrname in enumerate(TEST_ATTRS):\n",
    "                latex_table += \" &\"\n",
    "                my_val = result_table[i_model_o, i_attr]\n",
    "                if dummy:\n",
    "                    best_results[attrname].append(my_val)\n",
    "                    best_results_grouped[attrname][group].append(my_val)\n",
    "                    continue\n",
    "                if np.isnan(my_val):\n",
    "                    latex_table += r\"   --  \"\n",
    "                    continue\n",
    "                is_best = my_val + eps >= np.max(best_results[attrname])\n",
    "                if len(best_results[attrname]) > 1:\n",
    "                    is_secd = my_val + eps >= np.sort(best_results[attrname])[-2]\n",
    "                else:\n",
    "                    is_secd = False\n",
    "                is_best_grp = my_val + eps >= np.max(best_results_grouped[attrname][group])\n",
    "                is_best_grp &= len(best_results_grouped[attrname][group]) > 1\n",
    "                sc_base = np.nanmedian(best_results[attrname])\n",
    "                sc_top = np.max(best_results[attrname])\n",
    "                sc = 100 * max(0, (my_val - sc_base) / (sc_top - sc_base))\n",
    "                latex_table += r\"\\cellcolor{cbg!\" + f\"{sc:.0f}\" + \"}\"\n",
    "                if show_pc:\n",
    "                    my_val = my_val * 100\n",
    "                latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best:\n",
    "                    latex_table += r\"\\tcf{\"\n",
    "                elif is_secd:\n",
    "                    latex_table += r\"\\tcs{\"\n",
    "                else:\n",
    "                    # latex_table += \"     \"\n",
    "                    pass\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best_grp:\n",
    "                    latex_table += r\"\\tcg{\"\n",
    "                else:\n",
    "                    # latex_table += \"     \"\n",
    "                    pass\n",
    "                latex_table += show_fmt.format(my_val)\n",
    "                if highlight_best:\n",
    "                    latex_table += r\"}\" if is_best or is_secd else \" \"\n",
    "                    latex_table += r\"}\" if is_best_grp else \" \"\n",
    "                latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "            latex_table += r\" \\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\bottomrule\" + \"\\n\"\n",
    "    latex_table += r\"\\end{tabular}\" + \"\\n\"\n",
    "    latex_table += r\"%} % resizebox\" + \"\\n\"\n",
    "\n",
    "print()\n",
    "print(f\"There are {len(cmds)} commands to execute to generate missing datapoints\")\n",
    "if show_commands:\n",
    "    for cmd in cmds:\n",
    "        print(cmd)\n",
    "\n",
    "print()\n",
    "print(\"Done!\")\n",
    "print()\n",
    "print(f\"Here is your results table for {metric_key}, {clustererstr}:\")\n",
    "print()\n",
    "print()\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FGVC Aircraft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_levels = [\"manufacturer\", \"family\", \"variant\"]\n",
    "attrs = np.stack(\n",
    "    [\n",
    "        torchvision.datasets.FGVCAircraft(\n",
    "            os.path.expanduser(\"~/Datasets\"),\n",
    "            split=\"test\",\n",
    "            annotation_level=annotation_level,\n",
    "        )._labels\n",
    "        for annotation_level in annotation_levels\n",
    "    ],\n",
    "    axis=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_attr in range(len(annotation_levels)):\n",
    "    print(annotation_levels[i_attr], len(np.unique(attrs[:, i_attr])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "MyWQRN9iqBkI",
    "outputId": "2642efe3-b0ef-46f3-dc70-f2f5c69645d4"
   },
   "outputs": [],
   "source": [
    "metric_key = \"AMI\"  # AMI  num_cluster_pred  silhouette-euclidean_pred  silhouette-og-euclidean_pred\n",
    "show_pc = True\n",
    "show_fmt = \"{:4.0f}\"\n",
    "show_commands = False\n",
    "highlight_best = True\n",
    "use_si_num = False\n",
    "eps = 0.005\n",
    "override_fields = {\n",
    "    \"predictions_dir\": \"y_pred\",\n",
    "}\n",
    "\n",
    "backbone = \"ResNet-50\"  # \"ResNet-50\" or \"ViT-B\"\n",
    "\n",
    "if metric_key == \"num_cluster_pred\":\n",
    "    CLUSTERERS = [\"AC w/o C\", \"AffinityPropagation\", \"HDBSCAN\"]\n",
    "    show_pc = False\n",
    "    show_fmt = \"{:4.0f}\"\n",
    "    highlight_best = False\n",
    "    use_si_num = True\n",
    "else:\n",
    "    CLUSTERERS = [\n",
    "        \"KMeans\",\n",
    "        \"SpectralClustering\",\n",
    "        \"AC w/ C\",\n",
    "        \"AC w/o C\",\n",
    "        \"AffinityPropagation\",\n",
    "        \"HDBSCAN\",\n",
    "    ]\n",
    "if metric_key.startswith(\"silhouette\"):\n",
    "    show_pc = False\n",
    "    show_fmt = \"{:5.2f}\"\n",
    "\n",
    "print(MODEL_GROUPS)\n",
    "\n",
    "dataset = \"aircraft\"\n",
    "\n",
    "TEST_ATTRS = annotation_levels\n",
    "print(TEST_ATTRS)\n",
    "best_results = {k: [] for k in TEST_ATTRS}\n",
    "best_results_grouped = {k: defaultdict(list) for k in TEST_ATTRS}\n",
    "\n",
    "for dummy in [True, False]:\n",
    "    cmds = []\n",
    "    latex_table = r\"% Results for \" + f\"{dataset} breakdown, {metric_key}, {backbone}\" + \"\\n\"\n",
    "    now_str = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    latex_table += r\"% Generated \" + now_str + \"\\n\"\n",
    "    latex_table += r\"% Using hparams \" + BEST_PARAMS[\"_version\"] + \"\\n\"\n",
    "    label = backbone\n",
    "    if metric_key == \"AMI\":\n",
    "        latex_table += r\"\\label{tab:\" + label + r\"}\" + \"\\n\"\n",
    "    label = metric_key.replace(\"_\", \"-\") + \":\" + label\n",
    "    latex_table += r\"\\label{tab:\" + label + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"%\\resizebox{\\textwidth}{!}{%\" + \"\\n\"  # Disabled\n",
    "    latex_table += r\"\\begin{tabular}{ll\" + r\"r\" * len(TEST_ATTRS) + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    latex_table += r\"& \" + f\"{'Encoder':<11s}\"\n",
    "    for attr in TEST_ATTRS:\n",
    "        latex_table += r\"&\" + f\"{attr.replace('_', ' '):^15s}\"\n",
    "    latex_table += r\"\\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    print(MODEL_GROUPS[backbone])\n",
    "    if metric_key == \"num_cluster_pred\":\n",
    "        latex_table += r\"& Num targets\"\n",
    "        for i_attr, attr in enumerate(TEST_ATTRS):\n",
    "            sdf = select_rows(test_runs_df, {\"dataset\": dataset}, allow_missing=False)\n",
    "            sdf = sdf[~pd.isna(sdf[\"num_cluster_true\"])]\n",
    "            latex_table += r\"& \"\n",
    "            latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "            latex_table += f\"{sdf.iloc[0]['num_cluster_true'].item()}\"\n",
    "            latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "        latex_table += r\"\\\\\" + \"\\n\"\n",
    "        latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    elif metric_key.endswith(\"_pred\"):\n",
    "        metric_key2 = metric_key.replace(\"_pred\", \"_true\")\n",
    "        clusterername = \"G.T.\"\n",
    "        latex_table += (\n",
    "            r\"\\parbox[t]{2mm}{\\multirow{\"\n",
    "            + str(len(MODEL_GROUPS[backbone]))\n",
    "            + r\"}{*}{\"\n",
    "            + r\"\\scalebox{0.9}{\"\n",
    "            + r\"\\rotatebox[origin=c]{90}{\"\n",
    "            + clusterername\n",
    "            + r\"}}}\"\n",
    "            + r\"}\"\n",
    "        )\n",
    "        latex_table += \"\\n\"\n",
    "        for i_group, model in enumerate(list(MODEL_GROUPS[backbone])):\n",
    "            latex_table += f\"& {MODEL2SH[model]:<10s}\"\n",
    "            for i_attr, attr in enumerate(TEST_ATTRS):\n",
    "                latex_table += \" &\"\n",
    "                filter1 = {\"model\": model, \"dataset\": dataset}\n",
    "                if model == \"timm_vit_base_patch16_224.mae\":\n",
    "                    filter1[\"dim_reducer\"] = \"PCA\"\n",
    "                    filter1[\"pca_variance\"] = 0.95\n",
    "                else:\n",
    "                    filter1[\"dim_reducer_man\"] = \"UMAP\"\n",
    "                    filter1[\"ndim_reduced_man\"] = 50\n",
    "                    filter1[\"dim_reducer_man_metric\"] = \"euclidean\"\n",
    "                sdf = select_rows(test_runs_df, filter1, allow_missing=False)\n",
    "                sdf = sdf[~pd.isna(sdf[metric_key2])]\n",
    "                my_val = np.nanmedian(sdf[metric_key])\n",
    "                if sum(sdf[metric_key2] != my_val) > 0:\n",
    "                    pass\n",
    "                if dummy:\n",
    "                    best_results_grouped[attr][clusterername].append(my_val)\n",
    "                    continue\n",
    "                is_best_grp = my_val + eps >= np.max(best_results_grouped[attr][clusterername])\n",
    "                latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "                latex_table += \"     \"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best_grp:\n",
    "                    latex_table += r\"\\tcg{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                latex_table += show_fmt.format(my_val)\n",
    "                if highlight_best:\n",
    "                    latex_table += r\"}\" if is_best_grp else \" \"\n",
    "                latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "\n",
    "            latex_table += r\" \\\\\" + \"\\n\"\n",
    "        latex_table += r\"\\toprule\" + \"\\n\"\n",
    "\n",
    "    first_agg = True\n",
    "    for i_clusterer, clusterer in enumerate(CLUSTERERS):\n",
    "        clusterername = CLUSTERER2SH.get(clusterer, clusterer)\n",
    "        my_override_fields = override_fields.copy()\n",
    "        if first_agg and clusterer == \"AgglomerativeClustering\" and metric_key != \"num_cluster_pred\":\n",
    "            first_agg = False\n",
    "            my_override_fields[\"aggclust_dist_thresh\"] = None\n",
    "            clusterername = \"AC  w/ C\"\n",
    "        elif clusterer == \"AgglomerativeClustering\":\n",
    "            clusterername = \"AC w/o C\"\n",
    "            if \"aggclust_dist_thresh\" in my_override_fields:\n",
    "                del my_override_fields[\"aggclust_dist_thresh\"]\n",
    "\n",
    "        if i_clusterer > 0:\n",
    "            latex_table += r\"\\midrule\" + \"\\n\"\n",
    "\n",
    "        latex_table += (\n",
    "            r\"\\parbox[t]{2mm}{\\multirow{\"\n",
    "            + str(len(MODEL_GROUPS[backbone]))\n",
    "            + r\"}{*}{\"\n",
    "            + r\"\\scalebox{0.9}{\"\n",
    "            + r\"\\rotatebox[origin=c]{90}{\"\n",
    "            + clusterername\n",
    "            + r\"}}}\"\n",
    "            + r\"}\"\n",
    "        )\n",
    "        latex_table += \"\\n\"\n",
    "\n",
    "        for i_group, model in enumerate(list(MODEL_GROUPS[backbone])):\n",
    "            latex_table += f\"& {MODEL2SH[model]:<10s}\"\n",
    "            filter1 = {\"model\": model, \"dataset\": dataset}\n",
    "            filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "            filter2.update(filter1)\n",
    "            filter2.update(my_override_fields)\n",
    "            filter2 = fixup_filter(filter2)\n",
    "            sdf = select_rows(test_runs_df, filter2, allow_missing=False)\n",
    "            if len(sdf) < 1:\n",
    "                print(f\"No data for {model}-{dataset}-{clusterer}\\n{filter2}\")\n",
    "                cmds.append(filter2command(filter2, partition=\"test\"))\n",
    "                if not dummy:\n",
    "                    # latex_table += r\"\\multicolumn{1}{c}{--}\"\n",
    "                    latex_table += r\"   --  \"\n",
    "                continue\n",
    "            if len(sdf) > 1:\n",
    "                if sum(np.abs(sdf[metric_key] - sdf.iloc[0][metric_key]) > 1e-6) > 0:\n",
    "                    print()\n",
    "                    print(\n",
    "                        f\"More than one result with {metric_key} values\",\n",
    "                        list(sdf[metric_key]),\n",
    "                    )\n",
    "                    print(f\"for search {filter2}\")\n",
    "                    dif_cols = find_differing_columns(sdf, config_keys)\n",
    "                    print(f\"columns which differ: {dif_cols}\")\n",
    "                    if dif_cols:\n",
    "                        for col in dif_cols:\n",
    "                            print(f\"  {col}: {list(sdf[col])}\")\n",
    "            y_pred = np.load(\"../\" + get_pred_path(sdf.iloc[0]))[\"y_pred\"]\n",
    "            for i_attr, attr in enumerate(TEST_ATTRS):\n",
    "                latex_table += \" &\"\n",
    "                if metric_key.lower() != \"ami\":\n",
    "                    raise NotImplementedError()\n",
    "                my_val = sklearn.metrics.adjusted_mutual_info_score(attrs[:, i_attr], y_pred)\n",
    "                if dummy:\n",
    "                    best_results[attr].append(my_val)\n",
    "                    best_results_grouped[attr][clusterername].append(my_val)\n",
    "                    continue\n",
    "                if np.isnan(my_val):\n",
    "                    latex_table += r\"   --  \"\n",
    "                    continue\n",
    "                is_best = my_val + eps >= np.max(best_results[attr])\n",
    "                if len(best_results[attr]) > 1:\n",
    "                    is_secd = my_val + eps >= np.sort(best_results[attr])[-2]\n",
    "                else:\n",
    "                    is_secd = False\n",
    "                is_best_grp = my_val + eps >= np.max(best_results_grouped[attr][clusterername])\n",
    "                sc_base = np.nanmedian(best_results[attr])\n",
    "                sc_top = np.max(best_results[attr])\n",
    "                sc = 100 * max(0, (my_val - sc_base) / (sc_top - sc_base))\n",
    "                latex_table += r\"\\cellcolor{cbg!\" + f\"{sc:.0f}\" + \"}\"\n",
    "                if show_pc:\n",
    "                    my_val = my_val * 100\n",
    "                latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best:\n",
    "                    latex_table += r\"\\tcf{\"\n",
    "                elif is_secd:\n",
    "                    latex_table += r\"\\tcs{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best_grp:\n",
    "                    latex_table += r\"\\tcg{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                latex_table += show_fmt.format(my_val)\n",
    "                if highlight_best:\n",
    "                    latex_table += r\"}\" if is_best or is_secd else \" \"\n",
    "                    latex_table += r\"}\" if is_best_grp else \" \"\n",
    "                latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "            latex_table += r\" \\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\bottomrule\" + \"\\n\"\n",
    "    latex_table += r\"\\end{tabular}\" + \"\\n\"\n",
    "    latex_table += r\"%}\" + \"\\n\"  # Disabled\n",
    "\n",
    "print()\n",
    "print(f\"There are {len(cmds)} commands to execute to generate missing datapoints\")\n",
    "if show_commands:\n",
    "    for cmd in cmds:\n",
    "        print(cmd)\n",
    "\n",
    "print()\n",
    "print(\"Done!\")\n",
    "print()\n",
    "print(f\"Here is your {dataset} results table for {metric_key}, {backbone}:\")\n",
    "print()\n",
    "print()\n",
    "print(latex_table)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
