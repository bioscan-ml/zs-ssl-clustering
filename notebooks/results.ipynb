{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnxDlvP8szOj"
   },
   "source": [
    "# Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-9BelQPaJVo2"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import datetime\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.colors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from IPython.display import display\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGS_DIR = \"figs\"\n",
    "os.makedirs(FIGS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9AH-GlrfS4Pf"
   },
   "outputs": [],
   "source": [
    "VALIDATION_DATASETS = [\"imagenet\", \"imagenette\", \"imagewoof\"]\n",
    "RESNET50_MODELS = [\n",
    "    \"random_resnet50\",\n",
    "    \"resnet50\",\n",
    "    \"mocov3_resnet50\",\n",
    "    \"dino_resnet50\",\n",
    "    \"vicreg_resnet50\",\n",
    "    \"clip_RN50\",\n",
    "]\n",
    "VITB16_MODELS = [\n",
    "    \"random_vitb16\",\n",
    "    \"vitb16\",\n",
    "    \"mocov3_vit_base\",\n",
    "    \"dino_vitb16\",\n",
    "    \"timm_vit_base_patch16_224.mae\",\n",
    "    \"mae_pretrain_vit_base_global\",\n",
    "    \"clip_vitb16\",\n",
    "]\n",
    "FT_RESNET50_MODELS = [\n",
    "    \"ft_mocov3_resnet50\",\n",
    "    \"ft_dino_resnet50\",\n",
    "    \"ft_vicreg_resnet50\",\n",
    "]\n",
    "FT_VITB16_MODELS = [\n",
    "    \"ft_mocov3_vit_base\",\n",
    "    \"ft_dino_vitb16\",\n",
    "    \"mae_finetuned_vit_base_global\",\n",
    "]\n",
    "FT_MODELS = FT_RESNET50_MODELS + FT_VITB16_MODELS\n",
    "ALL_MODELS = [\"none\"] + RESNET50_MODELS + VITB16_MODELS + FT_RESNET50_MODELS + FT_VITB16_MODELS\n",
    "CLUSTERERS = [\n",
    "    \"KMeans\",\n",
    "    \"AgglomerativeClustering\",\n",
    "    \"AffinityPropagation\",\n",
    "    \"SpectralClustering\",\n",
    "    \"HDBSCAN\",\n",
    "    \"OPTICS\",\n",
    "]\n",
    "ALL_CLUSTERERS = copy.deepcopy(CLUSTERERS)\n",
    "DISTANCE_METRICS = [\n",
    "    \"euclidean\",\n",
    "    \"l1\",\n",
    "    \"chebyshev\",\n",
    "    \"cosine\",\n",
    "    \"arccos\",\n",
    "    \"braycurtis\",\n",
    "    \"canberra\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0mJrqF3FAbEG"
   },
   "outputs": [],
   "source": [
    "DATASET2LS = {\n",
    "    \"imagenet\": \"-.\",\n",
    "    \"imagenette\": \"--\",\n",
    "    \"imagewoof\": \":\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OqPoCd4kbokG"
   },
   "outputs": [],
   "source": [
    "DEFAULT_PARAMS = {\n",
    "    \"all\": {\n",
    "        \"dim_reducer\": \"None\",\n",
    "        \"dim_reducer_man\": \"None\",\n",
    "        \"zscore\": False,\n",
    "        \"normalize\": False,\n",
    "        \"zscore2\": False,\n",
    "        \"ndim_correction\": False,\n",
    "    },\n",
    "    \"KMeans\": {\"clusterer\": \"KMeans\"},\n",
    "    \"AffinityPropagation\": {\n",
    "        \"clusterer\": \"AffinityPropagation\",\n",
    "        \"affinity_damping\": 0.9,\n",
    "        \"affinity_conv_iter\": 15,\n",
    "    },\n",
    "    \"SpectralClustering\": {\n",
    "        \"clusterer\": \"SpectralClustering\",\n",
    "        \"spectral_assigner\": \"cluster_qr\",\n",
    "    },\n",
    "    \"AgglomerativeClustering\": {\n",
    "        \"clusterer\": \"AgglomerativeClustering\",\n",
    "        \"distance_metric\": \"euclidean\",\n",
    "        \"aggclust_linkage\": \"ward\",\n",
    "    },\n",
    "    \"HDBSCAN\": {\n",
    "        \"clusterer\": \"HDBSCAN\",\n",
    "        \"hdbscan_method\": \"eom\",\n",
    "        \"min_samples\": 5,\n",
    "        \"max_samples\": 0.2,\n",
    "        \"distance_metric\": \"euclidean\",\n",
    "    },\n",
    "    \"OPTICS\": {\n",
    "        \"clusterer\": \"OPTICS\",\n",
    "        \"optics_method\": \"xi\",\n",
    "        \"optics_xi\": 0.05,\n",
    "        \"distance_metric\": \"euclidean\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VKTbU-Oasdl"
   },
   "source": [
    "## Set best params\n",
    "\n",
    "These were discovered by the search in hpsearch.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLuhE2xibLOC"
   },
   "source": [
    "### Num dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAgJs3H-uqiJ"
   },
   "outputs": [],
   "source": [
    "models = RESNET50_MODELS + VITB16_MODELS\n",
    "BEST_PARAMS = {\n",
    "    clusterer: {model: copy.deepcopy(DEFAULT_PARAMS[clusterer]) for model in models}\n",
    "    for clusterer in ALL_CLUSTERERS\n",
    "}\n",
    "\n",
    "# KMeans\n",
    "# Use UMAP (num dims unimportant; we select 50d for consistency) for every encoder except\n",
    "# - clip_RN50 : a little better to use PCA with 500d than UMAP. UMAP beats PCA if you\n",
    "#   reduce the PCA dims below 500.\n",
    "# - clip_vitb16 : same behaviour as clip_RN50\n",
    "# - timm_vit_base_patch16_224.mae : best is PCA 0.85 variance explained. Need at least\n",
    "#   200 PCA dims, and PCA perf beats UMAP throughout\n",
    "\n",
    "for model in RESNET50_MODELS + VITB16_MODELS:\n",
    "    if model.startswith(\"clip\") or model == \"timm_vit_base_patch16_224.mae\":\n",
    "        continue\n",
    "    BEST_PARAMS[\"KMeans\"][model].update(\n",
    "        {\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50}\n",
    "    )\n",
    "\n",
    "BEST_PARAMS[\"KMeans\"][\"clip_RN50\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"ndim_reduced\": 500, \"zscore\": True, \"pca_variance\": None}\n",
    ")\n",
    "BEST_PARAMS[\"KMeans\"][\"clip_vitb16\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"ndim_reduced\": 500, \"zscore\": True, \"pca_variance\": None}\n",
    ")\n",
    "BEST_PARAMS[\"KMeans\"][\"timm_vit_base_patch16_224.mae\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"pca_variance\": 0.85, \"zscore\": True, \"ndim_reduced\": None}\n",
    ")\n",
    "\n",
    "# AffinityPropagation\n",
    "# Use PCA with 10 dims for every encoder except\n",
    "# - resnet50 (supervised) : original embeddings, no reduction (AMI=0.62);\n",
    "#   perf gets worse if they are whitened (AMI=0.55) and although the perf increases\n",
    "#   as num dims are reduced it doesn't quite recover. PCA perf peaks at 10-20 dim (AMI=0.57).\n",
    "# - dino_resnet50 : does marginally better at UMAP 50 (AMI=0.52495) than PCA 10 (AMI=0.5044)\n",
    "# - timm_vit_base_patch16_224.mae : PCA 0.95 variance explained (AMI=0.303).\n",
    "#   Definite improvement from 10 to 20 dims, but not much improvement above that.\n",
    "\n",
    "for model in models:\n",
    "    if model in [\"resnet50\", \"dino_resnet50\", \"timm_vit_base_patch16_224.mae\"]:\n",
    "        continue\n",
    "    BEST_PARAMS[\"AffinityPropagation\"][model].update(\n",
    "        {\n",
    "            \"dim_reducer\": \"PCA\",\n",
    "            \"ndim_reduced\": 10,\n",
    "            \"zscore\": True,\n",
    "            \"pca_variance\": None,\n",
    "            \"dim_reducer_man\": \"None\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "BEST_PARAMS[\"AffinityPropagation\"][\"resnet50\"].update(\n",
    "    {\"dim_reducer\": \"None\", \"dim_reducer_man\": \"None\", \"zscore\": False}\n",
    ")\n",
    "BEST_PARAMS[\"AffinityPropagation\"][\"dino_resnet50\"].update(\n",
    "    {\n",
    "        \"dim_reducer\": \"PCA\",\n",
    "        \"pca_variance\": 0.95,\n",
    "        \"zscore\": True,\n",
    "        \"ndim_reduced\": None,\n",
    "        \"dim_reducer_man\": \"None\",\n",
    "    }\n",
    ")\n",
    "BEST_PARAMS[\"AffinityPropagation\"][\"timm_vit_base_patch16_224.mae\"].update(\n",
    "    {\n",
    "        \"dim_reducer\": \"PCA\",\n",
    "        \"pca_variance\": 0.95,\n",
    "        \"zscore\": True,\n",
    "        \"ndim_reduced\": None,\n",
    "        \"dim_reducer_man\": \"None\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# AgglomerativeClustering\n",
    "# Use UMAP (num dims unimportant; we select 50d for consistency) for every encoder except\n",
    "# - timm_vit_base_patch16_224.mae : PCA 0.98 variance explained (i.e. nearly all\n",
    "#   dimensions kept), which is not noticably better than using 500 dim PCA but there is\n",
    "#   an increase compared to using less than 500d.\n",
    "\n",
    "for model in models:\n",
    "    if model == \"timm_vit_base_patch16_224.mae\":\n",
    "        continue\n",
    "    BEST_PARAMS[\"AgglomerativeClustering\"][model].update(\n",
    "        {\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50, \"dim_reducer\": \"None\"}\n",
    "    )\n",
    "\n",
    "BEST_PARAMS[\"AgglomerativeClustering\"][\"timm_vit_base_patch16_224.mae\"].update(\n",
    "    {\n",
    "        \"dim_reducer\": \"PCA\",\n",
    "        \"pca_variance\": 0.98,\n",
    "        \"zscore\": True,\n",
    "        \"ndim_reduced\": None,\n",
    "        \"dim_reducer_man\": \"None\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# HDBSCAN\n",
    "# Use UMAP for every encoder except\n",
    "# - timm_vit_base_patch16_224.mae : PCA 0.95 variance explained (AMI=0.085) which is\n",
    "#   not noticably better than PCA with 50 dim\n",
    "\n",
    "for model in models:\n",
    "    if model in [\"timm_vit_base_patch16_224.mae\"]:\n",
    "        continue\n",
    "    BEST_PARAMS[\"HDBSCAN\"][model].update(\n",
    "        {\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50, \"dim_reducer\": \"None\"}\n",
    "    )\n",
    "\n",
    "BEST_PARAMS[\"HDBSCAN\"][\"timm_vit_base_patch16_224.mae\"].update(\n",
    "    {\n",
    "        \"dim_reducer\": \"PCA\",\n",
    "        \"pca_variance\": 0.95,\n",
    "        \"zscore\": True,\n",
    "        \"ndim_reduced\": None,\n",
    "        \"dim_reducer_man\": \"None\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# OPTICS\n",
    "# Use UMAP for every encoder, no exceptions necessary\n",
    "for model in models:\n",
    "    BEST_PARAMS[\"OPTICS\"][model].update(\n",
    "        {\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50, \"dim_reducer\": \"None\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_PARAMS_v1 = copy.deepcopy(BEST_PARAMS)\n",
    "BEST_PARAMS_v1[\"_version\"] = \"v1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2nA5YxFqFo7x",
    "outputId": "3b02ac29-9036-42a0-8c5d-a888d9794865"
   },
   "outputs": [],
   "source": [
    "BEST_PARAMS_v2 = copy.deepcopy(BEST_PARAMS)\n",
    "BEST_PARAMS_v2[\"_version\"] = \"v2.0\"\n",
    "\n",
    "print(\"Updating dim choices for new method\")\n",
    "# Updated dim choices\n",
    "# (changed to this when we swapped to using weighted average instead of straight\n",
    "# average between Imagenet-1k, Imagenette, Imagewoof)\n",
    "\n",
    "# Changed KMeans clip_RN50 from PCA 500 to UMAP 50, so it uses fewer dimensions\n",
    "# (probably more stable than using 500-d which is what PCA needs to marginally beat UMAP)\n",
    "BEST_PARAMS_v2[\"KMeans\"][\"clip_RN50\"].update(\n",
    "    {\"dim_reducer\": None, \"ndim_reduced\": None, \"zscore\": False, \"pca_variance\": None}\n",
    ")\n",
    "BEST_PARAMS_v2[\"KMeans\"][\"clip_RN50\"].update(\n",
    "    {\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50}\n",
    ")\n",
    "# Changed KMeans MAE from PCA 85% to PCA 200\n",
    "# (since we see perf above plateaus at 200-d, there is no point going above that)\n",
    "BEST_PARAMS_v2[\"KMeans\"][\"timm_vit_base_patch16_224.mae\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"zscore\": True, \"ndim_reduced\": 200, \"pca_variance\": None}\n",
    ")\n",
    "# Changed KMeans clip_vitb16 from PCA 500 to PCA 75%\n",
    "# (gives a notably better train set AMI measurement above)\n",
    "BEST_PARAMS_v2[\"KMeans\"][\"clip_vitb16\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"zscore\": True, \"pca_variance\": 0.75, \"ndim_reduced\": None}\n",
    ")\n",
    "\n",
    "# Changed AffinityPropagation dino_resnet50 from PCA 95% to PCA 10\n",
    "# (performance is basically equal, so no point using higher-dim space;\n",
    "# could have done UMAP 50 instead with basically equal train AMI to PCA 10,\n",
    "# but didn't for consistency with other models)\n",
    "BEST_PARAMS_v2[\"AffinityPropagation\"][\"dino_resnet50\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"zscore\": True, \"ndim_reduced\": 10, \"pca_variance\": None}\n",
    ")\n",
    "# Changed AffinityPropagation MAE from PCA 95% to PCA 100\n",
    "BEST_PARAMS_v2[\"AffinityPropagation\"][\"timm_vit_base_patch16_224.mae\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"zscore\": True, \"ndim_reduced\": 100, \"pca_variance\": None}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Updating dim choices to use Affinity Prop dim results found with 0.9 damping,\"\n",
    "    \" prefering PCA reduction by percentage variance explained\"\n",
    ")\n",
    "BEST_PARAMS_v3 = {\n",
    "    clusterer: {model: copy.deepcopy(DEFAULT_PARAMS[clusterer]) for model in ALL_MODELS}\n",
    "    for clusterer in ALL_CLUSTERERS\n",
    "}\n",
    "BEST_PARAMS_v3[\"_version\"] = \"v3.0\"\n",
    "\n",
    "# KMeans\n",
    "for model in RESNET50_MODELS + VITB16_MODELS + FT_MODELS:\n",
    "    if (\n",
    "        model == \"none\"\n",
    "        or model.startswith(\"random\")\n",
    "        or model.startswith(\"clip\")\n",
    "        or model == \"timm_vit_base_patch16_224.mae\"\n",
    "    ):\n",
    "        continue\n",
    "    BEST_PARAMS_v3[\"KMeans\"][model].update(\n",
    "        {\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50}\n",
    "    )\n",
    "\n",
    "BEST_PARAMS_v3[\"KMeans\"][\"none\"].update(\n",
    "    {\"image_size\": 32, \"dim_reducer\": \"PCA\", \"pca_variance\": 0.98, \"zscore\": True}\n",
    ")\n",
    "BEST_PARAMS_v3[\"KMeans\"][\"random_resnet50\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"pca_variance\": 0.95, \"zscore\": True}\n",
    ")\n",
    "BEST_PARAMS_v3[\"KMeans\"][\"random_vitb16\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"ndim_reduced\": 100, \"zscore\": True}\n",
    ")\n",
    "\n",
    "BEST_PARAMS_v3[\"KMeans\"][\"clip_RN50\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"pca_variance\": 0.85, \"zscore\": True}\n",
    ")\n",
    "BEST_PARAMS_v3[\"KMeans\"][\"clip_vitb16\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"pca_variance\": 0.75, \"zscore\": True}\n",
    ")\n",
    "BEST_PARAMS_v3[\"KMeans\"][\"timm_vit_base_patch16_224.mae\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"pca_variance\": 0.85, \"zscore\": True}\n",
    ")\n",
    "\n",
    "# AffinityPropagation\n",
    "for model in ALL_MODELS:\n",
    "    BEST_PARAMS_v3[\"AffinityPropagation\"][model].update({\"affinity_damping\": 0.9})\n",
    "\n",
    "for model in (\n",
    "    [\n",
    "        \"resnet50\",\n",
    "        \"clip_RN50\",\n",
    "        \"vitb16\",\n",
    "        \"mocov3_vit_base\",\n",
    "        \"mae_pretrain_vit_base_global\",\n",
    "        \"dino_vitb16\",\n",
    "        \"clip_vitb16\",\n",
    "    ] + FT_MODELS\n",
    "):\n",
    "    BEST_PARAMS_v3[\"AffinityPropagation\"][model].update(\n",
    "        {\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50}\n",
    "    )\n",
    "for model in [\"mocov3_resnet50\", \"vicreg_resnet50\", \"dino_resnet50\"]:\n",
    "    BEST_PARAMS_v3[\"AffinityPropagation\"][model].update(\n",
    "        {\"dim_reducer_man\": \"PaCMAP\", \"ndim_reduced_man\": 50, \"dim_reducer_man_nn\": None}\n",
    "    )\n",
    "\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"none\"].update(\n",
    "    {\"image_size\": 32, \"dim_reducer\": \"PCA\", \"pca_variance\": 0.8, \"zscore\": True}\n",
    ")\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"random_resnet50\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"pca_variance\": 0.99, \"zscore\": True}\n",
    ")\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"random_vitb16\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"pca_variance\": 0.98, \"zscore\": True}\n",
    ")\n",
    "\n",
    "BEST_PARAMS_v3[\"KMeans\"][\"timm_vit_base_patch16_224.mae\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"pca_variance\": 0.99, \"zscore\": True}\n",
    ")\n",
    "\n",
    "# AgglomerativeClustering\n",
    "for model in ALL_MODELS:\n",
    "    if (\n",
    "        model == \"none\"\n",
    "        or model.startswith(\"random\")\n",
    "        or model == \"timm_vit_base_patch16_224.mae\"\n",
    "    ):\n",
    "        continue\n",
    "    BEST_PARAMS_v3[\"AgglomerativeClustering\"][model].update(\n",
    "        {\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50, \"dim_reducer\": \"None\"}\n",
    "    )\n",
    "\n",
    "BEST_PARAMS_v3[\"AgglomerativeClustering\"][\"none\"].update(\n",
    "    {\"image_size\": 32, \"dim_reducer\": \"PCA\", \"pca_variance\": 0.75, \"zscore\": True}\n",
    ")\n",
    "BEST_PARAMS_v3[\"AgglomerativeClustering\"][\"random_resnet50\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"pca_variance\": 0.98, \"zscore\": True}\n",
    ")\n",
    "BEST_PARAMS_v3[\"AgglomerativeClustering\"][\"random_vitb16\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"pca_variance\": 0.85, \"zscore\": True}\n",
    ")\n",
    "BEST_PARAMS_v3[\"AgglomerativeClustering\"][\"timm_vit_base_patch16_224.mae\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"pca_variance\": 0.98, \"zscore\": True}\n",
    ")\n",
    "\n",
    "# HDBSCAN\n",
    "for model in ALL_MODELS:\n",
    "    if model in [\"timm_vit_base_patch16_224.mae\"]:\n",
    "        continue\n",
    "    BEST_PARAMS_v3[\"HDBSCAN\"][model].update(\n",
    "        {\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50, \"dim_reducer\": \"None\"}\n",
    "    )\n",
    "\n",
    "BEST_PARAMS_v3[\"HDBSCAN\"][\"none\"].update(\n",
    "    {\"image_size\": 32}\n",
    ")\n",
    "BEST_PARAMS_v3[\"HDBSCAN\"][\"timm_vit_base_patch16_224.mae\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"pca_variance\": 0.95, \"zscore\": True}\n",
    ")\n",
    "\n",
    "# OPTICS - TODO\n",
    "# Use UMAP for every encoder, no exceptions necessary (not checked raw or random)\n",
    "for model in ALL_MODELS:\n",
    "    BEST_PARAMS_v3[\"OPTICS\"][model].update(\n",
    "        {\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50, \"dim_reducer\": \"None\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Updating dim choices to use Affinity Prop dim results found with 0.9 damping,\"\n",
    "    \" stop PCA at 95%\"\n",
    ")\n",
    "BEST_PARAMS_v4 = {\n",
    "    clusterer: {model: copy.deepcopy(DEFAULT_PARAMS[clusterer]) for model in ALL_MODELS}\n",
    "    for clusterer in ALL_CLUSTERERS\n",
    "}\n",
    "BEST_PARAMS_v4[\"_version\"] = \"v4.0\"\n",
    "for clusterer in BEST_PARAMS_v4:\n",
    "    if clusterer.startswith(\"_\"):\n",
    "        continue\n",
    "    BEST_PARAMS_v4[clusterer][\"none\"].update({\"image_size\": 32})\n",
    "\n",
    "# KMeans\n",
    "for model in RESNET50_MODELS + VITB16_MODELS + FT_MODELS:\n",
    "    if (\n",
    "        model == \"none\"\n",
    "        or model.startswith(\"random\")\n",
    "        or model.startswith(\"clip\")\n",
    "        or model == \"timm_vit_base_patch16_224.mae\"\n",
    "        or model == \"mae_pretrain_vit_base_global\"\n",
    "    ):\n",
    "        continue\n",
    "    BEST_PARAMS_v4[\"KMeans\"][model].update(\n",
    "        {\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50}\n",
    "    )\n",
    "\n",
    "BEST_PARAMS_v4[\"KMeans\"][\"none\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"pca_variance\": 0.90, \"zscore\": True}\n",
    ")\n",
    "BEST_PARAMS_v4[\"KMeans\"][\"random_resnet50\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"pca_variance\": 0.95, \"zscore\": True}\n",
    ")\n",
    "BEST_PARAMS_v4[\"KMeans\"][\"random_vitb16\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"ndim_reduced\": 100, \"zscore\": True}\n",
    ")\n",
    "\n",
    "BEST_PARAMS_v4[\"KMeans\"][\"clip_RN50\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"pca_variance\": 0.85, \"zscore\": True}\n",
    ")\n",
    "BEST_PARAMS_v4[\"KMeans\"][\"clip_vitb16\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"pca_variance\": 0.75, \"zscore\": True}\n",
    ")\n",
    "BEST_PARAMS_v4[\"KMeans\"][\"timm_vit_base_patch16_224.mae\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"pca_variance\": 0.95, \"zscore\": True}\n",
    ")\n",
    "BEST_PARAMS_v4[\"KMeans\"][\"mae_pretrain_vit_base_global\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"pca_variance\": 0.9, \"zscore\": True}\n",
    ")\n",
    "\n",
    "# AffinityPropagation\n",
    "for model in ALL_MODELS:\n",
    "    BEST_PARAMS_v4[\"AffinityPropagation\"][model].update({\"affinity_damping\": 0.9})\n",
    "\n",
    "for model in (\n",
    "    [\n",
    "        \"resnet50\",\n",
    "        \"clip_RN50\",\n",
    "        \"vitb16\",\n",
    "        \"mocov3_vit_base\",\n",
    "        \"mae_pretrain_vit_base_global\",\n",
    "        \"dino_vitb16\",\n",
    "        \"clip_vitb16\",\n",
    "    ] + FT_MODELS\n",
    "):\n",
    "    BEST_PARAMS_v4[\"AffinityPropagation\"][model].update(\n",
    "        {\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50}\n",
    "    )\n",
    "for model in [\"mocov3_resnet50\", \"vicreg_resnet50\", \"dino_resnet50\"]:\n",
    "    # tbc\n",
    "    BEST_PARAMS_v4[\"AffinityPropagation\"][model].update(\n",
    "        {\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50, \"dim_reducer_man_nn\": None}\n",
    "    )\n",
    "\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"none\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"pca_variance\": 0.8, \"zscore\": True}\n",
    ")\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"random_resnet50\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"pca_variance\": 0.9, \"zscore\": True}\n",
    ")\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"random_vitb16\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"pca_variance\": 0.9, \"zscore\": True}\n",
    ")\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"timm_vit_base_patch16_224.mae\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"ndim_reduced\": 200, \"zscore\": True}\n",
    ")\n",
    "\n",
    "# AgglomerativeClustering\n",
    "for model in ALL_MODELS:\n",
    "    if (\n",
    "        model == \"none\"\n",
    "        or model.startswith(\"random\")\n",
    "        or model == \"timm_vit_base_patch16_224.mae\"\n",
    "        or model == \"mae_pretrain_vit_base_global\"\n",
    "    ):\n",
    "        continue\n",
    "    BEST_PARAMS_v4[\"AgglomerativeClustering\"][model].update(\n",
    "        {\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50, \"dim_reducer\": \"None\"}\n",
    "    )\n",
    "\n",
    "BEST_PARAMS_v4[\"AgglomerativeClustering\"][\"none\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"ndim_reduced\": 200, \"zscore\": True}\n",
    ")\n",
    "BEST_PARAMS_v4[\"AgglomerativeClustering\"][\"random_resnet50\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"ndim_reduced\": 200, \"zscore\": True}\n",
    ")\n",
    "BEST_PARAMS_v4[\"AgglomerativeClustering\"][\"random_vitb16\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"pca_variance\": 0.85, \"zscore\": True}\n",
    ")\n",
    "BEST_PARAMS_v4[\"AgglomerativeClustering\"][\"timm_vit_base_patch16_224.mae\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"pca_variance\": 0.90, \"zscore\": True}\n",
    ")\n",
    "BEST_PARAMS_v4[\"AgglomerativeClustering\"][\"mae_pretrain_vit_base_global\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"pca_variance\": 0.85, \"zscore\": True}\n",
    ")\n",
    "\n",
    "# HDBSCAN\n",
    "for model in ALL_MODELS:\n",
    "    if model in [\"timm_vit_base_patch16_224.mae\"]:\n",
    "        continue\n",
    "    BEST_PARAMS_v4[\"HDBSCAN\"][model].update(\n",
    "        {\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50, \"dim_reducer\": \"None\"}\n",
    "    )\n",
    "\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"timm_vit_base_patch16_224.mae\"].update(\n",
    "    {\"dim_reducer\": \"PCA\", \"pca_variance\": 0.95, \"zscore\": True}\n",
    ")\n",
    "\n",
    "# OPTICS - TODO\n",
    "# Use UMAP for every encoder, no exceptions necessary (not checked raw or random)\n",
    "for model in ALL_MODELS:\n",
    "    BEST_PARAMS_v4[\"OPTICS\"][model].update(\n",
    "        {\"dim_reducer_man\": \"UMAP\", \"ndim_reduced_man\": 50, \"dim_reducer\": \"None\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9scRlYvoa9tr"
   },
   "source": [
    "### Agglomerative specific settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cv8_GiZ5PfDF"
   },
   "outputs": [],
   "source": [
    "for model in [\n",
    "    \"resnet50\",\n",
    "    \"mocov3_resnet50\",\n",
    "    \"vicreg_resnet50\",\n",
    "    \"vitb16\",\n",
    "    \"timm_vit_base_patch16_224.mae\",\n",
    "]:\n",
    "    BEST_PARAMS_v1[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"euclidean\",\n",
    "            \"aggclust_linkage\": \"ward\",\n",
    "        }\n",
    "    )\n",
    "for model in [\"dino_resnet50\", \"clip_RN50\", \"dino_vitb16\"]:\n",
    "    BEST_PARAMS_v1[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"euclidean\",\n",
    "            \"aggclust_linkage\": \"average\",\n",
    "        }\n",
    "    )\n",
    "for model in [\"mocov3_vit_base\", \"clip_vitb16\"]:\n",
    "    BEST_PARAMS_v1[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"chebyshev\",\n",
    "            \"aggclust_linkage\": \"average\",\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4CouZvuSjPvc"
   },
   "outputs": [],
   "source": [
    "# vicreg_resnet50 is the only change from v1 to v2\n",
    "for model in [\"resnet50\", \"mocov3_resnet50\", \"vitb16\", \"timm_vit_base_patch16_224.mae\"]:\n",
    "    BEST_PARAMS_v2[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"euclidean\",\n",
    "            \"aggclust_linkage\": \"ward\",\n",
    "        }\n",
    "    )\n",
    "for model in [\"vicreg_resnet50\", \"dino_resnet50\", \"clip_RN50\", \"dino_vitb16\"]:\n",
    "    BEST_PARAMS_v2[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"euclidean\",\n",
    "            \"aggclust_linkage\": \"average\",\n",
    "        }\n",
    "    )\n",
    "for model in [\"mocov3_vit_base\", \"clip_vitb16\"]:\n",
    "    BEST_PARAMS_v2[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"chebyshev\",\n",
    "            \"aggclust_linkage\": \"average\",\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4CouZvuSjPvc"
   },
   "outputs": [],
   "source": [
    "for model in [\"none\", \"resnet50\", \"mocov3_resnet50\", \"vitb16\"] + FT_MODELS:\n",
    "    BEST_PARAMS_v3[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"euclidean\",\n",
    "            \"aggclust_linkage\": \"ward\",\n",
    "        }\n",
    "    )\n",
    "for model in [\"vicreg_resnet50\", \"dino_resnet50\", \"clip_RN50\", \"dino_vitb16\"]:\n",
    "    BEST_PARAMS_v3[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"euclidean\",\n",
    "            \"aggclust_linkage\": \"average\",\n",
    "        }\n",
    "    )\n",
    "for model in [\"mocov3_vit_base\", \"clip_vitb16\", \"random_resnet50\", \"random_vitb16\"]:\n",
    "    BEST_PARAMS_v3[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"chebyshev\",\n",
    "            \"aggclust_linkage\": \"average\",\n",
    "        }\n",
    "    )\n",
    "for model in [\"timm_vit_base_patch16_224.mae\"]:\n",
    "    BEST_PARAMS_v3[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"cosine\",\n",
    "            \"aggclust_linkage\": \"average\",\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - mae_pretrain_vit_base_global\n",
    "# - clip_vitb16 (leaving as-is for now)\n",
    "for model in ALL_MODELS:\n",
    "    BEST_PARAMS_v4[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"tbd\",\n",
    "            \"aggclust_linkage\": \"tbd\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "for model in [\"resnet50\", \"mocov3_resnet50\", \"vitb16\"] + FT_MODELS:\n",
    "    BEST_PARAMS_v4[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"euclidean\",\n",
    "            \"aggclust_linkage\": \"ward\",\n",
    "        }\n",
    "    )\n",
    "for model in [\"vicreg_resnet50\", \"dino_resnet50\", \"clip_RN50\", \"dino_vitb16\"]:\n",
    "    BEST_PARAMS_v4[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"euclidean\",\n",
    "            \"aggclust_linkage\": \"average\",\n",
    "        }\n",
    "    )\n",
    "for model in [\"mocov3_vit_base\", \"clip_vitb16\", \"random_resnet50\", \"random_vitb16\"]:\n",
    "    BEST_PARAMS_v4[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"chebyshev\",\n",
    "            \"aggclust_linkage\": \"average\",\n",
    "        }\n",
    "    )\n",
    "for model in [\"none\", \"timm_vit_base_patch16_224.mae\", \"mae_pretrain_vit_base_global\"]:\n",
    "    BEST_PARAMS_v4[\"AgglomerativeClustering\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"cosine\",\n",
    "            \"aggclust_linkage\": \"average\",\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Thn1g4rpeoo"
   },
   "outputs": [],
   "source": [
    "BEST_PARAMS_v1[\"AC w/ C\"] = copy.deepcopy(BEST_PARAMS_v1[\"AgglomerativeClustering\"])\n",
    "BEST_PARAMS_v1[\"AC w/o C\"] = copy.deepcopy(BEST_PARAMS_v1[\"AgglomerativeClustering\"])\n",
    "BEST_PARAMS_v2[\"AC w/ C\"] = copy.deepcopy(BEST_PARAMS_v2[\"AgglomerativeClustering\"])\n",
    "BEST_PARAMS_v2[\"AC w/o C\"] = copy.deepcopy(BEST_PARAMS_v2[\"AgglomerativeClustering\"])\n",
    "BEST_PARAMS_v3[\"AC w/ C\"] = copy.deepcopy(BEST_PARAMS_v3[\"AgglomerativeClustering\"])\n",
    "BEST_PARAMS_v3[\"AC w/o C\"] = copy.deepcopy(BEST_PARAMS_v3[\"AgglomerativeClustering\"])\n",
    "BEST_PARAMS_v4[\"AC w/ C\"] = copy.deepcopy(BEST_PARAMS_v4[\"AgglomerativeClustering\"])\n",
    "BEST_PARAMS_v4[\"AC w/o C\"] = copy.deepcopy(BEST_PARAMS_v4[\"AgglomerativeClustering\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Gixn5KGtuNr"
   },
   "outputs": [],
   "source": [
    "for model in BEST_PARAMS_v1[\"AC w/ C\"]:\n",
    "    BEST_PARAMS_v1[\"AC w/ C\"][model].update({\"aggclust_dist_thresh\": None})\n",
    "for model in BEST_PARAMS_v2[\"AC w/ C\"]:\n",
    "    BEST_PARAMS_v2[\"AC w/ C\"][model].update({\"aggclust_dist_thresh\": None})\n",
    "for model in BEST_PARAMS_v3[\"AC w/ C\"]:\n",
    "    BEST_PARAMS_v3[\"AC w/ C\"][model].update({\"aggclust_dist_thresh\": None})\n",
    "for model in BEST_PARAMS_v4[\"AC w/ C\"]:\n",
    "    BEST_PARAMS_v4[\"AC w/ C\"][model].update({\"aggclust_dist_thresh\": None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7EqXneH6kv27"
   },
   "outputs": [],
   "source": [
    "for model in BEST_PARAMS_v2[\"AC w/o C\"]:\n",
    "    BEST_PARAMS_v2[\"AC w/o C\"][model].update(\n",
    "        {\"zscore2\": \"average\", \"ndim_correction\": True}\n",
    "    )\n",
    "for model in BEST_PARAMS_v3[\"AC w/o C\"]:\n",
    "    BEST_PARAMS_v3[\"AC w/o C\"][model].update(\n",
    "        {\"zscore2\": \"average\", \"ndim_correction\": True}\n",
    "    )\n",
    "for model in BEST_PARAMS_v4[\"AC w/o C\"]:\n",
    "    BEST_PARAMS_v4[\"AC w/o C\"][model].update(\n",
    "        {\"zscore2\": \"average\", \"ndim_correction\": True}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yrmctdctSRgu"
   },
   "outputs": [],
   "source": [
    "# Run AgglomerativeClustering experiments with number of clusters unknown\n",
    "# \tresnet50        \t20.0\n",
    "# \tmocov3_resnet50 \t20.0\n",
    "# \tvicreg_resnet50 \t20.0\n",
    "# \tvitb16 \t            20.0\n",
    "# \tdino_resnet50     \t 1.0\n",
    "# \tclip_RN50 \t         1.0\n",
    "# \tdino_vitb16 \t     2.0\n",
    "# \tmocov3_vit_base \t 1.0\n",
    "# \tclip_vitb16 \t     0.5\n",
    "# \ttimm_vit_base_patch16_224.mae \t200.0\n",
    "\n",
    "for model in [\"resnet50\", \"mocov3_resnet50\", \"vicreg_resnet50\", \"vitb16\"]:\n",
    "    BEST_PARAMS_v1[\"AC w/o C\"][model].update({\"aggclust_dist_thresh\": 20.0})\n",
    "for model in [\"dino_resnet50\", \"clip_RN50\", \"mocov3_vit_base\"]:\n",
    "    BEST_PARAMS_v1[\"AC w/o C\"][model].update({\"aggclust_dist_thresh\": 1.0})\n",
    "BEST_PARAMS_v1[\"AC w/o C\"][\"dino_vitb16\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v1[\"AC w/o C\"][\"clip_vitb16\"][\"aggclust_dist_thresh\"] = 0.5\n",
    "BEST_PARAMS_v1[\"AC w/o C\"][\"timm_vit_base_patch16_224.mae\"][\n",
    "    \"aggclust_dist_thresh\"\n",
    "] = 200.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nEW3rz_5aQ70"
   },
   "outputs": [],
   "source": [
    "BEST_PARAMS_v2[\"AC w/o C\"][\"resnet50\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v2[\"AC w/o C\"][\"mocov3_resnet50\"][\"aggclust_dist_thresh\"] = 10.0\n",
    "BEST_PARAMS_v2[\"AC w/o C\"][\"vicreg_resnet50\"][\"aggclust_dist_thresh\"] = 0.5\n",
    "BEST_PARAMS_v2[\"AC w/o C\"][\"dino_resnet50\"][\"aggclust_dist_thresh\"] = 0.5\n",
    "BEST_PARAMS_v2[\"AC w/o C\"][\"clip_RN50\"][\"aggclust_dist_thresh\"] = 0.5\n",
    "BEST_PARAMS_v2[\"AC w/o C\"][\"vitb16\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v2[\"AC w/o C\"][\"mocov3_vit_base\"][\"aggclust_dist_thresh\"] = 1.0\n",
    "BEST_PARAMS_v2[\"AC w/o C\"][\"timm_vit_base_patch16_224.mae\"][\n",
    "    \"aggclust_dist_thresh\"\n",
    "] = 5.0\n",
    "BEST_PARAMS_v2[\"AC w/o C\"][\"dino_vitb16\"][\"aggclust_dist_thresh\"] = 0.2\n",
    "BEST_PARAMS_v2[\"AC w/o C\"][\"clip_vitb16\"][\"aggclust_dist_thresh\"] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_PARAMS_v3[\"AC w/o C\"][\"none\"][\"aggclust_dist_thresh\"] = 10.0\n",
    "BEST_PARAMS_v3[\"AC w/o C\"][\"random_resnet50\"][\"aggclust_dist_thresh\"] = 10.0\n",
    "BEST_PARAMS_v3[\"AC w/o C\"][\"resnet50\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v3[\"AC w/o C\"][\"mocov3_resnet50\"][\"aggclust_dist_thresh\"] = 10.0\n",
    "BEST_PARAMS_v3[\"AC w/o C\"][\"dino_resnet50\"][\"aggclust_dist_thresh\"] = 0.5\n",
    "BEST_PARAMS_v3[\"AC w/o C\"][\"vicreg_resnet50\"][\"aggclust_dist_thresh\"] = 0.5\n",
    "BEST_PARAMS_v3[\"AC w/o C\"][\"clip_RN50\"][\"aggclust_dist_thresh\"] = 0.5\n",
    "BEST_PARAMS_v3[\"AC w/o C\"][\"random_vitb16\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v3[\"AC w/o C\"][\"vitb16\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v3[\"AC w/o C\"][\"mocov3_vit_base\"][\"aggclust_dist_thresh\"] = 1.0\n",
    "BEST_PARAMS_v3[\"AC w/o C\"][\"dino_vitb16\"][\"aggclust_dist_thresh\"] = 0.2\n",
    "BEST_PARAMS_v3[\"AC w/o C\"][\"timm_vit_base_patch16_224.mae\"][\"aggclust_dist_thresh\"] = 0.5\n",
    "BEST_PARAMS_v3[\"AC w/o C\"][\"clip_vitb16\"][\"aggclust_dist_thresh\"] = 1.0\n",
    "BEST_PARAMS_v3[\"AC w/o C\"][\"ft_mocov3_resnet50\"][\"aggclust_dist_thresh\"] = 1.0\n",
    "BEST_PARAMS_v3[\"AC w/o C\"][\"ft_dino_resnet50\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v3[\"AC w/o C\"][\"ft_vicreg_resnet50\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v3[\"AC w/o C\"][\"ft_mocov3_vit_base\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v3[\"AC w/o C\"][\"ft_dino_vitb16\"][\"aggclust_dist_thresh\"] = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - none\n",
    "# - random_resnet50\n",
    "# - timm_vit_base_patch16_224.mae\n",
    "# - mae_pretrain_vit_base_global\n",
    "# - clip_vitb16 (leave as-is)\n",
    "# - ft_mocov3_resnet50 (tbc)\n",
    "# - mae_finetuned_vit_base_global\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"resnet50\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"mocov3_resnet50\"][\"aggclust_dist_thresh\"] = 10.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"dino_resnet50\"][\"aggclust_dist_thresh\"] = 0.5\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"vicreg_resnet50\"][\"aggclust_dist_thresh\"] = 0.5\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"random_vitb16\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"vitb16\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"mocov3_vit_base\"][\"aggclust_dist_thresh\"] = 1.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"dino_vitb16\"][\"aggclust_dist_thresh\"] = 0.2\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"ft_mocov3_resnet50\"][\"aggclust_dist_thresh\"] = 2.0  # tbc\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"ft_dino_resnet50\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"ft_vicreg_resnet50\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"ft_mocov3_vit_base\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"ft_dino_vitb16\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"_version\"] = \"v4.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - none\n",
    "# - timm_vit_base_patch16_224.mae (tbc)\n",
    "# - mae_pretrain_vit_base_global\n",
    "# - clip_vitb16 (leave as-is)\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"random_resnet50\"][\"aggclust_dist_thresh\"] = 10.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"resnet50\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"mocov3_resnet50\"][\"aggclust_dist_thresh\"] = 10.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"dino_resnet50\"][\"aggclust_dist_thresh\"] = 0.5\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"vicreg_resnet50\"][\"aggclust_dist_thresh\"] = 0.5\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"clip_RN50\"][\"aggclust_dist_thresh\"] = 0.5\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"random_vitb16\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"vitb16\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"mocov3_vit_base\"][\"aggclust_dist_thresh\"] = 1.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"dino_vitb16\"][\"aggclust_dist_thresh\"] = 0.2\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"timm_vit_base_patch16_224.mae\"][\"aggclust_dist_thresh\"] = 0.5  # tbc\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"clip_vitb16\"][\"aggclust_dist_thresh\"] = 1.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"ft_mocov3_resnet50\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"ft_dino_resnet50\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"ft_vicreg_resnet50\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"ft_mocov3_vit_base\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"ft_dino_vitb16\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"mae_finetuned_vit_base_global\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"_version\"] = \"v4.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v4.4\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"none\"][\"aggclust_dist_thresh\"] = 0.71\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"random_resnet50\"][\"aggclust_dist_thresh\"] = 10.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"resnet50\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"mocov3_resnet50\"][\"aggclust_dist_thresh\"] = 10.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"dino_resnet50\"][\"aggclust_dist_thresh\"] = 0.5\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"vicreg_resnet50\"][\"aggclust_dist_thresh\"] = 0.5\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"clip_RN50\"][\"aggclust_dist_thresh\"] = 0.5\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"random_vitb16\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"vitb16\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"mocov3_vit_base\"][\"aggclust_dist_thresh\"] = 1.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"dino_vitb16\"][\"aggclust_dist_thresh\"] = 0.2\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"timm_vit_base_patch16_224.mae\"][\"aggclust_dist_thresh\"] = 0.71\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"mae_pretrain_vit_base_global\"][\"aggclust_dist_thresh\"] = 0.71\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"clip_vitb16\"][\"aggclust_dist_thresh\"] = 1.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"ft_mocov3_resnet50\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"ft_dino_resnet50\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"ft_vicreg_resnet50\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"ft_mocov3_vit_base\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"ft_dino_vitb16\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"AC w/o C\"][\"mae_finetuned_vit_base_global\"][\"aggclust_dist_thresh\"] = 2.0\n",
    "BEST_PARAMS_v4[\"_version\"] = \"v4.4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Affinity Prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in BEST_PARAMS_v1[\"AffinityPropagation\"]:\n",
    "    BEST_PARAMS_v1[\"AffinityPropagation\"][model][\"affinity_damping\"] = 0.5\n",
    "for model in BEST_PARAMS_v2[\"AffinityPropagation\"]:\n",
    "    BEST_PARAMS_v2[\"AffinityPropagation\"][model][\"affinity_damping\"] = 0.5\n",
    "for model in BEST_PARAMS_v3[\"AffinityPropagation\"]:\n",
    "    BEST_PARAMS_v3[\"AffinityPropagation\"][model][\"affinity_damping\"] = 0.9\n",
    "for model in BEST_PARAMS_v4[\"AffinityPropagation\"]:\n",
    "    BEST_PARAMS_v4[\"AffinityPropagation\"][model][\"affinity_damping\"] = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"none\"][\"affinity_damping\"] = 0.85\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"random_resnet50\"][\"affinity_damping\"] = 0.5\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"resnet50\"][\"affinity_damping\"] = 0.9\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"mocov3_resnet50\"][\"affinity_damping\"] = 0.8\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"dino_resnet50\"][\"affinity_damping\"] = 0.8\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"vicreg_resnet50\"][\"affinity_damping\"] = 0.75\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"clip_RN50\"][\"affinity_damping\"] = 0.85\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"random_vitb16\"][\"affinity_damping\"] = 0.7\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"vitb16\"][\"affinity_damping\"] = 0.9\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"mocov3_vit_base\"][\"affinity_damping\"] = 0.75\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"dino_vitb16\"][\"affinity_damping\"] = 0.85\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"timm_vit_base_patch16_224.mae\"][\"affinity_damping\"] = 0.5\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"mae_pretrain_vit_base_global\"][\"affinity_damping\"] = 0.9  # To match\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"clip_vitb16\"][\"affinity_damping\"] = 0.95\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"ft_mocov3_resnet50\"][\"affinity_damping\"] = 0.9  # Match supervised/ft resnet50\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"ft_vicreg_resnet50\"][\"affinity_damping\"] = 0.9\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"ft_dino_vitb16\"][\"affinity_damping\"] = 0.9\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"ft_mocov3_vit_base\"][\"affinity_damping\"] = 0.9  # Match supervised/ft resnet50\n",
    "BEST_PARAMS_v3[\"AffinityPropagation\"][\"mae_finetuned_vit_base_global\"][\"affinity_damping\"] = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"none\"][\"affinity_damping\"] = 0.85\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"random_resnet50\"][\"affinity_damping\"] = 0.9\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"resnet50\"][\"affinity_damping\"] = 0.9\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"mocov3_resnet50\"][\"affinity_damping\"] = 0.75\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"dino_resnet50\"][\"affinity_damping\"] = 0.9\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"vicreg_resnet50\"][\"affinity_damping\"] = 0.8\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"clip_RN50\"][\"affinity_damping\"] = 0.85\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"random_vitb16\"][\"affinity_damping\"] = 0.95\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"vitb16\"][\"affinity_damping\"] = 0.9\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"mocov3_vit_base\"][\"affinity_damping\"] = 0.75\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"dino_vitb16\"][\"affinity_damping\"] = 0.85\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"timm_vit_base_patch16_224.mae\"][\"affinity_damping\"] = 0.6\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"mae_pretrain_vit_base_global\"][\"affinity_damping\"] = 0.6\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"clip_vitb16\"][\"affinity_damping\"] = 0.95\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"ft_mocov3_resnet50\"][\"affinity_damping\"] = 0.95\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"ft_dino_resnet50\"][\"affinity_damping\"] = 0.9\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"ft_vicreg_resnet50\"][\"affinity_damping\"] = 0.9\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"ft_mocov3_vit_base\"][\"affinity_damping\"] = 0.95\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"ft_dino_vitb16\"][\"affinity_damping\"] = 0.9\n",
    "BEST_PARAMS_v4[\"AffinityPropagation\"][\"mae_finetuned_vit_base_global\"][\"affinity_damping\"] = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KeuptU7acB0d"
   },
   "source": [
    "### HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHODpPwVL0FT"
   },
   "outputs": [],
   "source": [
    "for model in RESNET50_MODELS + VITB16_MODELS:\n",
    "    BEST_PARAMS_v1[\"HDBSCAN\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"euclidean\",\n",
    "            \"hdbscan_method\": \"eom\",\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZoUxa6TG03b9"
   },
   "source": [
    "v2 selection\n",
    "\n",
    "|    | model                         | distance_metric   | hdbscan_method   |      AMI |\n",
    "|---:|:------------------------------|:------------------|:-----------------|---------:|\n",
    "|  0 | resnet50                      | euclidean         | eom              | 0.828368 |\n",
    "|  1 | mocov3_resnet50               | euclidean         | eom              | 0.531644 |\n",
    "|  2 | vicreg_resnet50               | l1                | eom              | 0.472324 |\n",
    "|  3 | dino_resnet50                 | l1                | eom              | 0.503147 |\n",
    "|  4 | clip_RN50                     | l1                | eom              | 0.461363 |\n",
    "|  5 | vitb16                        | chebyshev         | eom              | 0.906110 |\n",
    "|  6 | mocov3_vit_base               | euclidean         | eom              | 0.629966 |\n",
    "|  7 | timm_vit_base_patch16_224.mae | euclidean         | eom              | 0.070495 |\n",
    "|  8 | dino_vitb16                   | l1                | eom              | 0.691547 |\n",
    "|  9 | clip_vitb16                   | l1                | eom              | 0.592489 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDMQjXpoKb6_"
   },
   "outputs": [],
   "source": [
    "for model in RESNET50_MODELS + VITB16_MODELS:\n",
    "    BEST_PARAMS_v2[\"HDBSCAN\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"euclidean\",\n",
    "            \"hdbscan_method\": \"eom\",\n",
    "        }\n",
    "    )\n",
    "for model in [\n",
    "    \"vicreg_resnet50\",\n",
    "    \"dino_resnet50\",\n",
    "    \"clip_RN50\",\n",
    "    \"dino_vitb16\",\n",
    "    \"clip_vitb16\",\n",
    "]:\n",
    "    BEST_PARAMS_v2[\"HDBSCAN\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"l1\",\n",
    "        }\n",
    "    )\n",
    "BEST_PARAMS_v2[\"HDBSCAN\"][\"vitb16\"][\"distance_metric\"] = \"chebyshev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nKYseOkOz8RU"
   },
   "outputs": [],
   "source": [
    "for model in [\n",
    "    \"resnet50\",\n",
    "    \"mocov3_resnet50\",\n",
    "    \"mocov3_vit_base\",\n",
    "    \"timm_vit_base_patch16_224.mae\",\n",
    "]:\n",
    "    BEST_PARAMS_v3[\"HDBSCAN\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"euclidean\",\n",
    "            \"hdbscan_method\": \"eom\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "for model in [\n",
    "    \"random_resnet50\",\n",
    "    \"vicreg_resnet50\",\n",
    "    \"dino_resnet50\",\n",
    "    \"clip_RN50\",\n",
    "    \"random_vitb16\",\n",
    "    \"dino_vitb16\",\n",
    "    \"clip_vitb16\",\n",
    "]:\n",
    "    BEST_PARAMS_v3[\"HDBSCAN\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"l1\",\n",
    "            \"hdbscan_method\": \"eom\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "for model in [\"vitb16\"]:\n",
    "    BEST_PARAMS_v3[\"HDBSCAN\"][model].update(\n",
    "        {\n",
    "            \"distance_metric\": \"chebyshev\",\n",
    "            \"hdbscan_method\": \"eom\",\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"none\"][\"distance_metric\"] = \"euclidean\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"none\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"random_resnet50\"][\"distance_metric\"] = \"l1\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"random_resnet50\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"resnet50\"][\"distance_metric\"] = \"euclidean\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"resnet50\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"mocov3_resnet50\"][\"distance_metric\"] = \"euclidean\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"mocov3_resnet50\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"dino_resnet50\"][\"distance_metric\"] = \"l1\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"dino_resnet50\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"vicreg_resnet50\"][\"distance_metric\"] = \"l1\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"vicreg_resnet50\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"clip_RN50\"][\"distance_metric\"] = \"l1\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"clip_RN50\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"random_vitb16\"][\"distance_metric\"] = \"l1\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"random_vitb16\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"vitb16\"][\"distance_metric\"] = \"chebyshev\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"vitb16\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"mocov3_vit_base\"][\"distance_metric\"] = \"euclidean\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"mocov3_vit_base\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"dino_vitb16\"][\"distance_metric\"] = \"l1\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"dino_vitb16\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"timm_vit_base_patch16_224.mae\"][\"distance_metric\"] = \"euclidean\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"timm_vit_base_patch16_224.mae\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"mae_pretrain_vit_base_global\"][\"distance_metric\"] = \"l1\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"mae_pretrain_vit_base_global\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"clip_vitb16\"][\"distance_metric\"] = \"l1\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"clip_vitb16\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"ft_mocov3_resnet50\"][\"distance_metric\"] = \"chebyshev\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"ft_mocov3_resnet50\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"ft_dino_resnet50\"][\"distance_metric\"] = \"l1\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"ft_dino_resnet50\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"ft_vicreg_resnet50\"][\"distance_metric\"] = \"euclidean\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"ft_vicreg_resnet50\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"ft_mocov3_vit_base\"][\"distance_metric\"] = \"chebyshev\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"ft_mocov3_vit_base\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"ft_dino_vitb16\"][\"distance_metric\"] = \"chebyshev\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"ft_dino_vitb16\"][\"hdbscan_method\"] = \"eom\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"mae_finetuned_vit_base_global\"][\"distance_metric\"] = \"chebyshev\"\n",
    "BEST_PARAMS_v4[\"HDBSCAN\"][\"mae_finetuned_vit_base_global\"][\"hdbscan_method\"] = \"eom\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, set overall hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_PARAMS = BEST_PARAMS_v4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSVZM4cns-gm"
   },
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8w21wu6JUk4d"
   },
   "outputs": [],
   "source": [
    "def categorical_cmap(nc, nsc, cmap=\"tab10\", continuous=False):\n",
    "    \"\"\"\n",
    "    Create a colormap with a certain number of shades of colours.\n",
    "\n",
    "    https://stackoverflow.com/a/47232942/1960959\n",
    "    \"\"\"\n",
    "    if nc > plt.get_cmap(cmap).N:\n",
    "        raise ValueError(\"Too many categories for colormap.\")\n",
    "    if continuous:\n",
    "        ccolors = plt.get_cmap(cmap)(np.linspace(0, 1, nc))\n",
    "    else:\n",
    "        ccolors = plt.get_cmap(cmap)(np.arange(nc, dtype=int))\n",
    "    cols = np.zeros((nc * nsc, 3))\n",
    "    for i, c in enumerate(ccolors):\n",
    "        chsv = matplotlib.colors.rgb_to_hsv(c[:3])\n",
    "        arhsv = np.tile(chsv, nsc).reshape(nsc, 3)\n",
    "        arhsv[:, 1] = np.linspace(chsv[1], 0.25, nsc)\n",
    "        arhsv[:, 2] = np.linspace(chsv[2], 1, nsc)\n",
    "        rgb = matplotlib.colors.hsv_to_rgb(arhsv)\n",
    "        cols[i * nsc : (i + 1) * nsc, :] = rgb\n",
    "    cmap = matplotlib.colors.ListedColormap(cols)\n",
    "    return cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "id": "Zt-xAviSUwWV",
    "outputId": "0feb0c9b-c97d-4ec7-955f-8fdcc521be3c"
   },
   "outputs": [],
   "source": [
    "categorical_cmap(len(RESNET50_MODELS), len(VALIDATION_DATASETS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zs_ssl_clustering.datasets import image_dataset_sizes\n",
    "\n",
    "\n",
    "def clip_imgsize(dataset, target_image_size):\n",
    "    if target_image_size is None:\n",
    "        return target_image_size\n",
    "    dataset_imsize = image_dataset_sizes(dataset)[1]\n",
    "    if dataset_imsize is None:\n",
    "        return target_image_size\n",
    "    return min(target_image_size, dataset_imsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixup_filter(filters):\n",
    "    dataset = filters.get(\"dataset_name\", filters.get(\"dataset\", None))\n",
    "    if dataset and \"image_size\" in filters:\n",
    "        filters[\"image_size\"] = clip_imgsize(dataset, filters[\"image_size\"])\n",
    "    if dataset and \"min_samples\" in filters:\n",
    "        if dataset.lower() in [\"celeba\", \"utkface\"]:\n",
    "            filters[\"min_samples\"] = 2\n",
    "    return filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i5hY2oc2I-PO"
   },
   "outputs": [],
   "source": [
    "def select_rows(df, filters, allow_missing=True, fixup=True):\n",
    "    if fixup:\n",
    "        filters = fixup_filter(filters)\n",
    "    select = np.ones(len(df), dtype=bool)\n",
    "    for col, val in filters.items():\n",
    "        if col == \"dataset\":\n",
    "            col = \"dataset_name\"\n",
    "        if col == \"clusterer\":\n",
    "            col = \"clusterer_name\"\n",
    "        if val is None or val == \"None\" or val == \"none\":\n",
    "            select_i = pd.isna(df[col])\n",
    "            select_i |= df[col] == \"None\"\n",
    "            select_i |= df[col] == \"none\"\n",
    "        else:\n",
    "            select_i = df[col] == val\n",
    "            select_i |= df[col] == str(val)\n",
    "            if allow_missing or val == \"None\" or val == \"none\":\n",
    "                select_i |= pd.isna(df[col])\n",
    "        select &= select_i\n",
    "    return df[select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7iOVrUnC_Jg-"
   },
   "outputs": [],
   "source": [
    "def find_differing_columns(df, cols=None):\n",
    "    if cols is None:\n",
    "        cols = df.columns\n",
    "    my_cols = []\n",
    "    for col in cols:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        if df[col].nunique(dropna=False) > 1:\n",
    "            my_cols.append(col)\n",
    "    return my_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wI7MkSQiz89O"
   },
   "outputs": [],
   "source": [
    "def filter2command(*filters, partition=\"val\"):\n",
    "    f = {}\n",
    "    for filter in filters:\n",
    "        for k, v in filter.items():\n",
    "            f[k] = v\n",
    "    dataset = f.get(\"dataset\", \"\")\n",
    "    clusterer = f.get(\"clusterer\", \"\")\n",
    "\n",
    "    MEM = \"2G\"\n",
    "\n",
    "    if clusterer == \"AffinityPropagation\":\n",
    "        if dataset in [\"inaturalist\"]:\n",
    "            MEM = \"292G\"\n",
    "        elif dataset in [\"imagenet-sketch\", \"imagenet\"]:\n",
    "            MEM = \"96G\"\n",
    "        elif dataset in [\"places365\", \"imagenet-r\", \"svhn\", \"nabirds\"]:\n",
    "            MEM = \"64G\"\n",
    "        elif dataset in [\"celeba\"]:\n",
    "            MEM = \"12G\"\n",
    "        elif dataset in [\"imagenetv2\", \"cifar10\", \"cifar100\", \"lsun\", \"mnist\", \"fashionmnist\", \"stanfordcars\"]:\n",
    "            MEM = \"6G\"\n",
    "        elif (\n",
    "            dataset.startswith(\"in9\")\n",
    "            or dataset in [\"flowers102\", \"utkface\", \"eurosat\", \"aircraft\", \"imagenet-o\", \"dtd\"]\n",
    "        ):\n",
    "            MEM = \"2G\"\n",
    "        elif dataset in [\"imagenette\", \"imagewoof\"]:\n",
    "            MEM = \"1G\"\n",
    "        else:\n",
    "            MEM = \"8G\"\n",
    "\n",
    "    if clusterer == \"AgglomerativeClustering\":\n",
    "        if dataset in [\"inaturalist\"]:\n",
    "            MEM = \"72G\"\n",
    "        elif dataset in [\"imagenet-sketch\", \"imagenet\"]:\n",
    "            MEM = \"20G\"\n",
    "        elif dataset in [\"places365\", \"imagenet-r\", \"svhn\", \"nabirds\"]:\n",
    "            MEM = \"16G\"\n",
    "        elif dataset in [\"celeba\"]:\n",
    "            MEM = \"12G\"\n",
    "        elif dataset in [\"imagenetv2\", \"cifar10\", \"cifar100\", \"lsun\", \"mnist\", \"fashionmnist\", \"stanfordcars\"]:\n",
    "            MEM = \"6G\"\n",
    "        elif (\n",
    "            dataset.startswith(\"in9\")\n",
    "            or dataset in [\"flowers102\", \"utkface\", \"eurosat\", \"aircraft\", \"imagenet-o\", \"dtd\"]\n",
    "        ):\n",
    "            MEM = \"4G\"\n",
    "        elif dataset in [\"imagenette\", \"imagewoof\"]:\n",
    "            MEM = \"2G\"\n",
    "        else:\n",
    "            MEM = \"8G\"\n",
    "\n",
    "    if clusterer in [\"HDBSCAN\", \"KMeans\"]:\n",
    "        if dataset in [\"inaturalist\"]:\n",
    "            MEM = \"6G\"\n",
    "        elif dataset in [\"imagenet-sketch\", \"imagenet\"]:\n",
    "            MEM = \"4G\"\n",
    "        elif dataset in [\"places365\", \"imagenet-r\", \"svhn\", \"nabirds\"]:\n",
    "            MEM = \"4G\"\n",
    "        elif dataset in [\"celeba\"]:\n",
    "            MEM = \"4G\"\n",
    "        elif dataset in [\"imagenetv2\", \"cifar10\", \"cifar100\", \"lsun\", \"mnist\", \"fashionmnist\", \"stanfordcars\"]:\n",
    "            MEM = \"2G\"\n",
    "        elif (\n",
    "            dataset.startswith(\"in9\")\n",
    "            or dataset in [\"flowers102\", \"utkface\", \"eurosat\", \"aircraft\", \"imagenet-o\", \"dtd\"]\n",
    "        ):\n",
    "            MEM = \"2G\"\n",
    "        elif dataset in [\"imagenette\", \"imagewoof\"]:\n",
    "            MEM = \"1G\"\n",
    "        else:\n",
    "            MEM = \"4G\"\n",
    "\n",
    "    if partition == \"val\":\n",
    "        seed = 100\n",
    "    elif partition == \"test\":\n",
    "        seed = 1\n",
    "    else:\n",
    "        seed = 0\n",
    "    s = (\n",
    "        f\"sbatch --array={seed} --mem={MEM}\"\n",
    "        f' --job-name=\"zsc-{f.get(\"model\", \"\")}-{dataset}-{clusterer}\"'\n",
    "        f\" slurm/cluster.slrm --partition={partition}\"\n",
    "    )\n",
    "    for k, v in f.items():\n",
    "        if v is None:\n",
    "            continue\n",
    "        if k == \"zscore\":\n",
    "            if v == \"False\" or not v:\n",
    "                s += \" --no-zscore\"\n",
    "            elif v == \"True\" or v:\n",
    "                s += \" --zscore\"\n",
    "            continue\n",
    "        if k == \"normalize\":\n",
    "            if v == \"False\" or not v:\n",
    "                pass\n",
    "            elif v == \"True\" or v:\n",
    "                s += \" --normalize\"\n",
    "            continue\n",
    "        if k == \"zscore2\":\n",
    "            if v == \"False\" or not v:\n",
    "                s += \" --no-zscore2\"\n",
    "            elif v == \"average\":\n",
    "                s += \" --azscore2\"\n",
    "            elif v == \"standard\" or v:\n",
    "                s += \" --zscore2\"\n",
    "            continue\n",
    "        if k == \"ndim_correction\":\n",
    "            if v == \"False\" or not v:\n",
    "                s += \" --no-ndim-correction\"\n",
    "            elif v == \"True\" or v:\n",
    "                s += \" --ndim-correction\"\n",
    "            continue\n",
    "        s += f\" --{k.replace('_', '-')}={v}\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOKljXoMNGG7"
   },
   "source": [
    "# Final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "CuPbp58ONv4Z"
   },
   "outputs": [],
   "source": [
    "TEST_DATASETS = [\n",
    "    \"imagenet\",\n",
    "    \"imagenetv2\",\n",
    "    \"imagenet-o\",\n",
    "    \"cifar10\",\n",
    "    \"cifar100\",\n",
    "    \"in9original\",\n",
    "    \"in9mixednext\",\n",
    "    \"in9onlybgt\",\n",
    "    \"in9onlyfg\",\n",
    "    \"imagenet-r\",\n",
    "    \"imagenet-sketch\",\n",
    "    \"aircraft\",\n",
    "    \"stanfordcars\",\n",
    "    \"flowers102\",\n",
    "    \"nabirds\",\n",
    "    \"inaturalist\",\n",
    "    \"celeba\",\n",
    "    \"utkface\",\n",
    "    \"dtd\",\n",
    "    \"eurosat\",\n",
    "    \"lsun\",\n",
    "    \"places365\",\n",
    "    \"mnist\",\n",
    "    \"fashionmnist\",\n",
    "    \"svhn\",\n",
    "]\n",
    "DATASET2SH = {\n",
    "    \"aircraft\": \"Air\",\n",
    "    \"celeba\": \"CelA\",\n",
    "    \"cifar10\": \"C10\",\n",
    "    \"cifar100\": \"C100\",\n",
    "    \"dtd\": \"DTD\",\n",
    "    \"eurosat\": \"ESAT\",\n",
    "    \"flowers102\": \"F102\",\n",
    "    \"fashionmnist\": \"fMNIST\",\n",
    "    \"imagenet\": \"IN1k\",\n",
    "    \"imagenet-o\": \"IN-O\",\n",
    "    \"imagenet-r\": \"IN-R\",\n",
    "    \"imagenet-sketch\": \"IN-S\",\n",
    "    \"imagenetv2\": \"INv2\",\n",
    "    \"imagenette\": \"IN10\",\n",
    "    \"imagewoof\": \"INwf\",\n",
    "    \"in9original\": \"IN9\",\n",
    "    \"in9mixednext\": \"IN9-MN\",\n",
    "    \"in9mixedrand\": \"IN9-MR\",\n",
    "    \"in9mixedsame\": \"IN9-MS\",\n",
    "    \"in9nofg\": \"IN9-NoFG\",\n",
    "    \"in9onlybgb\": \"IN9-BGB\",\n",
    "    \"in9onlybgt\": \"IN9-BGT\",\n",
    "    \"in9onlyfg\": \"IN9-FG\",\n",
    "    \"inaturalist\": \"iNat21\",\n",
    "    \"lsun\": \"LSUN\",\n",
    "    \"mnist\": \"MNIST\",\n",
    "    \"nabirds\": \"Birds\",\n",
    "    \"places365\": \"P365\",\n",
    "    \"stanfordcars\": \"Cars\",\n",
    "    \"svhn\": \"SVHN\",\n",
    "    \"utkface\": \"UTKF\",\n",
    "}\n",
    "MODEL_GROUPS = {\n",
    "    \"ResNet-50\": RESNET50_MODELS,\n",
    "    \"ViT-B\": VITB16_MODELS,\n",
    "    \"ResNet-50 [FT]\": FT_RESNET50_MODELS,\n",
    "    \"ViT-B [FT]\": FT_VITB16_MODELS,\n",
    "    \"all\": ALL_MODELS,\n",
    "}\n",
    "MODEL2SH = {\n",
    "    \"none\": \"Raw image\",\n",
    "    \"random_resnet50\": \"Random\",\n",
    "    \"random_vitb16\": \"Random\",\n",
    "    \"resnet50\": \"Supervised\",\n",
    "    \"mocov3_resnet50\": \"MoCo-v3\",\n",
    "    \"dino_resnet50\": \"DINO\",\n",
    "    \"vicreg_resnet50\": \"VICReg\",\n",
    "    \"clip_RN50\": \"CLIP\",\n",
    "    \"vitb16\": \"Supervised\",\n",
    "    \"mocov3_vit_base\": \"MoCo-v3\",\n",
    "    \"dino_vitb16\": \"DINO\",\n",
    "    \"timm_vit_base_patch16_224.mae\": \"MAE (CLS)\",\n",
    "    \"mae_pretrain_vit_base_global\": \"MAE (avg)\",\n",
    "    \"clip_vitb16\": \"CLIP\",\n",
    "    \"ft_mocov3_resnet50\": \"MoCo-v3 [FT]\",\n",
    "    \"ft_dino_resnet50\": \"DINO [FT]\",\n",
    "    \"ft_vicreg_resnet50\": \"VICReg [FT]\",\n",
    "    \"ft_mocov3_vit_base\": \"MoCo-v3 [FT]\",\n",
    "    \"ft_dino_vitb16\": \"DINO [FT]\",\n",
    "    \"mae_finetuned_vit_base_global\": \"MAE (avg) [FT]\",\n",
    "}\n",
    "CLUSTERER2SH = {\n",
    "    \"KMeans\": \"K-Means\",\n",
    "    \"AffinityPropagation\": \"Affinity Prop\",\n",
    "    \"AgglomerativeClustering\": \"AC\",\n",
    "    \"AC w/ C\": \"AC w/  C\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATASETS_GROUPED = {\n",
    "    \"In-domain\": [\n",
    "        \"imagenet\",\n",
    "        \"imagenetv2\",\n",
    "        \"imagenet-o\",\n",
    "        \"in9original\",\n",
    "        \"cifar10\",\n",
    "        \"cifar100\",\n",
    "    ],\n",
    "    \"Domain-shift\": [\n",
    "        \"imagenet-r\",\n",
    "        \"imagenet-sketch\",\n",
    "        \"in9mixednext\",\n",
    "        \"in9onlybgt\",\n",
    "        \"in9onlyfg\",\n",
    "    ],\n",
    "    \"Fine-grained\": [\n",
    "        \"aircraft\",\n",
    "        \"stanfordcars\",\n",
    "        \"flowers102\",\n",
    "        \"nabirds\",\n",
    "        \"inaturalist\",\n",
    "    ],\n",
    "    \"Near-OOD\": [\n",
    "        \"lsun\",\n",
    "        \"places365\",\n",
    "    ],\n",
    "    \"Far-OOD\": [\n",
    "        \"celeba\",\n",
    "        \"utkface\",\n",
    "        \"dtd\",\n",
    "        \"eurosat\",\n",
    "        \"mnist\",\n",
    "        \"fashionmnist\",\n",
    "        \"svhn\",\n",
    "    ],\n",
    "}\n",
    "DATASETGROUP2TITLE = {\n",
    "    \"Domain-shift\": \"Domain-shifted\",\n",
    "    \"Out-of-distribution\": \"OOD\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTERER2COLORSTR = {\n",
    "    \"KMeans\": \"tab:purple\",\n",
    "    \"AC w/ C\": \"tab:red\",\n",
    "    \"AC w/o C\": \"tab:orange\",\n",
    "    \"AffinityPropagation\": \"tab:green\",\n",
    "    \"HDBSCAN\": \"tab:blue\",\n",
    "}\n",
    "CLUSTERER2COLORRGB = {k: matplotlib.colors.to_rgb(v) for k, v in CLUSTERER2COLORSTR.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL2COLORSTR = {\n",
    "    \"none\": \"black\",\n",
    "    \"random_resnet50\": \"tab:grey\",\n",
    "    \"random_vitb16\": \"tab:grey\",\n",
    "    \"resnet50\": \"tab:red\",\n",
    "    \"mocov3_resnet50\": \"tab:cyan\",\n",
    "    \"dino_resnet50\": \"tab:green\",\n",
    "    \"vicreg_resnet50\": \"tab:purple\",\n",
    "    \"clip_RN50\": \"tab:blue\",\n",
    "    \"vitb16\": \"tab:red\",\n",
    "    \"mocov3_vit_base\": \"tab:cyan\",\n",
    "    \"dino_vitb16\": \"tab:green\",\n",
    "    \"timm_vit_base_patch16_224.mae\": \"tab:olive\",\n",
    "    \"mae_pretrain_vit_base_global\": \"tab:brown\",\n",
    "    \"clip_vitb16\": \"tab:blue\",\n",
    "    \"ft_mocov3_resnet50\": \"tab:cyan\",\n",
    "    \"ft_dino_resnet50\": \"tab:green\",\n",
    "    \"ft_vicreg_resnet50\": \"tab:purple\",\n",
    "    \"ft_mocov3_vit_base\": \"tab:cyan\",\n",
    "    \"ft_dino_vitb16\": \"tab:green\",\n",
    "    \"mae_finetuned_vit_base_global\": \"tab:brown\",\n",
    "}\n",
    "MODEL2COLORRGB = {k: matplotlib.colors.to_rgb(v) for k, v in MODEL2COLORSTR.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRNXaeD-tWUo"
   },
   "source": [
    "## Fetch results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_df_long = pd.DataFrame({\"id\": []})\n",
    "config_keys = set()\n",
    "summary_keys = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previous results from CSV file\n",
    "CSV_FNAME = \"test_runs_df.csv\"\n",
    "if os.path.isfile(CSV_FNAME):\n",
    "    pass\n",
    "    # runs_df_long = test_runs_df = pd.read_csv(CSV_FNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "dytOzTA3NMeF",
    "outputId": "19a7efdd-c32a-4f3d-c467-092870aba23e"
   },
   "outputs": [],
   "source": [
    "# Project is specified by <entity/project-name>\n",
    "api = wandb.Api(timeout=720)\n",
    "runs = api.runs(\n",
    "    \"uoguelph_mlrg/zs-ssl-clustering\",\n",
    "    filters={\"state\": \"Finished\", \"config.partition\": \"test\"},  # \"config.predictions_dir\": \"y_pred\"},\n",
    "    per_page=10_000,\n",
    ")\n",
    "len(runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(runs_df_long)} runs currently in dataframe\")\n",
    "rows_to_add = []\n",
    "existing_ids = set(runs_df_long[\"id\"].values)\n",
    "for run in tqdm(runs):\n",
    "    if run.id in existing_ids:\n",
    "        continue\n",
    "    # .summary contains the output keys/values for metrics like accuracy.\n",
    "    #  We call ._json_dict to omit large files\n",
    "    summary = run.summary._json_dict\n",
    "    # .config contains the hyperparameters.\n",
    "    #  We remove special values that start with _.\n",
    "    config = {k: v for k, v in run.config.items() if not k.startswith(\"_\")}\n",
    "    # .name is the human-readable name of the run.\n",
    "    row = {\"id\": run.id, \"name\": run.name}\n",
    "    row.update({k: v for k, v in config.items() if not k.startswith(\"_\")})\n",
    "    row.update({k: v for k, v in summary.items() if not k.startswith(\"_\")})\n",
    "    if \"_timestamp\" in summary:\n",
    "        row[\"_timestamp\"] = summary[\"_timestamp\"]\n",
    "    rows_to_add.append(row)\n",
    "    config_keys = config_keys.union(config.keys())\n",
    "    summary_keys = summary_keys.union(summary.keys())\n",
    "\n",
    "if not len(rows_to_add):\n",
    "    print(\"No new runs to add\")\n",
    "else:\n",
    "    print(f\"Adding {len(rows_to_add)} runs\")\n",
    "    runs_df_long = pd.concat([runs_df_long, pd.DataFrame.from_records(rows_to_add)])\n",
    "print(f\"{len(runs_df_long)} runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove entries without an AMI metric\n",
    "test_runs_df = runs_df_long[~runs_df_long[\"AMI\"].isna()]\n",
    "len(test_runs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle changed default value for spectral_assigner after config arg was introduced\n",
    "if \"spectral_assigner\" not in test_runs_df.columns:\n",
    "    test_runs_df[\"spectral_assigner\"] = None\n",
    "select = test_runs_df[\"clusterer_name\"] != \"SpectralClustering\"\n",
    "test_runs_df.loc[select, \"spectral_assigner\"] = None\n",
    "select = (test_runs_df[\"clusterer_name\"] == \"SpectralClustering\") & pd.isna(\n",
    "    test_runs_df[\"spectral_assigner\"]\n",
    ")\n",
    "test_runs_df.loc[select, \"spectral_assigner\"] = \"kmeans\"\n",
    "\n",
    "# Accidentally wasn't clearing this hparam when it was unused\n",
    "if \"spectral_affinity\" not in test_runs_df.columns:\n",
    "    test_runs_df[\"spectral_affinity\"] = None\n",
    "select = test_runs_df[\"clusterer_name\"] != \"SpectralClustering\"\n",
    "test_runs_df.loc[select, \"spectral_affinity\"] = None\n",
    "\n",
    "if \"zscore2\" not in test_runs_df.columns:\n",
    "    test_runs_df[\"zscore2\"] = False\n",
    "test_runs_df.loc[pd.isna(test_runs_df[\"zscore2\"]), \"zscore2\"] = False\n",
    "\n",
    "if \"ndim_correction\" not in test_runs_df.columns:\n",
    "    test_runs_df[\"ndim_correction\"] = False\n",
    "test_runs_df.loc[pd.isna(test_runs_df[\"ndim_correction\"]), \"ndim_correction\"] = False\n",
    "\n",
    "if \"dim_reducer_man_nn\" not in test_runs_df.columns:\n",
    "    test_runs_df[\"dim_reducer_man_nn\"] = None\n",
    "\n",
    "if \"image_size\" not in test_runs_df.columns:\n",
    "    test_runs_df[\"image_size\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV file, so we can optionally skip downloading them\n",
    "test_runs_df.to_csv(CSV_FNAME, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "6bohRRomX9gO"
   },
   "outputs": [],
   "source": [
    "config_keys = config_keys.difference(\n",
    "    {\"workers\", \"memory_avail_GB\", \"memory_total_GB\", \"memory_slurm\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "k_KrOF5GNj9l",
    "outputId": "7649f12f-19ed-4113-bc66-88681d41ba3a"
   },
   "outputs": [],
   "source": [
    "test_runs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draft table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "_cBYVKo7PZFY",
    "outputId": "a1b016d5-a137-4100-b22b-67482af9ce74",
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric_key = \"AMI\"\n",
    "show_pc = True\n",
    "show_fmt = \"{:5.1f}\"\n",
    "show_commands = False\n",
    "eps = 0.001\n",
    "override_fields = {\n",
    "    # \"aggclust_dist_thresh\": None,  # to flip between unknown/known n clusters for AC\n",
    "    # \"predictions_dir\": \"y_pred\",\n",
    "}\n",
    "\n",
    "# KMeans  AffinityPropagation  AgglomerativeClustering  HDBSCAN\n",
    "backbones = MODEL_GROUPS.keys()\n",
    "clusterer = \"AgglomerativeClustering\"\n",
    "\n",
    "best_results = {k: [] for k in TEST_DATASETS}\n",
    "for dummy in [True, False]:\n",
    "    cmds = []\n",
    "    latex_table = r\"% Results for \" + f\"{metric_key}, {clusterer}\" + \"\\n\"\n",
    "    now_str = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    latex_table += r\"% Generated \" + now_str + \"\\n\"\n",
    "    latex_table += r\"% Using hparams \" + BEST_PARAMS[\"_version\"] + \"\\n\"\n",
    "    latex_table += r\"\\label{tab:\" + clusterer + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"\\resizebox{\\textwidth}{!}{%\" + \"\\n\"\n",
    "    latex_table += r\"\\begin{tabular}{ll\" + r\"r\" * len(TEST_DATASETS) + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    latex_table += r\"& \" + f\"{'Encoder':<11s}\"\n",
    "    for dataset in TEST_DATASETS:\n",
    "        latex_table += r\"&\" + \"{:^15s}\".format(DATASET2SH.get(dataset, dataset))\n",
    "    latex_table += r\"\\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    for i_group, model_group_name in enumerate(list(backbones)):\n",
    "        if i_group > 0:\n",
    "            latex_table += r\"\\midrule\" + \"\\n\"\n",
    "        for i_model, model in enumerate(MODEL_GROUPS[model_group_name]):\n",
    "            if i_model == 0:\n",
    "                latex_table += (\n",
    "                    r\"\\parbox[t]{2mm}{\\multirow{5}{*}{\\rotatebox[origin=c]{90}{\"\n",
    "                    + model_group_name\n",
    "                    + \"}}}\"\n",
    "                )\n",
    "                latex_table += \"\\n\"\n",
    "            latex_table += f\"& {MODEL2SH.get(model, model):<10s}\"\n",
    "            for i_dataset, dataset in enumerate(TEST_DATASETS):\n",
    "                latex_table += \" &\"\n",
    "                filter1 = {\n",
    "                    \"model\": model,\n",
    "                    \"dataset\": dataset,\n",
    "                    \"clusterer\": clusterer,\n",
    "                }\n",
    "                filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "                filter2.update(filter1)\n",
    "                filter2.update(override_fields)\n",
    "                filter2 = fixup_filter(filter2)\n",
    "                sdf = select_rows(test_runs_df, filter2, allow_missing=False)\n",
    "                if len(sdf) < 1:\n",
    "                    # print(f\"No data for {filter2}\")\n",
    "                    if clusterer == \"AffinityPropagation\" and dataset in [\n",
    "                        \"imagenet\",\n",
    "                        \"inaturalist\",\n",
    "                    ]:\n",
    "                        continue\n",
    "                        pass\n",
    "                    cmds.append(filter2command(filter2, partition=\"test\"))\n",
    "                    continue\n",
    "                if len(sdf) > 1:\n",
    "                    if sum(np.abs(sdf[metric_key] - sdf.iloc[0][metric_key]) > 1e-6) > 0:\n",
    "                        print()\n",
    "                        print(f\"More than one result with {metric_key} values\", list(sdf[metric_key]))\n",
    "                        print(f\"for search {filter2}\")\n",
    "                        dif_cols = find_differing_columns(sdf, config_keys)\n",
    "                        print(f\"columns which differ: {dif_cols}\")\n",
    "                        if dif_cols:\n",
    "                            for col in dif_cols:\n",
    "                                print(f\"  {col}: {list(sdf[col])}\")\n",
    "                my_val = np.median(sdf[metric_key])\n",
    "                if dummy:\n",
    "                    best_results[dataset].append(my_val)\n",
    "                    continue\n",
    "                is_best = my_val + eps >= np.max(best_results[dataset])\n",
    "                if len(best_results[dataset]) > 1:\n",
    "                    is_secd = my_val + eps >= np.sort(best_results[dataset])[-2]\n",
    "                else:\n",
    "                    is_secd = False\n",
    "                if show_pc:\n",
    "                    my_val = my_val * 100\n",
    "                latex_table += \" $\"\n",
    "                if is_best:\n",
    "                    latex_table += r\"\\tcf{\"\n",
    "                elif is_secd:\n",
    "                    latex_table += r\"\\tcs{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                latex_table += show_fmt.format(my_val)\n",
    "                latex_table += r\"}\" if is_best or is_secd else \" \"\n",
    "                latex_table += \"$\"\n",
    "            latex_table += r\" \\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\bottomrule\" + \"\\n\"\n",
    "    latex_table += r\"\\end{tabular}\" + \"\\n\"\n",
    "    latex_table += r\"}\" + \"\\n\"\n",
    "\n",
    "print()\n",
    "print(f\"There are {len(cmds)} commands to execute to generate missing datapoints\")\n",
    "if show_commands:\n",
    "    for cmd in cmds:\n",
    "        print(cmd)\n",
    "\n",
    "print()\n",
    "print(\"Done!\")\n",
    "print()\n",
    "print(f\"Here is your results table for {clusterer}:\")\n",
    "print()\n",
    "print()\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-9lxUOUuhaj"
   },
   "source": [
    "## Grouping by encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "n8sCItDK_2VT",
    "outputId": "18c89619-30d7-44fb-8f43-897901b62704",
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric_key = \"AMI\"\n",
    "show_pc = True\n",
    "show_fmt = \"{:4.0f}\"\n",
    "show_commands = False\n",
    "eps = 0.001\n",
    "override_fields = {\n",
    "    # \"predictions_dir\": \"y_pred\",\n",
    "}\n",
    "\n",
    "backbone = \"ResNet-50\"\n",
    "\n",
    "CLUSTERERS = [\n",
    "    \"KMeans\",\n",
    "    \"AgglomerativeClustering\",\n",
    "    \"AgglomerativeClustering\",\n",
    "    \"AffinityPropagation\",\n",
    "    \"HDBSCAN\",\n",
    "]\n",
    "print(MODEL2SH)\n",
    "\n",
    "best_results = {k: [] for k in TEST_DATASETS}\n",
    "for dummy in [True, False]:\n",
    "    cmds = []\n",
    "    latex_table = r\"% Results for \" + f\"{metric_key}, {backbone}\" + \"\\n\"\n",
    "    now_str = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    latex_table += r\"% Generated \" + now_str + \"\\n\"\n",
    "    latex_table += r\"% Using hparams \" + BEST_PARAMS[\"_version\"] + \"\\n\"\n",
    "    latex_table += r\"\\label{tab:\" + backbone + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"\\resizebox{\\textwidth}{!}{%\" + \"\\n\"\n",
    "    latex_table += r\"\\begin{tabular}{ll\" + r\"r\" * len(TEST_DATASETS) + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    latex_table += r\"& \" + f\"{'Clusterer':<11s}\"\n",
    "    for dataset in TEST_DATASETS:\n",
    "        latex_table += r\"&\" + \"{:^15s}\".format(DATASET2SH.get(dataset, dataset))\n",
    "    latex_table += r\"\\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    print(MODEL_GROUPS[backbone])\n",
    "    for i_group, model in enumerate(list(MODEL_GROUPS[backbone])):\n",
    "        print(model)\n",
    "        if i_group > 0:\n",
    "            latex_table += r\"\\midrule\" + \"\\n\"\n",
    "\n",
    "        first_agg = True\n",
    "        for i_clusterer, clusterer in enumerate(CLUSTERERS):\n",
    "            if i_clusterer == 0:\n",
    "                latex_table += (\n",
    "                    r\"\\parbox[t]{2mm}{\\multirow{5}{*}{\\rotatebox[origin=c]{90}{\"\n",
    "                    + MODEL2SH[model]\n",
    "                    + \"}}}\"\n",
    "                )\n",
    "                latex_table += \"\\n\"\n",
    "            clusterername = CLUSTERER2SH.get(clusterer, clusterer)\n",
    "\n",
    "            my_override_fields = override_fields.copy()\n",
    "            if (\n",
    "                first_agg\n",
    "                and clusterer == \"AgglomerativeClustering\"\n",
    "                and metric_key != \"num_cluster_pred\"\n",
    "            ):\n",
    "                first_agg = False\n",
    "                my_override_fields[\"aggclust_dist_thresh\"] = None\n",
    "                clusterername = \"AC  w/ C\"\n",
    "            elif clusterer == \"AgglomerativeClustering\":\n",
    "                clusterername = \"AC w/o C\"\n",
    "                if \"aggclust_dist_thresh\" in my_override_fields:\n",
    "                    del my_override_fields[\"aggclust_dist_thresh\"]\n",
    "\n",
    "            if clusterer == \"HDBSCAN\" and dataset in [\"celeba\", \"utkface\"]:\n",
    "                my_override_fields[\"min_samples\"] = 2\n",
    "            elif \"min_samples\" in my_override_fields:\n",
    "                del my_override_fields[\"min_samples\"]\n",
    "\n",
    "            latex_table += f\"& {clusterername:<10s}\"\n",
    "            for i_dataset, dataset in enumerate(TEST_DATASETS):\n",
    "                latex_table += \" &\"\n",
    "                filter1 = {\n",
    "                    \"model\": model,\n",
    "                    \"dataset\": dataset,\n",
    "                    \"clusterer\": clusterer,\n",
    "                }\n",
    "                filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "                filter2.update(filter1)\n",
    "                filter2.update(my_override_fields)\n",
    "                filter2 = fixup_filter(filter2)\n",
    "                sdf = select_rows(test_runs_df, filter2, allow_missing=False)\n",
    "                if len(sdf) < 1:\n",
    "                    # print(f\"No data for {filter2}\")\n",
    "                    cmds.append(filter2command(filter2, partition=\"test\"))\n",
    "                    continue\n",
    "                if len(sdf) > 1:\n",
    "                    if sum(np.abs(sdf[metric_key] - sdf.iloc[0][metric_key]) > 1e-6) > 0:\n",
    "                        print()\n",
    "                        print(f\"More than one result with {metric_key} values\", list(sdf[metric_key]))\n",
    "                        print(f\"for search {filter2}\")\n",
    "                        dif_cols = find_differing_columns(sdf, config_keys)\n",
    "                        print(f\"columns which differ: {dif_cols}\")\n",
    "                        if dif_cols:\n",
    "                            for col in dif_cols:\n",
    "                                print(f\"  {col}: {list(sdf[col])}\")\n",
    "                my_val = np.median(sdf[metric_key])\n",
    "                if dummy:\n",
    "                    best_results[dataset].append(my_val)\n",
    "                    continue\n",
    "                is_best = my_val + eps >= np.max(best_results[dataset])\n",
    "                if len(best_results[dataset]) > 1:\n",
    "                    is_secd = my_val + eps >= np.sort(best_results[dataset])[-2]\n",
    "                else:\n",
    "                    is_secd = False\n",
    "                if show_pc:\n",
    "                    my_val = my_val * 100\n",
    "                latex_table += \" $\"\n",
    "                if is_best:\n",
    "                    latex_table += r\"\\tcf{\"\n",
    "                elif is_secd:\n",
    "                    latex_table += r\"\\tcs{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                latex_table += show_fmt.format(my_val)\n",
    "                latex_table += r\"}\" if is_best or is_secd else \" \"\n",
    "                latex_table += \"$\"\n",
    "            latex_table += r\" \\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\bottomrule\" + \"\\n\"\n",
    "    latex_table += r\"\\end{tabular}\" + \"\\n\"\n",
    "    latex_table += r\"}\" + \"\\n\"\n",
    "\n",
    "print()\n",
    "print(f\"There are {len(cmds)} commands to execute to generate missing datapoints\")\n",
    "if show_commands:\n",
    "    for cmd in cmds:\n",
    "        print(cmd)\n",
    "\n",
    "print()\n",
    "print(\"Done!\")\n",
    "print()\n",
    "print(f\"Here is your results table for {clusterer}:\")\n",
    "print()\n",
    "print()\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_isFUjNsuYc4"
   },
   "source": [
    "## Grouping by clusterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "MyWQRN9iqBkI",
    "outputId": "2642efe3-b0ef-46f3-dc70-f2f5c69645d4"
   },
   "outputs": [],
   "source": [
    "metric_key = \"AMI\"  # AMI  num_cluster_pred  silhouette-euclidean_pred  silhouette-og-euclidean_pred\n",
    "show_pc = True\n",
    "show_fmt = \"{:4.0f}\"\n",
    "show_commands = False\n",
    "highlight_best = True\n",
    "use_si_num = False\n",
    "eps = 0.005\n",
    "override_fields = {\n",
    "    # \"predictions_dir\": \"y_pred\",\n",
    "}\n",
    "\n",
    "backbone = \"ResNet-50\"  # \"ResNet-50\" or \"ViT-B\"\n",
    "\n",
    "if metric_key == \"num_cluster_pred\":\n",
    "    CLUSTERERS = [\"AC w/o C\", \"AffinityPropagation\", \"HDBSCAN\"]\n",
    "    show_pc = False\n",
    "    show_fmt = \"{:4.0f}\"\n",
    "    highlight_best = False\n",
    "    use_si_num = True\n",
    "else:\n",
    "    CLUSTERERS = [\"KMeans\", \"AC w/ C\", \"AC w/o C\", \"AffinityPropagation\", \"HDBSCAN\"]\n",
    "if metric_key.startswith(\"silhouette\"):\n",
    "    show_pc = False\n",
    "    show_fmt = \"{:5.2f}\"\n",
    "\n",
    "print(MODEL_GROUPS)\n",
    "\n",
    "best_results = {k: [] for k in TEST_DATASETS}\n",
    "best_results_grouped = {k: defaultdict(list) for k in TEST_DATASETS}\n",
    "\n",
    "for dummy in [True, False]:\n",
    "    cmds = []\n",
    "    latex_table = r\"% Results for \" + f\"{metric_key}, {backbone}\" + \"\\n\"\n",
    "    now_str = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    latex_table += r\"% Generated \" + now_str + \"\\n\"\n",
    "    latex_table += r\"% Using hparams \" + BEST_PARAMS[\"_version\"] + \"\\n\"\n",
    "    label = backbone\n",
    "    if metric_key == \"AMI\":\n",
    "        latex_table += r\"\\label{tab:\" + label + r\"}\" + \"\\n\"\n",
    "    label = metric_key.replace(\"_\", \"-\") + \":\" + label\n",
    "    latex_table += r\"\\label{tab:\" + label + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"\\resizebox{\\textwidth}{!}{%\" + \"\\n\"\n",
    "    latex_table += r\"\\begin{tabular}{ll\" + r\"r\" * len(TEST_DATASETS) + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    latex_table += r\"& \" + f\"{'Encoder':<11s}\"\n",
    "    for dataset in TEST_DATASETS:\n",
    "        latex_table += r\"&\" + \"{:^15s}\".format(DATASET2SH.get(dataset, dataset))\n",
    "    latex_table += r\"\\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    print(MODEL_GROUPS[backbone])\n",
    "    if metric_key == \"num_cluster_pred\":\n",
    "        latex_table += r\"& Num targets\"\n",
    "        for i_dataset, dataset in enumerate(TEST_DATASETS):\n",
    "            sdf = select_rows(test_runs_df, {\"dataset\": dataset}, allow_missing=False)\n",
    "            sdf = sdf[~pd.isna(sdf[\"num_cluster_true\"])]\n",
    "            latex_table += r\"& \"\n",
    "            latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "            latex_table += f\"{sdf.iloc[0]['num_cluster_true'].item()}\"\n",
    "            latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "        latex_table += r\"\\\\\" + \"\\n\"\n",
    "        latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    elif metric_key.endswith(\"_pred\"):\n",
    "        metric_key2 = metric_key.replace(\"_pred\", \"_true\")\n",
    "        clusterername = \"G.T.\"\n",
    "        latex_table += (\n",
    "            r\"\\parbox[t]{2mm}{\\multirow{\"\n",
    "            + str(len(MODEL_GROUPS[backbone]))\n",
    "            + r\"}{*}{\"\n",
    "            + r\"\\scalebox{0.9}{\"\n",
    "            + r\"\\rotatebox[origin=c]{90}{\"\n",
    "            + clusterername\n",
    "            + r\"}}}\"\n",
    "            + r\"}\"\n",
    "        )\n",
    "        latex_table += \"\\n\"\n",
    "        for i_group, model in enumerate(list(MODEL_GROUPS[backbone])):\n",
    "            latex_table += f\"& {MODEL2SH[model]:<10s}\"\n",
    "            for i_dataset, dataset in enumerate(TEST_DATASETS):\n",
    "                latex_table += \" &\"\n",
    "                filter1 = {\"model\": model, \"dataset\": dataset}\n",
    "                if model == \"timm_vit_base_patch16_224.mae\":\n",
    "                    filter1[\"dim_reducer\"] = \"PCA\"\n",
    "                    filter1[\"pca_variance\"] = 0.95\n",
    "                else:\n",
    "                    filter1[\"dim_reducer_man\"] = \"UMAP\"\n",
    "                    filter1[\"ndim_reduced_man\"] = 50\n",
    "                    filter1[\"dim_reducer_man_metric\"] = \"euclidean\"\n",
    "                sdf = select_rows(test_runs_df, filter1, allow_missing=False)\n",
    "                sdf = sdf[~pd.isna(sdf[metric_key2])]\n",
    "                my_val = np.nanmedian(sdf[metric_key])\n",
    "                if sum(sdf[metric_key2] != my_val) > 0:\n",
    "                    pass\n",
    "                if dummy:\n",
    "                    best_results_grouped[dataset][clusterername].append(my_val)\n",
    "                    continue\n",
    "                is_best_grp = my_val + eps >= np.max(\n",
    "                    best_results_grouped[dataset][clusterername]\n",
    "                )\n",
    "                latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "                latex_table += \"     \"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best_grp:\n",
    "                    latex_table += r\"\\tcg{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                latex_table += show_fmt.format(my_val)\n",
    "                if highlight_best:\n",
    "                    latex_table += r\"}\" if is_best_grp else \" \"\n",
    "                latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "\n",
    "            latex_table += r\" \\\\\" + \"\\n\"\n",
    "        latex_table += r\"\\toprule\" + \"\\n\"\n",
    "\n",
    "    first_agg = True\n",
    "    for i_clusterer, clusterer in enumerate(CLUSTERERS):\n",
    "        clusterername = CLUSTERER2SH.get(clusterer, clusterer)\n",
    "        my_override_fields = override_fields.copy()\n",
    "        if (\n",
    "            first_agg\n",
    "            and clusterer == \"AgglomerativeClustering\"\n",
    "            and metric_key != \"num_cluster_pred\"\n",
    "        ):\n",
    "            first_agg = False\n",
    "            my_override_fields[\"aggclust_dist_thresh\"] = None\n",
    "            clusterername = \"AC  w/ C\"\n",
    "        elif clusterer == \"AgglomerativeClustering\":\n",
    "            clusterername = \"AC w/o C\"\n",
    "            if \"aggclust_dist_thresh\" in my_override_fields:\n",
    "                del my_override_fields[\"aggclust_dist_thresh\"]\n",
    "\n",
    "        if i_clusterer > 0:\n",
    "            latex_table += r\"\\midrule\" + \"\\n\"\n",
    "\n",
    "        latex_table += (\n",
    "            r\"\\parbox[t]{2mm}{\\multirow{\"\n",
    "            + str(len(MODEL_GROUPS[backbone]))\n",
    "            + r\"}{*}{\"\n",
    "            + r\"\\scalebox{0.9}{\"\n",
    "            + r\"\\rotatebox[origin=c]{90}{\"\n",
    "            + clusterername\n",
    "            + r\"}}}\"\n",
    "            + r\"}\"\n",
    "        )\n",
    "        latex_table += \"\\n\"\n",
    "\n",
    "        for i_group, model in enumerate(list(MODEL_GROUPS[backbone])):\n",
    "            latex_table += f\"& {MODEL2SH[model]:<10s}\"\n",
    "            for i_dataset, dataset in enumerate(TEST_DATASETS):\n",
    "                latex_table += \" &\"\n",
    "                filter1 = {\"model\": model, \"dataset\": dataset}\n",
    "                filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "                filter2.update(filter1)\n",
    "                filter2.update(my_override_fields)\n",
    "                filter2 = fixup_filter(filter2)\n",
    "                sdf = select_rows(test_runs_df, filter2, allow_missing=False)\n",
    "                if len(sdf) < 1:\n",
    "                    # print(f\"No data for {model}-{dataset}-{clusterer}\\n{filter2}\")\n",
    "                    cmds.append(filter2command(filter2, partition=\"test\"))\n",
    "                    if not dummy:\n",
    "                        # latex_table += r\"\\multicolumn{1}{c}{--}\"\n",
    "                        latex_table += r\"   --  \"\n",
    "                    continue\n",
    "                if len(sdf) > 1:\n",
    "                    if sum(np.abs(sdf[metric_key] - sdf.iloc[0][metric_key]) > 1e-6) > 0:\n",
    "                        print()\n",
    "                        print(f\"More than one result with {metric_key} values\", list(sdf[metric_key]))\n",
    "                        print(f\"for search {filter2}\")\n",
    "                        dif_cols = find_differing_columns(sdf, config_keys)\n",
    "                        print(f\"columns which differ: {dif_cols}\")\n",
    "                        if dif_cols:\n",
    "                            for col in dif_cols:\n",
    "                                print(f\"  {col}: {list(sdf[col])}\")\n",
    "                my_val = np.nanmedian(sdf[metric_key])\n",
    "                if dummy:\n",
    "                    best_results[dataset].append(my_val)\n",
    "                    best_results_grouped[dataset][clusterername].append(my_val)\n",
    "                    continue\n",
    "                if np.isnan(my_val):\n",
    "                    latex_table += r\"   --  \"\n",
    "                    continue\n",
    "                is_best = my_val + eps >= np.max(best_results[dataset])\n",
    "                if len(best_results[dataset]) > 1:\n",
    "                    is_secd = my_val + eps >= np.sort(best_results[dataset])[-2]\n",
    "                else:\n",
    "                    is_secd = False\n",
    "                is_best_grp = my_val + eps >= np.max(\n",
    "                    best_results_grouped[dataset][clusterername]\n",
    "                )\n",
    "                sc_base = np.nanmedian(best_results[dataset])\n",
    "                sc_top = np.max(best_results[dataset])\n",
    "                sc = 100 * max(0, (my_val - sc_base) / (sc_top - sc_base))\n",
    "                latex_table += r\"\\cellcolor{cbg!\" + f\"{sc:.0f}\" + \"}\"\n",
    "                if show_pc:\n",
    "                    my_val = my_val * 100\n",
    "                latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best:\n",
    "                    latex_table += r\"\\tcf{\"\n",
    "                elif is_secd:\n",
    "                    latex_table += r\"\\tcs{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best_grp:\n",
    "                    latex_table += r\"\\tcg{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                latex_table += show_fmt.format(my_val)\n",
    "                if highlight_best:\n",
    "                    latex_table += r\"}\" if is_best or is_secd else \" \"\n",
    "                    latex_table += r\"}\" if is_best_grp else \" \"\n",
    "                latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "            latex_table += r\" \\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\bottomrule\" + \"\\n\"\n",
    "    latex_table += r\"\\end{tabular}\" + \"\\n\"\n",
    "    latex_table += r\"}\" + \"\\n\"\n",
    "\n",
    "print()\n",
    "print(f\"There are {len(cmds)} commands to execute to generate missing datapoints\")\n",
    "if show_commands:\n",
    "    for cmd in cmds:\n",
    "        print(cmd)\n",
    "\n",
    "print()\n",
    "print(\"Done!\")\n",
    "print()\n",
    "print(f\"Here is your results table for {metric_key}, {backbone}:\")\n",
    "print()\n",
    "print()\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cmd in cmds:\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With grouped datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "MyWQRN9iqBkI",
    "outputId": "2642efe3-b0ef-46f3-dc70-f2f5c69645d4"
   },
   "outputs": [],
   "source": [
    "metric_key = \"AMI\"  # AMI  num_cluster_pred  silhouette-euclidean_pred  silhouette-og-euclidean_pred\n",
    "show_pc = True\n",
    "show_fmt = \"{:4.0f}\"\n",
    "show_commands = False\n",
    "highlight_best = True\n",
    "use_si_num = False\n",
    "eps = 0.005\n",
    "override_fields = {\n",
    "    # \"predictions_dir\": \"y_pred\",\n",
    "}\n",
    "\n",
    "backbone = \"ViT-B\"  # \"ResNet-50\" or \"ViT-B\"\n",
    "\n",
    "if metric_key == \"num_cluster_pred\":\n",
    "    CLUSTERERS = [\"AC w/o C\", \"AffinityPropagation\", \"HDBSCAN\"]\n",
    "    show_pc = False\n",
    "    show_fmt = \"{:4.0f}\"\n",
    "    highlight_best = False\n",
    "    use_si_num = True\n",
    "else:\n",
    "    CLUSTERERS = [\"KMeans\", \"AC w/ C\", \"AC w/o C\", \"AffinityPropagation\", \"HDBSCAN\"]\n",
    "if metric_key.startswith(\"silhouette\"):\n",
    "    show_pc = False\n",
    "    show_fmt = \"{:5.2f}\"\n",
    "\n",
    "print(MODEL_GROUPS)\n",
    "\n",
    "test_datasets = []\n",
    "for datagroupname, datagroupset in TEST_DATASETS_GROUPED.items():\n",
    "    test_datasets.extend(datagroupset)\n",
    "\n",
    "best_results = {k: [] for k in test_datasets}\n",
    "best_results_grouped = {k: defaultdict(list) for k in test_datasets}\n",
    "\n",
    "for dummy in [True, False]:\n",
    "    cmds = []\n",
    "    latex_table = r\"% Results for \" + f\"{metric_key}, {backbone}\" + \"\\n\"\n",
    "    now_str = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    latex_table += r\"% Generated \" + now_str + \"\\n\"\n",
    "    latex_table += r\"% Using hparams \" + BEST_PARAMS[\"_version\"] + \"\\n\"\n",
    "    label = backbone\n",
    "    if metric_key == \"AMI\":\n",
    "        latex_table += r\"\\label{tab:\" + label + r\"}\" + \"\\n\"\n",
    "    label = metric_key.replace(\"_\", \"-\") + \":\" + label\n",
    "    latex_table += r\"\\label{tab:\" + label + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"\\resizebox{\\textwidth}{!}{%\" + \"\\n\"\n",
    "    latex_table += r\"\\begin{tabular}{ll\" + r\"r\" * len(test_datasets) + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    latex_table += r\"& \" + f\"{'':<11s}\"\n",
    "    for datagroupname, datagroupset in TEST_DATASETS_GROUPED.items():\n",
    "        latex_table += r\" & \\multicolumn{\" + str(len(datagroupset)) + r\"}{c}{\" + datagroupname + r\"}\"\n",
    "    latex_table += r\"\\\\\" + \"\\n\"\n",
    "    icol = 3\n",
    "    for datagroupname, datagroupset in TEST_DATASETS_GROUPED.items():\n",
    "        latex_table += r\"\\cmidrule(l){\" + f\"{icol}-{icol + len(datagroupset) - 1}\" + r\"}\"\n",
    "        icol += len(datagroupset)\n",
    "    latex_table += \"\\n\"\n",
    "    latex_table += r\"& \" + f\"{'Encoder':<11s}\"\n",
    "    for dataset in test_datasets:\n",
    "        latex_table += r\"&\" + \"{:^15s}\".format(DATASET2SH.get(dataset, dataset))\n",
    "    latex_table += r\"\\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    print(MODEL_GROUPS[backbone])\n",
    "    if metric_key == \"num_cluster_pred\":\n",
    "        latex_table += r\"& Num targets\"\n",
    "        for i_dataset, dataset in enumerate(test_datasets):\n",
    "            sdf = select_rows(test_runs_df, {\"dataset\": dataset}, allow_missing=False)\n",
    "            sdf = sdf[~pd.isna(sdf[\"num_cluster_true\"])]\n",
    "            latex_table += r\"& \"\n",
    "            latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "            latex_table += f\"{sdf.iloc[0]['num_cluster_true'].item()}\"\n",
    "            latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "        latex_table += r\"\\\\\" + \"\\n\"\n",
    "        latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    elif metric_key.endswith(\"_pred\"):\n",
    "        metric_key2 = metric_key.replace(\"_pred\", \"_true\")\n",
    "        clusterername = \"G.T.\"\n",
    "        latex_table += (\n",
    "            r\"\\parbox[t]{2mm}{\\multirow{\"\n",
    "            + str(len(MODEL_GROUPS[backbone]))\n",
    "            + r\"}{*}{\"\n",
    "            + r\"\\scalebox{0.9}{\"\n",
    "            + r\"\\rotatebox[origin=c]{90}{\"\n",
    "            + clusterername\n",
    "            + r\"}}}\"\n",
    "            + r\"}\"\n",
    "        )\n",
    "        latex_table += \"\\n\"\n",
    "        for i_group, model in enumerate(list(MODEL_GROUPS[backbone])):\n",
    "            latex_table += f\"& {MODEL2SH[model]:<10s}\"\n",
    "            for i_dataset, dataset in enumerate(test_datasets):\n",
    "                latex_table += \" &\"\n",
    "                filter1 = {\"model\": model, \"dataset\": dataset}\n",
    "                if model == \"timm_vit_base_patch16_224.mae\":\n",
    "                    filter1[\"dim_reducer\"] = \"PCA\"\n",
    "                    filter1[\"pca_variance\"] = 0.95\n",
    "                else:\n",
    "                    filter1[\"dim_reducer_man\"] = \"UMAP\"\n",
    "                    filter1[\"ndim_reduced_man\"] = 50\n",
    "                    filter1[\"dim_reducer_man_metric\"] = \"euclidean\"\n",
    "                sdf = select_rows(test_runs_df, filter1, allow_missing=False)\n",
    "                sdf = sdf[~pd.isna(sdf[metric_key2])]\n",
    "                my_val = np.nanmedian(sdf[metric_key])\n",
    "                if sum(sdf[metric_key2] != my_val) > 0:\n",
    "                    pass\n",
    "                if dummy:\n",
    "                    best_results_grouped[dataset][clusterername].append(my_val)\n",
    "                    continue\n",
    "                is_best_grp = my_val + eps >= np.max(\n",
    "                    best_results_grouped[dataset][clusterername]\n",
    "                )\n",
    "                latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "                latex_table += \"     \"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best_grp:\n",
    "                    latex_table += r\"\\tcg{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                latex_table += show_fmt.format(my_val)\n",
    "                if highlight_best:\n",
    "                    latex_table += r\"}\" if is_best_grp else \" \"\n",
    "                latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "\n",
    "            latex_table += r\" \\\\\" + \"\\n\"\n",
    "        latex_table += r\"\\toprule\" + \"\\n\"\n",
    "\n",
    "    first_agg = True\n",
    "    for i_clusterer, clusterer in enumerate(CLUSTERERS):\n",
    "        clusterername = CLUSTERER2SH.get(clusterer, clusterer)\n",
    "        my_override_fields = override_fields.copy()\n",
    "        if (\n",
    "            first_agg\n",
    "            and clusterer == \"AgglomerativeClustering\"\n",
    "            and metric_key != \"num_cluster_pred\"\n",
    "        ):\n",
    "            first_agg = False\n",
    "            my_override_fields[\"aggclust_dist_thresh\"] = None\n",
    "            clusterername = \"AC  w/ C\"\n",
    "        elif clusterer == \"AgglomerativeClustering\":\n",
    "            clusterername = \"AC w/o C\"\n",
    "            if \"aggclust_dist_thresh\" in my_override_fields:\n",
    "                del my_override_fields[\"aggclust_dist_thresh\"]\n",
    "\n",
    "        if i_clusterer > 0:\n",
    "            latex_table += r\"\\midrule\" + \"\\n\"\n",
    "\n",
    "        latex_table += (\n",
    "            r\"\\parbox[t]{2mm}{\\multirow{\"\n",
    "            + str(len(MODEL_GROUPS[backbone]))\n",
    "            + r\"}{*}{\"\n",
    "            + r\"\\scalebox{0.9}{\"\n",
    "            + r\"\\rotatebox[origin=c]{90}{\"\n",
    "            + clusterername\n",
    "            + r\"}}}\"\n",
    "            + r\"}\"\n",
    "        )\n",
    "        latex_table += \"\\n\"\n",
    "\n",
    "        for i_group, model in enumerate(list(MODEL_GROUPS[backbone])):\n",
    "            latex_table += f\"& {MODEL2SH[model]:<10s}\"\n",
    "            for i_dataset, dataset in enumerate(test_datasets):\n",
    "                latex_table += \" &\"\n",
    "                filter1 = {\"model\": model, \"dataset\": dataset}\n",
    "                filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "                filter2.update(filter1)\n",
    "                filter2.update(my_override_fields)\n",
    "                filter2 = fixup_filter(filter2)\n",
    "                sdf = select_rows(test_runs_df, filter2, allow_missing=False)\n",
    "                if len(sdf) < 1:\n",
    "                    # print(f\"No data for {model}-{dataset}-{clusterer}\\n{filter2}\")\n",
    "                    cmds.append(filter2command(filter2, partition=\"test\"))\n",
    "                    if not dummy:\n",
    "                        # latex_table += r\"\\multicolumn{1}{c}{--}\"\n",
    "                        latex_table += r\"   --  \"\n",
    "                    continue\n",
    "                if len(sdf) > 1:\n",
    "                    if sum(np.abs(sdf[metric_key] - sdf.iloc[0][metric_key]) > 1e-6) > 0:\n",
    "                        print()\n",
    "                        print(f\"More than one result with {metric_key} values\", list(sdf[metric_key]))\n",
    "                        print(f\"for search {filter2}\")\n",
    "                        dif_cols = find_differing_columns(sdf, config_keys)\n",
    "                        print(f\"columns which differ: {dif_cols}\")\n",
    "                        if dif_cols:\n",
    "                            for col in dif_cols:\n",
    "                                print(f\"  {col}: {list(sdf[col])}\")\n",
    "                my_val = np.nanmedian(sdf[metric_key])\n",
    "                if dummy:\n",
    "                    best_results[dataset].append(my_val)\n",
    "                    best_results_grouped[dataset][clusterername].append(my_val)\n",
    "                    continue\n",
    "                if np.isnan(my_val):\n",
    "                    latex_table += r\"   --  \"\n",
    "                    continue\n",
    "                is_best = my_val + eps >= np.max(best_results[dataset])\n",
    "                if len(best_results[dataset]) > 1:\n",
    "                    is_secd = my_val + eps >= np.sort(best_results[dataset])[-2]\n",
    "                else:\n",
    "                    is_secd = False\n",
    "                is_best_grp = my_val + eps >= np.max(\n",
    "                    best_results_grouped[dataset][clusterername]\n",
    "                )\n",
    "                sc_base = np.nanmedian(best_results[dataset])\n",
    "                sc_top = np.max(best_results[dataset])\n",
    "                sc = 100 * max(0, (my_val - sc_base) / (sc_top - sc_base))\n",
    "                latex_table += r\"\\cellcolor{cbg!\" + f\"{sc:.0f}\" + \"}\"\n",
    "                if show_pc:\n",
    "                    my_val = my_val * 100\n",
    "                latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best:\n",
    "                    latex_table += r\"\\tcf{\"\n",
    "                elif is_secd:\n",
    "                    latex_table += r\"\\tcs{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best_grp:\n",
    "                    latex_table += r\"\\tcg{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                latex_table += show_fmt.format(my_val)\n",
    "                if highlight_best:\n",
    "                    latex_table += r\"}\" if is_best or is_secd else \" \"\n",
    "                    latex_table += r\"}\" if is_best_grp else \" \"\n",
    "                latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "            latex_table += r\" \\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\bottomrule\" + \"\\n\"\n",
    "    latex_table += r\"\\end{tabular}\" + \"\\n\"\n",
    "    latex_table += r\"}\" + \"\\n\"\n",
    "\n",
    "print()\n",
    "print(f\"There are {len(cmds)} commands to execute to generate missing datapoints\")\n",
    "if show_commands:\n",
    "    for cmd in cmds:\n",
    "        print(cmd)\n",
    "\n",
    "print()\n",
    "print(\"Done!\")\n",
    "print()\n",
    "print(f\"Here is your results table for {metric_key}, {backbone}:\")\n",
    "print()\n",
    "print()\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWpBwhP8MOm0"
   },
   "source": [
    "## Correlation between AMI and SIlhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Gp5BaVDxG2Il",
    "outputId": "2ea12a0e-f294-47fa-8830-d59fbfd547b5"
   },
   "outputs": [],
   "source": [
    "metric_key1 = \"silhouette-og-euclidean_pred\"  # silhouette-euclidean_pred | silhouette-og-euclidean_pred\n",
    "metric_key2 = \"AMI\"\n",
    "\n",
    "override_fields = {}\n",
    "\n",
    "backbones = [\"ResNet-50\", \"ViT-B\"]\n",
    "CLUSTERERS = [\"KMeans\", \"AC w/ C\", \"AC w/o C\", \"AffinityPropagation\", \"HDBSCAN\"]\n",
    "\n",
    "fig, ax = plt.subplots(1, len(backbones), sharey=True, figsize=(6, 3))\n",
    "\n",
    "for i_backbone, backbone in enumerate(backbones):\n",
    "    my_valx_method = {clusterer: [] for clusterer in CLUSTERERS}\n",
    "    my_valy_method = {clusterer: [] for clusterer in CLUSTERERS}\n",
    "\n",
    "    print(backbone)\n",
    "    print(CLUSTERERS)\n",
    "    print(TEST_DATASETS)\n",
    "    print(MODEL_GROUPS[backbone])\n",
    "    print()\n",
    "\n",
    "    for i_clusterer, clusterer in enumerate(CLUSTERERS):\n",
    "        for i_dataset, dataset in enumerate(TEST_DATASETS):\n",
    "            for i_model, model in enumerate(list(MODEL_GROUPS[backbone])):\n",
    "                if model == \"timm_vit_base_patch16_224.mae\":\n",
    "                    # print(f\"Skipping {model}\")\n",
    "                    # continue\n",
    "                    pass\n",
    "                filter1 = {\"model\": model, \"dataset\": dataset}\n",
    "                filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "                filter2.update(filter1)\n",
    "                filter2.update(override_fields)\n",
    "                filter2 = fixup_filter(filter2)\n",
    "                sdf = select_rows(test_runs_df, filter2, allow_missing=False)\n",
    "                my_valx_method[clusterer].append(np.nanmedian(sdf[metric_key1]))\n",
    "                my_valy_method[clusterer].append(np.nanmedian(sdf[metric_key2]))\n",
    "\n",
    "    my_valx_method = {k: np.array(v) for k, v in my_valx_method.items()}\n",
    "    my_valy_method = {k: np.array(v) for k, v in my_valy_method.items()}\n",
    "    my_valx_overall = np.concatenate([my_valx_method[clusterer] for clusterer in CLUSTERERS])\n",
    "    my_valy_overall = np.concatenate([my_valy_method[clusterer] for clusterer in CLUSTERERS])\n",
    "    my_cols = np.concatenate(\n",
    "        [\n",
    "            np.tile(CLUSTERER2COLORRGB.get(clusterer, \"k\"), [len(my_valx_method[clusterer]), 1])\n",
    "            for clusterer in CLUSTERERS\n",
    "        ]\n",
    "    )\n",
    "    indices = np.arange(len(my_valx_overall))\n",
    "    np.random.shuffle(indices)\n",
    "    ax[i_backbone].scatter(\n",
    "        my_valx_overall[indices],\n",
    "        my_valy_overall[indices],\n",
    "        color=my_cols[indices],\n",
    "        s=20,\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    ax[i_backbone].set_xlabel(r\"$S$\" if metric_key1.startswith(\"silhouette\") else metric_key1)\n",
    "    if i_backbone == 0:\n",
    "        ax[i_backbone].set_ylabel(metric_key2)\n",
    "    ax[i_backbone].set_xlim(-1.05, 1.05)\n",
    "    ax[i_backbone].set_ylim(-0.05, max(max(my_valy_overall), 0.95))\n",
    "    ax[i_backbone].set_title(backbone)\n",
    "    print(f\"{backbone:<20s} Correlation coef\")\n",
    "    cors = []\n",
    "    for clusterer in CLUSTERERS:\n",
    "        sel = (~np.isnan(my_valx_method[clusterer])) & (~np.isnan(my_valy_method[clusterer]))\n",
    "        cor = np.corrcoef(my_valx_method[clusterer][sel], my_valy_method[clusterer][sel])[0, 1]\n",
    "        cors.append(cor)\n",
    "        print(f\"{clusterer:<20s} {cor:.4f}\")\n",
    "    print(f\"{'Average':<20s} {np.nanmean(cors):.4f}\")\n",
    "    sel = (~np.isnan(my_valx_overall)) & (~np.isnan(my_valy_overall))\n",
    "    cor = np.corrcoef(my_valx_overall[sel], my_valy_overall[sel])[0, 1]\n",
    "    print(f\"{'Overall':<20s} {cor:.4f}\")\n",
    "    print()\n",
    "    ax[i_backbone].text(-0.85, 0.85, f\"$r={cor:.2f}$\")\n",
    "    ax[i_backbone].text(-0.85, 0.75, r\"$\\bar{r}=\" + f\"{np.mean(cors):.2f}$\")\n",
    "\n",
    "label_fn = lambda c, marker: plt.plot(  # noqa:E731\n",
    "    [], [], color=c, ls=\"None\", marker=marker, linewidth=6\n",
    ")[0]\n",
    "handles = [label_fn(CLUSTERER2COLORRGB.get(clusterer), \"o\") for clusterer in CLUSTERERS]\n",
    "data_labels = CLUSTERERS\n",
    "ax[1].legend(handles, data_labels, loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "fig.savefig(os.path.join(FIGS_DIR, f\"scatter__{metric_key1}__{metric_key2}.pdf\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "MuURKU9mMXIN",
    "outputId": "72833c72-a908-43cd-a4f9-d0030733fb0d"
   },
   "outputs": [],
   "source": [
    "metric_key1 = \"AMI\"\n",
    "metric_key2 = \"silhouette-euclidean_pred\"\n",
    "\n",
    "override_fields = {}\n",
    "\n",
    "backbones = [\"ResNet-50\", \"ViT-B\"]\n",
    "CLUSTERERS = [\"KMeans\", \"AC w/ C\", \"AC w/o C\", \"AffinityPropagation\", \"HDBSCAN\"]\n",
    "test_datasets = TEST_DATASETS\n",
    "\n",
    "figenc, axenc = plt.subplots(1, 2, figsize=(6, 2))\n",
    "figclus, axclus = plt.subplots(1, 2, figsize=(6, 2))\n",
    "\n",
    "for i_backbone, backbone in enumerate(backbones):\n",
    "    result_table = np.nan * np.ones(\n",
    "        (len(MODEL_GROUPS[backbone]), len(CLUSTERERS), len(test_datasets))\n",
    "    )\n",
    "    for i_group, model in enumerate(list(MODEL_GROUPS[backbone])):\n",
    "        for i_clusterer, clusterer in enumerate(CLUSTERERS):\n",
    "            for i_dataset, dataset in enumerate(test_datasets):\n",
    "                filter1 = {\"model\": model, \"dataset\": dataset}\n",
    "                filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "                filter2.update(filter1)\n",
    "                filter2.update(override_fields)\n",
    "                filter2 = fixup_filter(filter2)\n",
    "                sdf = select_rows(test_runs_df, filter2, allow_missing=False)\n",
    "                if len(sdf) < 1:\n",
    "                    result_table[i_group, i_clusterer, i_dataset] = -100.0\n",
    "                    continue\n",
    "                result_table[i_group, i_clusterer, i_dataset] = np.median(\n",
    "                    sdf[metric_key1]\n",
    "                )\n",
    "\n",
    "    print(backbone)\n",
    "    print(MODEL_GROUPS[backbone])\n",
    "\n",
    "    # RANK PER ENCODER - go through each dataset, look at each clusterer,\n",
    "    # and determine the rank of each encoder in that setting\n",
    "    print(list(MODEL_GROUPS[backbone]))\n",
    "    ranks_encoders = np.nan * np.ones((len(MODEL_GROUPS[backbone]), len(CLUSTERERS), len(test_datasets)))\n",
    "    for i_dataset, dataset in enumerate(test_datasets):\n",
    "        for i_clusterer, clusterer in enumerate(CLUSTERERS):\n",
    "            cluster_data = result_table[:, i_clusterer, i_dataset]\n",
    "            if np.all(cluster_data == cluster_data[0]) or np.all(np.isnan(cluster_data)):\n",
    "                print(f\"Skipping {dataset} {clusterer} (all same)\")\n",
    "                continue\n",
    "            if np.any(cluster_data == -100.0):\n",
    "                print(f\"Skipping {dataset} {clusterer} (incomplete)\")\n",
    "                continue\n",
    "            rank = np.argsort(cluster_data)[::-1]\n",
    "            ranks_encoders[:, i_clusterer, i_dataset] = 1 + rank.argsort()\n",
    "    mean_rank_encoders = np.nanmean(ranks_encoders, axis=(1, 2))\n",
    "    std_rank_encoders = np.nanstd(ranks_encoders, axis=(1, 2))\n",
    "    # order = np.argsort(mean_rank_encoders)\n",
    "    order = np.arange(len(MODEL_GROUPS[backbone]))\n",
    "\n",
    "    for i_plot, i_model in enumerate(order):\n",
    "        axenc[i_backbone].barh(\n",
    "            i_plot,\n",
    "            mean_rank_encoders[i_model],\n",
    "            xerr=std_rank_encoders[i_model],\n",
    "            align=\"center\",\n",
    "            alpha=0.6,\n",
    "            ecolor=\"black\",\n",
    "            color=MODEL2COLORRGB.get(MODEL_GROUPS[backbone][i_model], \"k\"),\n",
    "            capsize=2,\n",
    "            zorder=10,\n",
    "        )\n",
    "\n",
    "    axenc[i_backbone].invert_yaxis()\n",
    "    axenc[i_backbone].set_yticks([])\n",
    "    axenc[i_backbone].set_yticklabels([])\n",
    "    axenc[i_backbone].set_xticks(np.arange(1, 1 + len(MODEL_GROUPS[backbone])))\n",
    "    axenc[i_backbone].set_xlim([0, 0.5 + len(MODEL_GROUPS[backbone])])\n",
    "    axenc[i_backbone].xaxis.grid(True, zorder=1, alpha=0.5)\n",
    "    axenc[i_backbone].set_title(backbone)\n",
    "\n",
    "    # RANK PER CLUSTERER - go through each dataset, look at each encoder,\n",
    "    # and determine the rank of each clusterer in that setting\n",
    "\n",
    "    print(CLUSTERERS)\n",
    "    ranks_clusterers = np.nan * np.ones((len(MODEL_GROUPS[backbone]), len(CLUSTERERS), len(test_datasets)))\n",
    "    for i_dataset, dataset in enumerate(test_datasets):\n",
    "        for i_encoder, encoder in enumerate(MODEL_GROUPS[backbone]):\n",
    "            encoder_data = result_table[i_encoder, :, i_dataset]\n",
    "            if np.all(encoder_data == encoder_data[0]) or np.all(np.isnan(encoder_data)):\n",
    "                print(f\"Skipping {dataset} {encoder} (all same)\")\n",
    "                continue\n",
    "            if np.any(encoder_data == -100.0):\n",
    "                print(f\"Skipping {dataset} {encoder} (incomplete)\")\n",
    "                continue\n",
    "            rank = np.argsort(encoder_data)[::-1]\n",
    "            ranks_clusterers[i_encoder, :, i_dataset] = 1 + rank.argsort()\n",
    "    mean_rank_clusters = np.nanmean(ranks_clusterers, axis=(0, 2))\n",
    "    std_rank_clusters = np.nanstd(ranks_clusterers, axis=(0, 2))\n",
    "    # order = np.argsort(mean_rank_clusters)\n",
    "    order = np.arange(len(CLUSTERERS))\n",
    "\n",
    "    for i_plot, i_clusterer in enumerate(order):\n",
    "        axclus[i_backbone].barh(\n",
    "            i_plot,\n",
    "            mean_rank_clusters[i_clusterer],\n",
    "            xerr=std_rank_clusters[i_clusterer],\n",
    "            align=\"center\",\n",
    "            alpha=0.6,\n",
    "            ecolor=\"black\",\n",
    "            color=CLUSTERER2COLORSTR.get(CLUSTERERS[i_clusterer], \"k\"),\n",
    "            capsize=2,\n",
    "            zorder=10,\n",
    "        )\n",
    "\n",
    "    axclus[i_backbone].invert_yaxis()\n",
    "    axclus[i_backbone].set_yticks([])\n",
    "    axclus[i_backbone].set_yticklabels([])\n",
    "    axclus[i_backbone].set_xticks(np.arange(1, 1 + len(CLUSTERERS)))\n",
    "    axclus[i_backbone].set_xlim([0, 0.6 + len(CLUSTERERS)])\n",
    "    axclus[i_backbone].xaxis.grid(True, zorder=1, alpha=0.5)\n",
    "    axclus[i_backbone].set_title(backbone)\n",
    "\n",
    "    axclus[i_backbone].set_xlabel(\"Rank\")\n",
    "    axenc[i_backbone].set_xlabel(\"Rank\")\n",
    "\n",
    "label_fn = lambda c, ls: plt.plot([], [], color=c, ls=ls, linewidth=3)[0]  # noqa:E731\n",
    "\n",
    "model_names = list(MODEL_GROUPS[\"ResNet-50\"]) + [\"timm_vit_base_patch16_224.mae\"]\n",
    "handles_enc = [label_fn(MODEL2COLORRGB[idx], \"-\") for idx in model_names]\n",
    "axenc[1].legend(\n",
    "    handles_enc,\n",
    "    [MODEL2SH[x] for x in model_names],\n",
    "    loc=\"center left\",\n",
    "    bbox_to_anchor=(1, 0.5),\n",
    ")\n",
    "\n",
    "handles_clus = [label_fn(CLUSTERER2COLORRGB[clusterer], \"-\") for clusterer in CLUSTERERS]\n",
    "axclus[1].legend(handles_clus, CLUSTERERS, loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "figenc.savefig(os.path.join(FIGS_DIR, \"ranking_enc.pdf\"), bbox_inches=\"tight\")\n",
    "figclus.savefig(os.path.join(FIGS_DIR, \"ranking_clus.pdf\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With grouped datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "MuURKU9mMXIN",
    "outputId": "72833c72-a908-43cd-a4f9-d0030733fb0d"
   },
   "outputs": [],
   "source": [
    "metric_key1 = \"AMI\"\n",
    "metric_key2 = \"silhouette-euclidean_pred\"\n",
    "\n",
    "override_fields = {}\n",
    "\n",
    "backbones = [\"ResNet-50\", \"ViT-B\"]\n",
    "CLUSTERERS = [\"KMeans\", \"AC w/ C\", \"AC w/o C\", \"AffinityPropagation\", \"HDBSCAN\"]\n",
    "\n",
    "for test_group, test_datasets in TEST_DATASETS_GROUPED.items():\n",
    "\n",
    "    figenc, axenc = plt.subplots(1, 2, figsize=(6, 1.6))\n",
    "    figclus, axclus = plt.subplots(1, 2, figsize=(6, 2))\n",
    "\n",
    "    for i_backbone, backbone in enumerate(backbones):\n",
    "        result_table = np.nan * np.ones(\n",
    "            (len(MODEL_GROUPS[backbone]), len(CLUSTERERS), len(test_datasets))\n",
    "        )\n",
    "        for i_group, model in enumerate(list(MODEL_GROUPS[backbone])):\n",
    "            for i_clusterer, clusterer in enumerate(CLUSTERERS):\n",
    "                for i_dataset, dataset in enumerate(test_datasets):\n",
    "                    filter1 = {\"model\": model, \"dataset\": dataset}\n",
    "                    filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "                    filter2.update(filter1)\n",
    "                    filter2.update(override_fields)\n",
    "                    filter2 = fixup_filter(filter2)\n",
    "                    sdf = select_rows(test_runs_df, filter2, allow_missing=False)\n",
    "                    if len(sdf) < 1:\n",
    "                        result_table[i_group, i_clusterer, i_dataset] = -100.0\n",
    "                        continue\n",
    "                    result_table[i_group, i_clusterer, i_dataset] = np.median(\n",
    "                        sdf[metric_key1]\n",
    "                    )\n",
    "\n",
    "        print(backbone)\n",
    "        print(MODEL_GROUPS[backbone])\n",
    "\n",
    "        # RANK PER ENCODER - go through each dataset, look at each clusterer,\n",
    "        # and determine the rank of each encoder in that setting\n",
    "        print(list(MODEL_GROUPS[backbone]))\n",
    "        ranks_encoders = np.nan * np.ones((len(MODEL_GROUPS[backbone]), len(CLUSTERERS), len(test_datasets)))\n",
    "        for i_dataset, dataset in enumerate(test_datasets):\n",
    "            for i_clusterer, clusterer in enumerate(CLUSTERERS):\n",
    "                cluster_data = result_table[:, i_clusterer, i_dataset]\n",
    "                if np.all(cluster_data == cluster_data[0]) or np.all(np.isnan(cluster_data)):\n",
    "                    print(f\"Skipping {dataset} {clusterer} (all same)\")\n",
    "                    continue\n",
    "                if np.any(cluster_data == -100.0):\n",
    "                    print(f\"Skipping {dataset} {clusterer} (incomplete)\")\n",
    "                    continue\n",
    "                rank = np.argsort(cluster_data)[::-1]\n",
    "                ranks_encoders[:, i_clusterer, i_dataset] = 1 + rank.argsort()\n",
    "        mean_rank_encoders = np.nanmean(ranks_encoders, axis=(1, 2))\n",
    "        std_rank_encoders = np.nanstd(ranks_encoders, axis=(1, 2))\n",
    "        # order = np.argsort(mean_rank_encoders)\n",
    "        order = np.arange(len(MODEL_GROUPS[backbone]))\n",
    "\n",
    "        for i_plot, i_model in enumerate(order):\n",
    "            axenc[i_backbone].barh(\n",
    "                i_plot,\n",
    "                mean_rank_encoders[i_model],\n",
    "                xerr=std_rank_encoders[i_model],\n",
    "                align=\"center\",\n",
    "                alpha=0.6,\n",
    "                ecolor=\"black\",\n",
    "                color=MODEL2COLORRGB.get(MODEL_GROUPS[backbone][i_model], \"k\"),\n",
    "                capsize=2,\n",
    "                zorder=10,\n",
    "            )\n",
    "\n",
    "        axenc[i_backbone].invert_yaxis()\n",
    "        axenc[i_backbone].set_yticks([])\n",
    "        axenc[i_backbone].set_yticklabels([])\n",
    "        axenc[i_backbone].set_xticks(np.arange(1, 1 + len(MODEL_GROUPS[backbone])))\n",
    "        axenc[i_backbone].set_xlim([0, 0.5 + len(MODEL_GROUPS[backbone])])\n",
    "        axenc[i_backbone].xaxis.grid(True, zorder=1, alpha=0.5)\n",
    "        axenc[i_backbone].set_title(f\"{DATASETGROUP2TITLE.get(test_group, test_group)}, {backbone}\")\n",
    "        axenc[i_backbone].set_xlabel(\"Rank\")\n",
    "\n",
    "        # RANK PER CLUSTERER - go through each dataset, look at each encoder,\n",
    "        # and determine the rank of each clusterer in that setting\n",
    "\n",
    "        print(CLUSTERERS)\n",
    "        ranks_clusterers = np.nan * np.ones((len(MODEL_GROUPS[backbone]), len(CLUSTERERS), len(test_datasets)))\n",
    "        for i_dataset, dataset in enumerate(test_datasets):\n",
    "            for i_encoder, encoder in enumerate(MODEL_GROUPS[backbone]):\n",
    "                encoder_data = result_table[i_encoder, :, i_dataset]\n",
    "                if np.all(encoder_data == encoder_data[0]) or np.all(np.isnan(encoder_data)):\n",
    "                    print(f\"Skipping {dataset} {encoder} (all same)\")\n",
    "                    continue\n",
    "                if np.any(encoder_data == -100.0):\n",
    "                    print(f\"Skipping {dataset} {encoder} (incomplete)\")\n",
    "                    continue\n",
    "                rank = np.argsort(encoder_data)[::-1]\n",
    "                ranks_clusterers[i_encoder, :, i_dataset] = 1 + rank.argsort()\n",
    "        mean_rank_clusters = np.nanmean(ranks_clusterers, axis=(0, 2))\n",
    "        std_rank_clusters = np.nanstd(ranks_clusterers, axis=(0, 2))\n",
    "        # order = np.argsort(mean_rank_clusters)\n",
    "        order = np.arange(len(CLUSTERERS))\n",
    "\n",
    "        for i_plot, i_clusterer in enumerate(order):\n",
    "            axclus[i_backbone].barh(\n",
    "                i_plot,\n",
    "                mean_rank_clusters[i_clusterer],\n",
    "                xerr=std_rank_clusters[i_clusterer],\n",
    "                align=\"center\",\n",
    "                alpha=0.6,\n",
    "                ecolor=\"black\",\n",
    "                color=CLUSTERER2COLORSTR.get(CLUSTERERS[i_clusterer], \"k\"),\n",
    "                capsize=2,\n",
    "                zorder=10,\n",
    "            )\n",
    "\n",
    "        axclus[i_backbone].invert_yaxis()\n",
    "        axclus[i_backbone].set_yticks([])\n",
    "        axclus[i_backbone].set_yticklabels([])\n",
    "        axclus[i_backbone].set_xticks(np.arange(1, 1 + len(CLUSTERERS)))\n",
    "        axclus[i_backbone].set_xlim([0, 0.6 + len(CLUSTERERS)])\n",
    "        axclus[i_backbone].xaxis.grid(True, zorder=1, alpha=0.5)\n",
    "        axclus[i_backbone].set_title(f\"{DATASETGROUP2TITLE.get(test_group, test_group)}, {backbone}\")\n",
    "        axclus[i_backbone].set_xlabel(\"Rank\")\n",
    "\n",
    "    label_fn = lambda c, ls: plt.plot([], [], color=c, ls=ls, linewidth=3)[0]  # noqa:E731\n",
    "\n",
    "    model_names = list(MODEL_GROUPS[\"ResNet-50\"]) + [\"timm_vit_base_patch16_224.mae\"]\n",
    "    handles_enc = [label_fn(MODEL2COLORRGB[idx], \"-\") for idx in model_names]\n",
    "    axenc[1].legend(\n",
    "        handles_enc,\n",
    "        [MODEL2SH[x] for x in model_names],\n",
    "        loc=\"center left\",\n",
    "        bbox_to_anchor=(1, 0.5),\n",
    "    )\n",
    "\n",
    "    handles_clus = [label_fn(CLUSTERER2COLORRGB[clusterer], \"-\") for clusterer in CLUSTERERS]\n",
    "    axclus[1].legend(handles_clus, CLUSTERERS, loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "    figenc.savefig(os.path.join(FIGS_DIR, f\"ranking_enc__{test_group}.pdf\"), bbox_inches=\"tight\")\n",
    "    figclus.savefig(os.path.join(FIGS_DIR, f\"ranking_clus__{test_group}.pdf\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plot sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from zs_ssl_clustering.io import sanitize_filename\n",
    "\n",
    "\n",
    "def get_pred_path(row):\n",
    "    \"\"\"\n",
    "    Generate path to y_pred file.\n",
    "    \"\"\"\n",
    "    run_id = row[\"name\"].split(\"__\")[-1]\n",
    "    fname = f\"{row['partition']}-{row['dataset_name']}__{row['model']}__{run_id}.npz\"\n",
    "    fname = sanitize_filename(fname)\n",
    "    fname = os.path.join(\n",
    "        row[\"predictions_dir\"],\n",
    "        sanitize_filename(row[\"partition\"] + f\"__z{float(row['zoom_ratio'])}\"),\n",
    "        fname,\n",
    "    )\n",
    "    return fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms.functional import crop, get_dimensions\n",
    "from torchvision.utils import _log_api_usage_once\n",
    "\n",
    "\n",
    "def center_squaring(img):\n",
    "    \"\"\"Crops the given image at the center.\n",
    "    If the image is torch Tensor, it is expected\n",
    "    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions.\n",
    "    If image size is smaller than output size along any edge, image is padded with 0 and then center cropped.\n",
    "\n",
    "    Args:\n",
    "        img (PIL Image or Tensor): Image to be cropped.\n",
    "        output_size (sequence or int): (height, width) of the crop box. If int or sequence with single int,\n",
    "            it is used for both directions.\n",
    "\n",
    "    Returns:\n",
    "        PIL Image or Tensor: Cropped image.\n",
    "    \"\"\"\n",
    "    if not torch.jit.is_scripting() and not torch.jit.is_tracing():\n",
    "        _log_api_usage_once(center_squaring)\n",
    "\n",
    "    _, image_height, image_width = get_dimensions(img)\n",
    "\n",
    "    if image_height == image_width:\n",
    "        return img\n",
    "\n",
    "    crop_height = crop_width = min(image_height, image_width)\n",
    "\n",
    "    crop_top = int(round((image_height - crop_height) / 2.0))\n",
    "    crop_left = int(round((image_width - crop_width) / 2.0))\n",
    "    return crop(img, crop_top, crop_left, crop_height, crop_width)\n",
    "\n",
    "\n",
    "class CenterSquaring(torch.nn.Module):\n",
    "    \"\"\"Crops the given image to the center square.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        _log_api_usage_once(self)\n",
    "\n",
    "    def forward(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image or Tensor): Image to be cropped.\n",
    "\n",
    "        Returns:\n",
    "            PIL Image or Tensor: Cropped image.\n",
    "        \"\"\"\n",
    "        return center_squaring(img)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zs_ssl_clustering import datasets\n",
    "\n",
    "\n",
    "def show_samples(\n",
    "    row, nsamp=12, ds=None, save=False, clusterer=\"\", nclusters=None, skip_existing=None\n",
    "):\n",
    "    if skip_existing is None:\n",
    "        skip_existing = save\n",
    "\n",
    "    if clusterer:\n",
    "        clusterer = clusterer.replace(\"/\", \"\").replace(\" \", \"\")\n",
    "    else:\n",
    "        clusterer = row[\"clusterer_name\"]\n",
    "\n",
    "    output_dir = \"../samples\"\n",
    "    output_fname = f\"samples__{row['dataset_name']}__{row['model']}__{clusterer}.png\"\n",
    "    output_fname = os.path.join(output_dir, output_fname)\n",
    "    if skip_existing and os.path.exists(output_fname):\n",
    "        print(f\"Output {output_fname} already exists. Skipping.\")\n",
    "        return\n",
    "\n",
    "    if ds is None:\n",
    "        dses = datasets.fetch_image_dataset(\n",
    "            row[\"dataset_name\"], transform_eval=CenterSquaring()\n",
    "        )\n",
    "        if row[\"partition\"] == \"train\":\n",
    "            ds = dses[0]\n",
    "        elif row[\"partition\"] == \"test\":\n",
    "            ds = dses[-1]\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    y_pred = np.load(\"../\" + get_pred_path(row))[\"y_pred\"]\n",
    "\n",
    "    u_labels, label_count = np.unique(y_pred, return_counts=True)\n",
    "    # Remove clusters with very few samples in the cluster\n",
    "    # u_labels = u_labels[label_count >= nsamp]\n",
    "\n",
    "    if nclusters is None:\n",
    "        nclusters = len(u_labels)\n",
    "    else:\n",
    "        nclusters = min(nclusters, len(u_labels))\n",
    "\n",
    "    fig, axs = plt.subplots(nclusters, nsamp, figsize=(nsamp / 2, nclusters / 2))\n",
    "\n",
    "    for i_label, label in enumerate(u_labels[:nclusters]):\n",
    "        indices = np.where(y_pred == label)[0]\n",
    "        np.random.default_rng(seed=label).shuffle(indices)\n",
    "        for i in range(nsamp):\n",
    "            if i < len(indices):\n",
    "                idx = indices[i]\n",
    "                axs[i_label, i].imshow(ds[idx][0].convert(\"RGB\"))\n",
    "            axs[i_label, i].axis(\"off\")\n",
    "\n",
    "    if save:\n",
    "        print(f\"Saving to {output_fname}\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        plt.savefig(output_fname, bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "def fetch_row(dataset, model, clusterer):\n",
    "    override_fields = {\n",
    "        \"predictions_dir\": \"y_pred\",\n",
    "    }\n",
    "    if clusterer == \"HDBSCAN\" and dataset in [\"celeba\", \"utkface\"]:\n",
    "        override_fields[\"min_samples\"] = 2\n",
    "    filter1 = {\"model\": model, \"dataset\": dataset}\n",
    "    filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "    filter2.update(filter1)\n",
    "    filter2.update(override_fields)\n",
    "    filter2 = fixup_filter(filter2)\n",
    "    sdf = select_rows(test_runs_df, filter2, allow_missing=False)\n",
    "    if len(sdf) < 1:\n",
    "        print(f\"No data for {filter2}\")\n",
    "        print(filter2command(filter2, partition=\"test\"))\n",
    "        return\n",
    "    elif len(sdf) > 1:\n",
    "        perf = sdf.iloc[0][\"AMI\"]\n",
    "        if sum(sdf[\"AMI\"] != perf) > 0:\n",
    "            print()\n",
    "            print(\"More than one result with AMIs:\", list(sdf[\"AMI\"]))\n",
    "            print(f\"for search {filter2}\")\n",
    "            dif_cols = find_differing_columns(sdf, config_keys)\n",
    "            print(f\"columns which differ: {dif_cols}\")\n",
    "            if dif_cols:\n",
    "                for col in dif_cols:\n",
    "                    print(f\"  {col}: {list(sdf[col])}\")\n",
    "        return\n",
    "    return sdf.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"svhn\"\n",
    "model = \"mocov3_resnet50\"\n",
    "clusterer = \"AC w/ C\"\n",
    "\n",
    "override_fields = {\n",
    "    \"predictions_dir\": \"y_pred\",\n",
    "    # \"aggclust_dist_thresh\": None,  # Use this to flip between unknown/known num clusters for Agglom\n",
    "}\n",
    "filter1 = {\"model\": model, \"dataset\": dataset}\n",
    "filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "filter2.update(filter1)\n",
    "filter2.update(override_fields)\n",
    "filter2 = fixup_filter(filter2)\n",
    "sdf = select_rows(test_runs_df, filter2, allow_missing=False)\n",
    "if len(sdf) < 1:\n",
    "    print(f\"No data for {filter2}\")\n",
    "    print(filter2command(filter2, partition=\"test\"))\n",
    "elif len(sdf) > 1:\n",
    "    perf = sdf.iloc[0][\"AMI\"]\n",
    "    if sum(sdf[\"AMI\"] != perf) > 0:\n",
    "        print()\n",
    "        print(\"More than one result with AMIs:\", list(sdf[\"AMI\"]))\n",
    "        print(f\"for search {filter2}\")\n",
    "        dif_cols = find_differing_columns(sdf, config_keys)\n",
    "        print(f\"columns which differ: {dif_cols}\")\n",
    "        if dif_cols:\n",
    "            for col in dif_cols:\n",
    "                print(f\"  {col}: {list(sdf[col])}\")\n",
    "else:\n",
    "    display(sdf)\n",
    "    row = sdf.iloc[0]\n",
    "    print(\n",
    "        row[\"name\"].split(\"__\")[-1],\n",
    "        \"\\n\" + row[\"name\"],\n",
    "        \"\\n  \" + row[\"dataset_name\"],\n",
    "        \"\\n  \" + row[\"model\"],\n",
    "        \"\\n  \" + row[\"clusterer_name\"],\n",
    "        f\"\\n  AMI={row['AMI']}\",\n",
    "        f\"\\n  S_reduced={row['silhouette-euclidean_pred']}\",\n",
    "        f\"\\n  S_originl={row['silhouette-og-euclidean_pred']}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.load(\"../\" + get_pred_path(row))[\"y_pred\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.fetch_image_dataset(row[\"dataset_name\"])[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.where(y_pred == 0)[0]\n",
    "np.random.default_rng(seed=0).shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 3\n",
    "nsamp = 10\n",
    "\n",
    "indices = np.where(y_pred == label)[0]\n",
    "np.random.default_rng(seed=label).shuffle(indices)\n",
    "\n",
    "fig, axs = plt.subplots(1, nsamp, figsize=(6, 2))\n",
    "\n",
    "for i in range(10):\n",
    "    idx = indices[i]\n",
    "    axs[i].imshow(ds[idx][0])\n",
    "    axs[i].axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamp = 10\n",
    "\n",
    "u_labels = np.unique(y_pred)\n",
    "\n",
    "fig, axs = plt.subplots(len(u_labels), nsamp, figsize=(len(u_labels) / 2, nsamp / 2))\n",
    "\n",
    "for i_label, label in enumerate(u_labels):\n",
    "    indices = np.where(y_pred == label)[0]\n",
    "    np.random.default_rng(seed=label).shuffle(indices)\n",
    "    for i in range(10):\n",
    "        idx = indices[i]\n",
    "        axs[i_label, i].imshow(ds[idx][0])\n",
    "        axs[i_label, i].axis(\"off\")\n",
    "\n",
    "# plt.savefig(f\"{row['dataset_name']}_{row['model']}_{row['clusterer_name']}.png\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.fetch_image_dataset(\"flowers102\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.fetch_image_dataset(\"flowers102\", transform_eval=CenterSquaring())[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"svhn\"\n",
    "model = \"mocov3_resnet50\"\n",
    "clusterer = \"AC w/ C\"\n",
    "\n",
    "row = fetch_row(dataset, model, clusterer)\n",
    "print(\n",
    "    row[\"name\"].split(\"__\")[-1],\n",
    "    \"\\n\" + row[\"name\"],\n",
    "    \"\\n  \" + row[\"dataset_name\"],\n",
    "    \"\\n  \" + row[\"model\"],\n",
    "    \"\\n  \" + row[\"clusterer_name\"],\n",
    "    f\"\\n  AMI        = {row['AMI']}\",\n",
    "    f\"\\n  S_reduced  = {row['silhouette-euclidean_pred']}\",\n",
    "    f\"\\n  S_original = {row['silhouette-og-euclidean_pred']}\",\n",
    ")\n",
    "show_samples(row, save=True, clusterer=clusterer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for clusterer in [\"AC w/ C\"]:  # , \"AC w/o C\"]:\n",
    "    for model in [\"mocov3_resnet50\", \"mocov3_vit_base\", \"dino_resnet50\", \"dino_vitb16\"]:\n",
    "        for dataset in [\n",
    "            \"mnist\",\n",
    "            \"fashionmnist\",\n",
    "            \"svhn\",\n",
    "            \"cifar10\",\n",
    "            \"cifar100\",\n",
    "            \"flowers102\",\n",
    "            \"aircraft\",\n",
    "        ]:\n",
    "            print()\n",
    "            print(f\"{dataset:<16s} {model:<32s} {clusterer}\")\n",
    "            row = fetch_row(dataset, model, clusterer)\n",
    "            if row is None:\n",
    "                print(\"No data with y_pred for\", dataset, model, clusterer)\n",
    "                continue\n",
    "            print(\n",
    "                row[\"name\"].split(\"__\")[-1],\n",
    "                \"\\n\" + row[\"name\"],\n",
    "                \"\\n  \" + row[\"dataset_name\"],\n",
    "                \"\\n  \" + row[\"model\"],\n",
    "                \"\\n  \" + row[\"clusterer_name\"],\n",
    "                f\"\\n  AMI        = {row['AMI']}\",\n",
    "                f\"\\n  S_reduced  = {row['silhouette-euclidean_pred']}\",\n",
    "                f\"\\n  S_original = {row['silhouette-og-euclidean_pred']}\",\n",
    "            )\n",
    "            fig = show_samples(row, save=True, clusterer=clusterer, nclusters=150)\n",
    "            # plt.show()\n",
    "            print(\"\\n\\nStopping early!\")\n",
    "            break\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for clusterer in [\"AC w/o C\"]:\n",
    "    for model in RESNET50_MODELS + VITB16_MODELS:\n",
    "        for dataset in [\n",
    "            \"mnist\",\n",
    "            \"fashionmnist\",\n",
    "            \"svhn\",\n",
    "            \"cifar10\",\n",
    "            \"cifar100\",\n",
    "            \"flowers102\",\n",
    "            \"aircraft\",\n",
    "        ]:\n",
    "            print()\n",
    "            print(f\"{dataset:<16s} {model:<32s} {clusterer}\")\n",
    "            row = fetch_row(dataset, model, clusterer)\n",
    "            if row is None:\n",
    "                print(\"No data with y_pred for\", dataset, model, clusterer)\n",
    "                continue\n",
    "            print(\n",
    "                row[\"name\"].split(\"__\")[-1],\n",
    "                \"\\n\" + row[\"name\"],\n",
    "                \"\\n  \" + row[\"dataset_name\"],\n",
    "                \"\\n  \" + row[\"model\"],\n",
    "                \"\\n  \" + row[\"clusterer_name\"],\n",
    "                f\"\\n  AMI        = {row['AMI']}\",\n",
    "                f\"\\n  S_reduced  = {row['silhouette-euclidean_pred']}\",\n",
    "                f\"\\n  S_original = {row['silhouette-og-euclidean_pred']}\",\n",
    "            )\n",
    "            fig = show_samples(row, save=True, clusterer=clusterer, nclusters=150)\n",
    "            # plt.show()\n",
    "            print(\"\\n\\nStopping early!\")\n",
    "            break\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clusterer in [\"AC w/o C\"]:\n",
    "    for model in RESNET50_MODELS + VITB16_MODELS:\n",
    "        for dataset in [\"inaturalist\"]:\n",
    "            print()\n",
    "            print(f\"{dataset:<16s} {model:<32s} {clusterer}\")\n",
    "            row = fetch_row(dataset, model, clusterer)\n",
    "            if row is None:\n",
    "                print(\"No data with y_pred for\", dataset, model, clusterer)\n",
    "                continue\n",
    "            print(\n",
    "                row[\"name\"].split(\"__\")[-1],\n",
    "                \"\\n\" + row[\"name\"],\n",
    "                \"\\n  \" + row[\"dataset_name\"],\n",
    "                \"\\n  \" + row[\"model\"],\n",
    "                \"\\n  \" + row[\"clusterer_name\"],\n",
    "                f\"\\n  AMI        = {row['AMI']}\",\n",
    "                f\"\\n  S_reduced  = {row['silhouette-euclidean_pred']}\",\n",
    "                f\"\\n  S_original = {row['silhouette-og-euclidean_pred']}\",\n",
    "            )\n",
    "            fig = show_samples(row, save=True, clusterer=clusterer, nclusters=150)\n",
    "            # plt.show()\n",
    "            plt.close()\n",
    "            print(\"\\n\\nStopping early!\")\n",
    "            break\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for clusterer in [\"AC w/o C\"]:\n",
    "    for model in RESNET50_MODELS + VITB16_MODELS:\n",
    "        for dataset in TEST_DATASETS:\n",
    "            print()\n",
    "            print(f\"{dataset:<16s} {model:<32s} {clusterer}\")\n",
    "            row = fetch_row(dataset, model, clusterer)\n",
    "            if row is None:\n",
    "                print(\"No data with y_pred for\", dataset, model, clusterer)\n",
    "                continue\n",
    "            print(\n",
    "                row[\"name\"].split(\"__\")[-1],\n",
    "                \"\\n\" + row[\"name\"],\n",
    "                \"\\n  \" + row[\"dataset_name\"],\n",
    "                \"\\n  \" + row[\"model\"],\n",
    "                \"\\n  \" + row[\"clusterer_name\"],\n",
    "                f\"\\n  AMI        = {row['AMI']}\",\n",
    "                f\"\\n  S_reduced  = {row['silhouette-euclidean_pred']}\",\n",
    "                f\"\\n  S_original = {row['silhouette-og-euclidean_pred']}\",\n",
    "            )\n",
    "            try:\n",
    "                fig = show_samples(row, save=True, clusterer=clusterer, nclusters=150)\n",
    "            except Exception:\n",
    "                print(f\"{dataset} not found\")\n",
    "            try:\n",
    "                plt.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "            print(\"\\n\\nStopping early!\")\n",
    "            break\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Breakdown information about datasets with multiple labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "import torchvision.datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### CelebA attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeba_test = torchvision.datasets.CelebA(\n",
    "    os.path.expanduser(\"~/Datasets\"),\n",
    "    target_type=\"attr\",\n",
    "    split=\"test\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "MyWQRN9iqBkI",
    "outputId": "2642efe3-b0ef-46f3-dc70-f2f5c69645d4"
   },
   "outputs": [],
   "source": [
    "metric_key = \"AMI\"  # AMI  num_cluster_pred  silhouette-euclidean_pred  silhouette-og-euclidean_pred\n",
    "show_pc = True\n",
    "show_fmt = \"{:4.0f}\"\n",
    "show_commands = False\n",
    "highlight_best = True\n",
    "use_si_num = False\n",
    "eps = 0.005\n",
    "override_fields = {\n",
    "    \"predictions_dir\": \"y_pred\",\n",
    "}\n",
    "\n",
    "backbone = \"ViT-B\"  # \"ResNet-50\" or \"ViT-B\"\n",
    "\n",
    "if metric_key == \"num_cluster_pred\":\n",
    "    CLUSTERERS = [\"AC w/o C\", \"AffinityPropagation\", \"HDBSCAN\"]\n",
    "    show_pc = False\n",
    "    show_fmt = \"{:4.0f}\"\n",
    "    highlight_best = False\n",
    "    use_si_num = True\n",
    "else:\n",
    "    CLUSTERERS = [\"KMeans\", \"AC w/ C\", \"AC w/o C\", \"AffinityPropagation\", \"HDBSCAN\"]\n",
    "if metric_key.startswith(\"silhouette\"):\n",
    "    show_pc = False\n",
    "    show_fmt = \"{:5.2f}\"\n",
    "\n",
    "print(MODEL_GROUPS)\n",
    "\n",
    "dataset = \"celeba\"\n",
    "\n",
    "TEST_ATTRS = [\"Identity\"] + celeba_test.attr_names[:-1]\n",
    "print(TEST_ATTRS)\n",
    "best_results = {k: [] for k in TEST_ATTRS}\n",
    "best_results_grouped = {k: defaultdict(list) for k in TEST_ATTRS}\n",
    "\n",
    "for dummy in [True, False]:\n",
    "    cmds = []\n",
    "    latex_table = r\"% Results for \" + f\"{dataset} breakdown, {metric_key}, {backbone}\" + \"\\n\"\n",
    "    now_str = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    latex_table += r\"% Generated \" + now_str + \"\\n\"\n",
    "    latex_table += r\"% Using hparams \" + BEST_PARAMS[\"_version\"] + \"\\n\"\n",
    "    label = backbone\n",
    "    if metric_key == \"AMI\":\n",
    "        latex_table += r\"\\label{tab:\" + label + r\"}\" + \"\\n\"\n",
    "    label = metric_key.replace(\"_\", \"-\") + \":\" + label\n",
    "    latex_table += r\"\\label{tab:\" + label + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"\\resizebox{\\textwidth}{!}{%\" + \"\\n\"\n",
    "    latex_table += r\"\\begin{tabular}{ll\" + r\"r\" * len(TEST_ATTRS) + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    latex_table += r\"& \" + f\"{'Encoder':<11s}\"\n",
    "    for attr in TEST_ATTRS:\n",
    "        latex_table += r\"&\" + f\"{attr.replace('_', ' '):^15s}\"\n",
    "    latex_table += r\"\\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    print(MODEL_GROUPS[backbone])\n",
    "    if metric_key == \"num_cluster_pred\":\n",
    "        latex_table += r\"& Num targets\"\n",
    "        for i_attr, attr in enumerate(TEST_ATTRS):\n",
    "            sdf = select_rows(test_runs_df, {\"dataset\": dataset}, allow_missing=False)\n",
    "            sdf = sdf[~pd.isna(sdf[\"num_cluster_true\"])]\n",
    "            latex_table += r\"& \"\n",
    "            latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "            latex_table += f\"{sdf.iloc[0]['num_cluster_true'].item()}\"\n",
    "            latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "        latex_table += r\"\\\\\" + \"\\n\"\n",
    "        latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    elif metric_key.endswith(\"_pred\"):\n",
    "        metric_key2 = metric_key.replace(\"_pred\", \"_true\")\n",
    "        clusterername = \"G.T.\"\n",
    "        latex_table += (\n",
    "            r\"\\parbox[t]{2mm}{\\multirow{\"\n",
    "            + str(len(MODEL_GROUPS[backbone]))\n",
    "            + r\"}{*}{\"\n",
    "            + r\"\\scalebox{0.9}{\"\n",
    "            + r\"\\rotatebox[origin=c]{90}{\"\n",
    "            + clusterername\n",
    "            + r\"}}}\"\n",
    "            + r\"}\"\n",
    "        )\n",
    "        latex_table += \"\\n\"\n",
    "        for i_group, model in enumerate(list(MODEL_GROUPS[backbone])):\n",
    "            latex_table += f\"& {MODEL2SH[model]:<10s}\"\n",
    "            for i_attr, attr in enumerate(TEST_ATTRS):\n",
    "                latex_table += \" &\"\n",
    "                filter1 = {\"model\": model, \"dataset\": dataset}\n",
    "                if model == \"timm_vit_base_patch16_224.mae\":\n",
    "                    filter1[\"dim_reducer\"] = \"PCA\"\n",
    "                    filter1[\"pca_variance\"] = 0.95\n",
    "                else:\n",
    "                    filter1[\"dim_reducer_man\"] = \"UMAP\"\n",
    "                    filter1[\"ndim_reduced_man\"] = 50\n",
    "                    filter1[\"dim_reducer_man_metric\"] = \"euclidean\"\n",
    "                sdf = select_rows(test_runs_df, filter1, allow_missing=False)\n",
    "                sdf = sdf[~pd.isna(sdf[metric_key2])]\n",
    "                my_val = np.nanmedian(sdf[metric_key])\n",
    "                if sum(sdf[metric_key2] != my_val) > 0:\n",
    "                    pass\n",
    "                if dummy:\n",
    "                    best_results_grouped[attr][clusterername].append(my_val)\n",
    "                    continue\n",
    "                is_best_grp = my_val + eps >= np.max(\n",
    "                    best_results_grouped[attr][clusterername]\n",
    "                )\n",
    "                latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "                latex_table += \"     \"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best_grp:\n",
    "                    latex_table += r\"\\tcg{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                latex_table += show_fmt.format(my_val)\n",
    "                if highlight_best:\n",
    "                    latex_table += r\"}\" if is_best_grp else \" \"\n",
    "                latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "\n",
    "            latex_table += r\" \\\\\" + \"\\n\"\n",
    "        latex_table += r\"\\toprule\" + \"\\n\"\n",
    "\n",
    "    first_agg = True\n",
    "    for i_clusterer, clusterer in enumerate(CLUSTERERS):\n",
    "        clusterername = CLUSTERER2SH.get(clusterer, clusterer)\n",
    "        my_override_fields = override_fields.copy()\n",
    "        if (\n",
    "            first_agg\n",
    "            and clusterer == \"AgglomerativeClustering\"\n",
    "            and metric_key != \"num_cluster_pred\"\n",
    "        ):\n",
    "            first_agg = False\n",
    "            my_override_fields[\"aggclust_dist_thresh\"] = None\n",
    "            clusterername = \"AC  w/ C\"\n",
    "        elif clusterer == \"AgglomerativeClustering\":\n",
    "            clusterername = \"AC w/o C\"\n",
    "            if \"aggclust_dist_thresh\" in my_override_fields:\n",
    "                del my_override_fields[\"aggclust_dist_thresh\"]\n",
    "\n",
    "        if i_clusterer > 0:\n",
    "            latex_table += r\"\\midrule\" + \"\\n\"\n",
    "\n",
    "        latex_table += (\n",
    "            r\"\\parbox[t]{2mm}{\\multirow{\"\n",
    "            + str(len(MODEL_GROUPS[backbone]))\n",
    "            + r\"}{*}{\"\n",
    "            + r\"\\scalebox{0.9}{\"\n",
    "            + r\"\\rotatebox[origin=c]{90}{\"\n",
    "            + clusterername\n",
    "            + r\"}}}\"\n",
    "            + r\"}\"\n",
    "        )\n",
    "        latex_table += \"\\n\"\n",
    "\n",
    "        for i_group, model in enumerate(list(MODEL_GROUPS[backbone])):\n",
    "            latex_table += f\"& {MODEL2SH[model]:<10s}\"\n",
    "            filter1 = {\"model\": model, \"dataset\": dataset}\n",
    "            filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "            filter2.update(filter1)\n",
    "            filter2.update(my_override_fields)\n",
    "            filter2 = fixup_filter(filter2)\n",
    "            sdf = select_rows(test_runs_df, filter2, allow_missing=False)\n",
    "            if len(sdf) < 1:\n",
    "                print(f\"No data for {model}-{dataset}-{clusterer}\\n{filter2}\")\n",
    "                cmds.append(filter2command(filter2, partition=\"test\"))\n",
    "                if not dummy:\n",
    "                    # latex_table += r\"\\multicolumn{1}{c}{--}\"\n",
    "                    latex_table += r\"   --  \"\n",
    "                continue\n",
    "            if len(sdf) > 1:\n",
    "                if sum(np.abs(sdf[metric_key] - sdf.iloc[0][metric_key]) > 1e-6) > 0:\n",
    "                    print()\n",
    "                    print(f\"More than one result with {metric_key} values\", list(sdf[metric_key]))\n",
    "                    print(f\"for search {filter2}\")\n",
    "                    dif_cols = find_differing_columns(sdf, config_keys)\n",
    "                    print(f\"columns which differ: {dif_cols}\")\n",
    "                    if dif_cols:\n",
    "                        for col in dif_cols:\n",
    "                            print(f\"  {col}: {list(sdf[col])}\")\n",
    "            y_pred = np.load(\"../\" + get_pred_path(sdf.iloc[0]))[\"y_pred\"]\n",
    "            for i_attr, attr in enumerate(TEST_ATTRS):\n",
    "                latex_table += \" &\"\n",
    "                if metric_key.lower() != \"ami\":\n",
    "                    raise NotImplementedError()\n",
    "                if attr.lower() == \"identity\":\n",
    "                    my_val = sklearn.metrics.adjusted_mutual_info_score(celeba_test.identity[:, 0], y_pred)\n",
    "                else:\n",
    "                    my_val = sklearn.metrics.adjusted_mutual_info_score(celeba_test.attr[:, i_attr - 1], y_pred)\n",
    "                if dummy:\n",
    "                    best_results[attr].append(my_val)\n",
    "                    best_results_grouped[attr][clusterername].append(my_val)\n",
    "                    continue\n",
    "                if np.isnan(my_val):\n",
    "                    latex_table += r\"   --  \"\n",
    "                    continue\n",
    "                is_best = my_val + eps >= np.max(best_results[attr])\n",
    "                if len(best_results[attr]) > 1:\n",
    "                    is_secd = my_val + eps >= np.sort(best_results[attr])[-2]\n",
    "                else:\n",
    "                    is_secd = False\n",
    "                is_best_grp = my_val + eps >= np.max(\n",
    "                    best_results_grouped[attr][clusterername]\n",
    "                )\n",
    "                sc_base = np.nanmedian(best_results[attr])\n",
    "                sc_top = np.max(best_results[attr])\n",
    "                sc = 100 * max(0, (my_val - sc_base) / (sc_top - sc_base))\n",
    "                latex_table += r\"\\cellcolor{cbg!\" + f\"{sc:.0f}\" + \"}\"\n",
    "                if show_pc:\n",
    "                    my_val = my_val * 100\n",
    "                latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best:\n",
    "                    latex_table += r\"\\tcf{\"\n",
    "                elif is_secd:\n",
    "                    latex_table += r\"\\tcs{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best_grp:\n",
    "                    latex_table += r\"\\tcg{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                latex_table += show_fmt.format(my_val)\n",
    "                if highlight_best:\n",
    "                    latex_table += r\"}\" if is_best or is_secd else \" \"\n",
    "                    latex_table += r\"}\" if is_best_grp else \" \"\n",
    "                latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "            latex_table += r\" \\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\bottomrule\" + \"\\n\"\n",
    "    latex_table += r\"\\end{tabular}\" + \"\\n\"\n",
    "    latex_table += r\"}\" + \"\\n\"\n",
    "\n",
    "print()\n",
    "print(f\"There are {len(cmds)} commands to execute to generate missing datapoints\")\n",
    "if show_commands:\n",
    "    for cmd in cmds:\n",
    "        print(cmd)\n",
    "\n",
    "print()\n",
    "print(\"Done!\")\n",
    "print()\n",
    "print(f\"Here is your {dataset} results table for {metric_key}, {backbone}:\")\n",
    "print()\n",
    "print()\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ImageNet-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenetr_test = datasets.fetch_image_dataset(\"imagenet-r\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artforms = [os.path.basename(fname[0]).split(\"_\")[0] for fname in imagenetr_test.imgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_artform, artform_ids = np.unique(artforms, return_inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(u_artform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(imagenetr_test.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_artform_ids = imagenetr_test.targets + artform_ids * 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(class_artform_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs = np.stack([imagenetr_test.targets, artform_ids, class_artform_ids], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "MyWQRN9iqBkI",
    "outputId": "2642efe3-b0ef-46f3-dc70-f2f5c69645d4"
   },
   "outputs": [],
   "source": [
    "metric_key = \"AMI\"  # AMI  num_cluster_pred  silhouette-euclidean_pred  silhouette-og-euclidean_pred\n",
    "show_pc = True\n",
    "show_fmt = \"{:4.0f}\"\n",
    "show_commands = False\n",
    "highlight_best = True\n",
    "use_si_num = False\n",
    "eps = 0.005\n",
    "override_fields = {\n",
    "    \"predictions_dir\": \"y_pred\",\n",
    "}\n",
    "\n",
    "backbone = \"ViT-B\"  # \"ResNet-50\" or \"ViT-B\"\n",
    "\n",
    "if metric_key == \"num_cluster_pred\":\n",
    "    CLUSTERERS = [\"AC w/o C\", \"AffinityPropagation\", \"HDBSCAN\"]\n",
    "    show_pc = False\n",
    "    show_fmt = \"{:4.0f}\"\n",
    "    highlight_best = False\n",
    "    use_si_num = True\n",
    "else:\n",
    "    CLUSTERERS = [\"KMeans\", \"AC w/ C\", \"AC w/o C\", \"AffinityPropagation\", \"HDBSCAN\"]\n",
    "if metric_key.startswith(\"silhouette\"):\n",
    "    show_pc = False\n",
    "    show_fmt = \"{:5.2f}\"\n",
    "\n",
    "print(MODEL_GROUPS)\n",
    "\n",
    "dataset = \"imagenet-r\"\n",
    "\n",
    "TEST_ATTRS = [\"Class\", \"Artform\", \"Both\"]\n",
    "print(TEST_ATTRS)\n",
    "best_results = {k: [] for k in TEST_ATTRS}\n",
    "best_results_grouped = {k: defaultdict(list) for k in TEST_ATTRS}\n",
    "\n",
    "for dummy in [True, False]:\n",
    "    cmds = []\n",
    "    latex_table = r\"% Results for \" + f\"{dataset} breakdown, {metric_key}, {backbone}\" + \"\\n\"\n",
    "    now_str = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    latex_table += r\"% Generated \" + now_str + \"\\n\"\n",
    "    latex_table += r\"% Using hparams \" + BEST_PARAMS[\"_version\"] + \"\\n\"\n",
    "    label = backbone\n",
    "    if metric_key == \"AMI\":\n",
    "        latex_table += r\"\\label{tab:\" + label + r\"}\" + \"\\n\"\n",
    "    label = metric_key.replace(\"_\", \"-\") + \":\" + label\n",
    "    latex_table += r\"\\label{tab:\" + label + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"%\\resizebox{\\textwidth}{!}{%\" + \"\\n\"  # Disabled\n",
    "    latex_table += r\"\\begin{tabular}{ll\" + r\"r\" * len(TEST_ATTRS) + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    latex_table += r\"& \" + f\"{'Encoder':<11s}\"\n",
    "    for attr in TEST_ATTRS:\n",
    "        latex_table += r\"&\" + f\"{attr.replace('_', ' '):^15s}\"\n",
    "    latex_table += r\"\\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    print(MODEL_GROUPS[backbone])\n",
    "    if metric_key == \"num_cluster_pred\":\n",
    "        latex_table += r\"& Num targets\"\n",
    "        for i_attr, attr in enumerate(TEST_ATTRS):\n",
    "            sdf = select_rows(test_runs_df, {\"dataset\": dataset}, allow_missing=False)\n",
    "            sdf = sdf[~pd.isna(sdf[\"num_cluster_true\"])]\n",
    "            latex_table += r\"& \"\n",
    "            latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "            latex_table += f\"{sdf.iloc[0]['num_cluster_true'].item()}\"\n",
    "            latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "        latex_table += r\"\\\\\" + \"\\n\"\n",
    "        latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    elif metric_key.endswith(\"_pred\"):\n",
    "        metric_key2 = metric_key.replace(\"_pred\", \"_true\")\n",
    "        clusterername = \"G.T.\"\n",
    "        latex_table += (\n",
    "            r\"\\parbox[t]{2mm}{\\multirow{\"\n",
    "            + str(len(MODEL_GROUPS[backbone]))\n",
    "            + r\"}{*}{\"\n",
    "            + r\"\\scalebox{0.9}{\"\n",
    "            + r\"\\rotatebox[origin=c]{90}{\"\n",
    "            + clusterername\n",
    "            + r\"}}}\"\n",
    "            + r\"}\"\n",
    "        )\n",
    "        latex_table += \"\\n\"\n",
    "        for i_group, model in enumerate(list(MODEL_GROUPS[backbone])):\n",
    "            latex_table += f\"& {MODEL2SH[model]:<10s}\"\n",
    "            for i_attr, attr in enumerate(TEST_ATTRS):\n",
    "                latex_table += \" &\"\n",
    "                filter1 = {\"model\": model, \"dataset\": dataset}\n",
    "                if model == \"timm_vit_base_patch16_224.mae\":\n",
    "                    filter1[\"dim_reducer\"] = \"PCA\"\n",
    "                    filter1[\"pca_variance\"] = 0.95\n",
    "                else:\n",
    "                    filter1[\"dim_reducer_man\"] = \"UMAP\"\n",
    "                    filter1[\"ndim_reduced_man\"] = 50\n",
    "                    filter1[\"dim_reducer_man_metric\"] = \"euclidean\"\n",
    "                sdf = select_rows(test_runs_df, filter1, allow_missing=False)\n",
    "                sdf = sdf[~pd.isna(sdf[metric_key2])]\n",
    "                my_val = np.nanmedian(sdf[metric_key])\n",
    "                if sum(sdf[metric_key2] != my_val) > 0:\n",
    "                    pass\n",
    "                if dummy:\n",
    "                    best_results_grouped[attr][clusterername].append(my_val)\n",
    "                    continue\n",
    "                is_best_grp = my_val + eps >= np.max(\n",
    "                    best_results_grouped[attr][clusterername]\n",
    "                )\n",
    "                latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "                latex_table += \"     \"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best_grp:\n",
    "                    latex_table += r\"\\tcg{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                latex_table += show_fmt.format(my_val)\n",
    "                if highlight_best:\n",
    "                    latex_table += r\"}\" if is_best_grp else \" \"\n",
    "                latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "\n",
    "            latex_table += r\" \\\\\" + \"\\n\"\n",
    "        latex_table += r\"\\toprule\" + \"\\n\"\n",
    "\n",
    "    first_agg = True\n",
    "    for i_clusterer, clusterer in enumerate(CLUSTERERS):\n",
    "        clusterername = CLUSTERER2SH.get(clusterer, clusterer)\n",
    "        my_override_fields = override_fields.copy()\n",
    "        if (\n",
    "            first_agg\n",
    "            and clusterer == \"AgglomerativeClustering\"\n",
    "            and metric_key != \"num_cluster_pred\"\n",
    "        ):\n",
    "            first_agg = False\n",
    "            my_override_fields[\"aggclust_dist_thresh\"] = None\n",
    "            clusterername = \"AC  w/ C\"\n",
    "        elif clusterer == \"AgglomerativeClustering\":\n",
    "            clusterername = \"AC w/o C\"\n",
    "            if \"aggclust_dist_thresh\" in my_override_fields:\n",
    "                del my_override_fields[\"aggclust_dist_thresh\"]\n",
    "\n",
    "        if i_clusterer > 0:\n",
    "            latex_table += r\"\\midrule\" + \"\\n\"\n",
    "\n",
    "        latex_table += (\n",
    "            r\"\\parbox[t]{2mm}{\\multirow{\"\n",
    "            + str(len(MODEL_GROUPS[backbone]))\n",
    "            + r\"}{*}{\"\n",
    "            + r\"\\scalebox{0.9}{\"\n",
    "            + r\"\\rotatebox[origin=c]{90}{\"\n",
    "            + clusterername\n",
    "            + r\"}}}\"\n",
    "            + r\"}\"\n",
    "        )\n",
    "        latex_table += \"\\n\"\n",
    "\n",
    "        for i_group, model in enumerate(list(MODEL_GROUPS[backbone])):\n",
    "            latex_table += f\"& {MODEL2SH[model]:<10s}\"\n",
    "            filter1 = {\"model\": model, \"dataset\": dataset}\n",
    "            filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "            filter2.update(filter1)\n",
    "            filter2.update(my_override_fields)\n",
    "            filter2 = fixup_filter(filter2)\n",
    "            sdf = select_rows(test_runs_df, filter2, allow_missing=False)\n",
    "            if len(sdf) < 1:\n",
    "                print(f\"No data for {model}-{dataset}-{clusterer}\\n{filter2}\")\n",
    "                cmds.append(filter2command(filter2, partition=\"test\"))\n",
    "                if not dummy:\n",
    "                    # latex_table += r\"\\multicolumn{1}{c}{--}\"\n",
    "                    latex_table += r\"   --  \"\n",
    "                continue\n",
    "            if len(sdf) > 1:\n",
    "                if sum(np.abs(sdf[metric_key] - sdf.iloc[0][metric_key]) > 1e-6) > 0:\n",
    "                    print()\n",
    "                    print(f\"More than one result with {metric_key} values\", list(sdf[metric_key]))\n",
    "                    print(f\"for search {filter2}\")\n",
    "                    dif_cols = find_differing_columns(sdf, config_keys)\n",
    "                    print(f\"columns which differ: {dif_cols}\")\n",
    "                    if dif_cols:\n",
    "                        for col in dif_cols:\n",
    "                            print(f\"  {col}: {list(sdf[col])}\")\n",
    "            y_pred = np.load(\"../\" + get_pred_path(sdf.iloc[0]))[\"y_pred\"]\n",
    "            for i_attr, attr in enumerate(TEST_ATTRS):\n",
    "                latex_table += \" &\"\n",
    "                if metric_key.lower() != \"ami\":\n",
    "                    raise NotImplementedError()\n",
    "                my_val = sklearn.metrics.adjusted_mutual_info_score(attrs[:, i_attr], y_pred)\n",
    "                if dummy:\n",
    "                    best_results[attr].append(my_val)\n",
    "                    best_results_grouped[attr][clusterername].append(my_val)\n",
    "                    continue\n",
    "                if np.isnan(my_val):\n",
    "                    latex_table += r\"   --  \"\n",
    "                    continue\n",
    "                is_best = my_val + eps >= np.max(best_results[attr])\n",
    "                if len(best_results[attr]) > 1:\n",
    "                    is_secd = my_val + eps >= np.sort(best_results[attr])[-2]\n",
    "                else:\n",
    "                    is_secd = False\n",
    "                is_best_grp = my_val + eps >= np.max(\n",
    "                    best_results_grouped[attr][clusterername]\n",
    "                )\n",
    "                sc_base = np.nanmedian(best_results[attr])\n",
    "                sc_top = np.max(best_results[attr])\n",
    "                sc = 100 * max(0, (my_val - sc_base) / (sc_top - sc_base))\n",
    "                latex_table += r\"\\cellcolor{cbg!\" + f\"{sc:.0f}\" + \"}\"\n",
    "                if show_pc:\n",
    "                    my_val = my_val * 100\n",
    "                latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best:\n",
    "                    latex_table += r\"\\tcf{\"\n",
    "                elif is_secd:\n",
    "                    latex_table += r\"\\tcs{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best_grp:\n",
    "                    latex_table += r\"\\tcg{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                latex_table += show_fmt.format(my_val)\n",
    "                if highlight_best:\n",
    "                    latex_table += r\"}\" if is_best or is_secd else \" \"\n",
    "                    latex_table += r\"}\" if is_best_grp else \" \"\n",
    "                latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "            latex_table += r\" \\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\bottomrule\" + \"\\n\"\n",
    "    latex_table += r\"\\end{tabular}\" + \"\\n\"\n",
    "    latex_table += r\"%}\" + \"\\n\"  # Disabled\n",
    "\n",
    "print()\n",
    "print(f\"There are {len(cmds)} commands to execute to generate missing datapoints\")\n",
    "if show_commands:\n",
    "    for cmd in cmds:\n",
    "        print(cmd)\n",
    "\n",
    "print()\n",
    "print(\"Done!\")\n",
    "print()\n",
    "print(f\"Here is your {dataset} results table for {metric_key}, {backbone}:\")\n",
    "print()\n",
    "print()\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FGVC Aircraft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_levels = [\"manufacturer\", \"family\", \"variant\"]\n",
    "attrs = np.stack(\n",
    "    [\n",
    "        torchvision.datasets.FGVCAircraft(\n",
    "            os.path.expanduser(\"~/Datasets\"), split=\"test\", annotation_level=annotation_level\n",
    "        )._labels\n",
    "        for annotation_level in annotation_levels\n",
    "    ],\n",
    "    axis=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_attr in range(len(annotation_levels)):\n",
    "    print(annotation_levels[i_attr], len(np.unique(attrs[:, i_attr])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "MyWQRN9iqBkI",
    "outputId": "2642efe3-b0ef-46f3-dc70-f2f5c69645d4"
   },
   "outputs": [],
   "source": [
    "metric_key = \"AMI\"  # AMI  num_cluster_pred  silhouette-euclidean_pred  silhouette-og-euclidean_pred\n",
    "show_pc = True\n",
    "show_fmt = \"{:4.0f}\"\n",
    "show_commands = False\n",
    "highlight_best = True\n",
    "use_si_num = False\n",
    "eps = 0.005\n",
    "override_fields = {\n",
    "    \"predictions_dir\": \"y_pred\",\n",
    "}\n",
    "\n",
    "backbone = \"ResNet-50\"  # \"ResNet-50\" or \"ViT-B\"\n",
    "\n",
    "if metric_key == \"num_cluster_pred\":\n",
    "    CLUSTERERS = [\"AC w/o C\", \"AffinityPropagation\", \"HDBSCAN\"]\n",
    "    show_pc = False\n",
    "    show_fmt = \"{:4.0f}\"\n",
    "    highlight_best = False\n",
    "    use_si_num = True\n",
    "else:\n",
    "    CLUSTERERS = [\"KMeans\", \"AC w/ C\", \"AC w/o C\", \"AffinityPropagation\", \"HDBSCAN\"]\n",
    "if metric_key.startswith(\"silhouette\"):\n",
    "    show_pc = False\n",
    "    show_fmt = \"{:5.2f}\"\n",
    "\n",
    "print(MODEL_GROUPS)\n",
    "\n",
    "dataset = \"aircraft\"\n",
    "\n",
    "TEST_ATTRS = annotation_levels\n",
    "print(TEST_ATTRS)\n",
    "best_results = {k: [] for k in TEST_ATTRS}\n",
    "best_results_grouped = {k: defaultdict(list) for k in TEST_ATTRS}\n",
    "\n",
    "for dummy in [True, False]:\n",
    "    cmds = []\n",
    "    latex_table = r\"% Results for \" + f\"{dataset} breakdown, {metric_key}, {backbone}\" + \"\\n\"\n",
    "    now_str = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    latex_table += r\"% Generated \" + now_str + \"\\n\"\n",
    "    latex_table += r\"% Using hparams \" + BEST_PARAMS[\"_version\"] + \"\\n\"\n",
    "    label = backbone\n",
    "    if metric_key == \"AMI\":\n",
    "        latex_table += r\"\\label{tab:\" + label + r\"}\" + \"\\n\"\n",
    "    label = metric_key.replace(\"_\", \"-\") + \":\" + label\n",
    "    latex_table += r\"\\label{tab:\" + label + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"%\\resizebox{\\textwidth}{!}{%\" + \"\\n\"  # Disabled\n",
    "    latex_table += r\"\\begin{tabular}{ll\" + r\"r\" * len(TEST_ATTRS) + r\"}\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    latex_table += r\"& \" + f\"{'Encoder':<11s}\"\n",
    "    for attr in TEST_ATTRS:\n",
    "        latex_table += r\"&\" + f\"{attr.replace('_', ' '):^15s}\"\n",
    "    latex_table += r\"\\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    print(MODEL_GROUPS[backbone])\n",
    "    if metric_key == \"num_cluster_pred\":\n",
    "        latex_table += r\"& Num targets\"\n",
    "        for i_attr, attr in enumerate(TEST_ATTRS):\n",
    "            sdf = select_rows(test_runs_df, {\"dataset\": dataset}, allow_missing=False)\n",
    "            sdf = sdf[~pd.isna(sdf[\"num_cluster_true\"])]\n",
    "            latex_table += r\"& \"\n",
    "            latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "            latex_table += f\"{sdf.iloc[0]['num_cluster_true'].item()}\"\n",
    "            latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "        latex_table += r\"\\\\\" + \"\\n\"\n",
    "        latex_table += r\"\\toprule\" + \"\\n\"\n",
    "    elif metric_key.endswith(\"_pred\"):\n",
    "        metric_key2 = metric_key.replace(\"_pred\", \"_true\")\n",
    "        clusterername = \"G.T.\"\n",
    "        latex_table += (\n",
    "            r\"\\parbox[t]{2mm}{\\multirow{\"\n",
    "            + str(len(MODEL_GROUPS[backbone]))\n",
    "            + r\"}{*}{\"\n",
    "            + r\"\\scalebox{0.9}{\"\n",
    "            + r\"\\rotatebox[origin=c]{90}{\"\n",
    "            + clusterername\n",
    "            + r\"}}}\"\n",
    "            + r\"}\"\n",
    "        )\n",
    "        latex_table += \"\\n\"\n",
    "        for i_group, model in enumerate(list(MODEL_GROUPS[backbone])):\n",
    "            latex_table += f\"& {MODEL2SH[model]:<10s}\"\n",
    "            for i_attr, attr in enumerate(TEST_ATTRS):\n",
    "                latex_table += \" &\"\n",
    "                filter1 = {\"model\": model, \"dataset\": dataset}\n",
    "                if model == \"timm_vit_base_patch16_224.mae\":\n",
    "                    filter1[\"dim_reducer\"] = \"PCA\"\n",
    "                    filter1[\"pca_variance\"] = 0.95\n",
    "                else:\n",
    "                    filter1[\"dim_reducer_man\"] = \"UMAP\"\n",
    "                    filter1[\"ndim_reduced_man\"] = 50\n",
    "                    filter1[\"dim_reducer_man_metric\"] = \"euclidean\"\n",
    "                sdf = select_rows(test_runs_df, filter1, allow_missing=False)\n",
    "                sdf = sdf[~pd.isna(sdf[metric_key2])]\n",
    "                my_val = np.nanmedian(sdf[metric_key])\n",
    "                if sum(sdf[metric_key2] != my_val) > 0:\n",
    "                    pass\n",
    "                if dummy:\n",
    "                    best_results_grouped[attr][clusterername].append(my_val)\n",
    "                    continue\n",
    "                is_best_grp = my_val + eps >= np.max(\n",
    "                    best_results_grouped[attr][clusterername]\n",
    "                )\n",
    "                latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "                latex_table += \"     \"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best_grp:\n",
    "                    latex_table += r\"\\tcg{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                latex_table += show_fmt.format(my_val)\n",
    "                if highlight_best:\n",
    "                    latex_table += r\"}\" if is_best_grp else \" \"\n",
    "                latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "\n",
    "            latex_table += r\" \\\\\" + \"\\n\"\n",
    "        latex_table += r\"\\toprule\" + \"\\n\"\n",
    "\n",
    "    first_agg = True\n",
    "    for i_clusterer, clusterer in enumerate(CLUSTERERS):\n",
    "        clusterername = CLUSTERER2SH.get(clusterer, clusterer)\n",
    "        my_override_fields = override_fields.copy()\n",
    "        if (\n",
    "            first_agg\n",
    "            and clusterer == \"AgglomerativeClustering\"\n",
    "            and metric_key != \"num_cluster_pred\"\n",
    "        ):\n",
    "            first_agg = False\n",
    "            my_override_fields[\"aggclust_dist_thresh\"] = None\n",
    "            clusterername = \"AC  w/ C\"\n",
    "        elif clusterer == \"AgglomerativeClustering\":\n",
    "            clusterername = \"AC w/o C\"\n",
    "            if \"aggclust_dist_thresh\" in my_override_fields:\n",
    "                del my_override_fields[\"aggclust_dist_thresh\"]\n",
    "\n",
    "        if i_clusterer > 0:\n",
    "            latex_table += r\"\\midrule\" + \"\\n\"\n",
    "\n",
    "        latex_table += (\n",
    "            r\"\\parbox[t]{2mm}{\\multirow{\"\n",
    "            + str(len(MODEL_GROUPS[backbone]))\n",
    "            + r\"}{*}{\"\n",
    "            + r\"\\scalebox{0.9}{\"\n",
    "            + r\"\\rotatebox[origin=c]{90}{\"\n",
    "            + clusterername\n",
    "            + r\"}}}\"\n",
    "            + r\"}\"\n",
    "        )\n",
    "        latex_table += \"\\n\"\n",
    "\n",
    "        for i_group, model in enumerate(list(MODEL_GROUPS[backbone])):\n",
    "            latex_table += f\"& {MODEL2SH[model]:<10s}\"\n",
    "            filter1 = {\"model\": model, \"dataset\": dataset}\n",
    "            filter2 = dict(DEFAULT_PARAMS[\"all\"], **BEST_PARAMS[clusterer][model])\n",
    "            filter2.update(filter1)\n",
    "            filter2.update(my_override_fields)\n",
    "            filter2 = fixup_filter(filter2)\n",
    "            sdf = select_rows(test_runs_df, filter2, allow_missing=False)\n",
    "            if len(sdf) < 1:\n",
    "                print(f\"No data for {model}-{dataset}-{clusterer}\\n{filter2}\")\n",
    "                cmds.append(filter2command(filter2, partition=\"test\"))\n",
    "                if not dummy:\n",
    "                    # latex_table += r\"\\multicolumn{1}{c}{--}\"\n",
    "                    latex_table += r\"   --  \"\n",
    "                continue\n",
    "            if len(sdf) > 1:\n",
    "                if sum(np.abs(sdf[metric_key] - sdf.iloc[0][metric_key]) > 1e-6) > 0:\n",
    "                    print()\n",
    "                    print(f\"More than one result with {metric_key} values\", list(sdf[metric_key]))\n",
    "                    print(f\"for search {filter2}\")\n",
    "                    dif_cols = find_differing_columns(sdf, config_keys)\n",
    "                    print(f\"columns which differ: {dif_cols}\")\n",
    "                    if dif_cols:\n",
    "                        for col in dif_cols:\n",
    "                            print(f\"  {col}: {list(sdf[col])}\")\n",
    "            y_pred = np.load(\"../\" + get_pred_path(sdf.iloc[0]))[\"y_pred\"]\n",
    "            for i_attr, attr in enumerate(TEST_ATTRS):\n",
    "                latex_table += \" &\"\n",
    "                if metric_key.lower() != \"ami\":\n",
    "                    raise NotImplementedError()\n",
    "                my_val = sklearn.metrics.adjusted_mutual_info_score(attrs[:, i_attr], y_pred)\n",
    "                if dummy:\n",
    "                    best_results[attr].append(my_val)\n",
    "                    best_results_grouped[attr][clusterername].append(my_val)\n",
    "                    continue\n",
    "                if np.isnan(my_val):\n",
    "                    latex_table += r\"   --  \"\n",
    "                    continue\n",
    "                is_best = my_val + eps >= np.max(best_results[attr])\n",
    "                if len(best_results[attr]) > 1:\n",
    "                    is_secd = my_val + eps >= np.sort(best_results[attr])[-2]\n",
    "                else:\n",
    "                    is_secd = False\n",
    "                is_best_grp = my_val + eps >= np.max(\n",
    "                    best_results_grouped[attr][clusterername]\n",
    "                )\n",
    "                sc_base = np.nanmedian(best_results[attr])\n",
    "                sc_top = np.max(best_results[attr])\n",
    "                sc = 100 * max(0, (my_val - sc_base) / (sc_top - sc_base))\n",
    "                latex_table += r\"\\cellcolor{cbg!\" + f\"{sc:.0f}\" + \"}\"\n",
    "                if show_pc:\n",
    "                    my_val = my_val * 100\n",
    "                latex_table += r\"\\num{\" if use_si_num else r\"$\"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best:\n",
    "                    latex_table += r\"\\tcf{\"\n",
    "                elif is_secd:\n",
    "                    latex_table += r\"\\tcs{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                if not highlight_best:\n",
    "                    pass\n",
    "                elif is_best_grp:\n",
    "                    latex_table += r\"\\tcg{\"\n",
    "                else:\n",
    "                    latex_table += \"     \"\n",
    "                latex_table += show_fmt.format(my_val)\n",
    "                if highlight_best:\n",
    "                    latex_table += r\"}\" if is_best or is_secd else \" \"\n",
    "                    latex_table += r\"}\" if is_best_grp else \" \"\n",
    "                latex_table += r\"}\" if use_si_num else r\"$\"\n",
    "            latex_table += r\" \\\\\" + \"\\n\"\n",
    "    latex_table += r\"\\bottomrule\" + \"\\n\"\n",
    "    latex_table += r\"\\end{tabular}\" + \"\\n\"\n",
    "    latex_table += r\"%}\" + \"\\n\"  # Disabled\n",
    "\n",
    "print()\n",
    "print(f\"There are {len(cmds)} commands to execute to generate missing datapoints\")\n",
    "if show_commands:\n",
    "    for cmd in cmds:\n",
    "        print(cmd)\n",
    "\n",
    "print()\n",
    "print(\"Done!\")\n",
    "print()\n",
    "print(f\"Here is your {dataset} results table for {metric_key}, {backbone}:\")\n",
    "print()\n",
    "print()\n",
    "print(latex_table)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
